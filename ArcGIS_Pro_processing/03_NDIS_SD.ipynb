{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kimo-N8ogrta"
      },
      "source": [
        "# NDIS Staggered Decision - CPM+RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrtZgSGmgrtc"
      },
      "outputs": [],
      "source": [
        "from arcgis.gis import GIS\n",
        "gis = GIS(\"home\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKdmAk-3grtf"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "#ArcGIS packages\n",
        "import arcpy\n",
        "#from arcgis.mapping import WebScene\n",
        "from arcgis.gis import GIS\n",
        "from arcgis.features import FeatureLayer\n",
        "from IPython.display import display\n",
        "from arcgis.features import GeoAccessor\n",
        "from arcgis import *\n",
        "from arcpy.sa import Raster, Int  # Raster float to integer\n",
        "# Raster processing for dataframe\n",
        "from rasterstats import zonal_stats\n",
        "\n",
        "# basic packages\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import timeit\n",
        "import random\n",
        "import string\n",
        "from pprint import pprint\n",
        "\n",
        "# Data management\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point  # to get points from long lat\n",
        "import gc\n",
        "import shutil\n",
        "import tempfile\n",
        "import ast\n",
        "from collections import Counter\n",
        "from typing import Optional, List\n",
        "\n",
        "# Request service\n",
        "#from requests import Request\n",
        "import json\n",
        "import re\n",
        "from functools import reduce\n",
        "#from owslib.wfs import WebFeatureService\n",
        "\n",
        "# Plotting packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning packages\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.tree import export_text\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "import multiprocessing\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "# To save the model and everything else to be API ready\n",
        "import joblib\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBPlzGdJgrtj"
      },
      "outputs": [],
      "source": [
        "# Extract useful field/column for staggered decision\n",
        "columns_to_drop = [\n",
        "    'selected_sensor',\n",
        "    'sensor_id',\n",
        "    'parameters',\n",
        "    'sensor_name',\n",
        "    's_encoded',\n",
        "    'sensor_weight',\n",
        "    'selected_drone',\n",
        "    'drone_speed',\n",
        "    'drone_flight_time'\n",
        "]\n",
        "\n",
        "ghz_sd = geohazard_df.drop(columns=columns_to_drop)\n",
        "ghz_sd.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5gcaXh_grtk"
      },
      "outputs": [],
      "source": [
        "# Function to obtain \"HazardStage\" --> \"pre_event\", \"active\", \"dormant\", \"aftershock\", \"triggered\", \"post_event\"\n",
        "def infer_stage(row):\n",
        "    ht = row[\"HazardType\"]\n",
        "    intensity = row[\"intensity\"]\n",
        "    duration = row[\"duration_minutes\"]\n",
        "\n",
        "    if ht == \"Fault\":\n",
        "        return \"pre_event\"\n",
        "    elif ht == \"Earthquake\":\n",
        "        if intensity >= 6:\n",
        "            return \"during\"\n",
        "        elif 4 <= intensity < 6:\n",
        "            return \"aftershock\"\n",
        "        else:\n",
        "            return \"pre_event\"\n",
        "    elif ht == \"Volcano\":\n",
        "        if intensity >= 4 or duration > 120:\n",
        "            return \"during\"\n",
        "        elif 1.5 <= intensity < 4:\n",
        "            return \"pre_event\"\n",
        "        elif duration <= 60:\n",
        "            return \"dormant\"\n",
        "        else:\n",
        "            return \"clean_up\"\n",
        "    elif ht == \"Landslide\":\n",
        "        if duration < 60:\n",
        "            return \"during\"\n",
        "        elif 60 <= duration < 180:\n",
        "            return \"post_event\"\n",
        "        else:\n",
        "            return \"pre_event\"\n",
        "    elif ht == \"Tsunami\":\n",
        "        if duration <= 60:\n",
        "            return \"during\"\n",
        "        elif 60 < duration <= 240:\n",
        "            return \"post_event\"\n",
        "        else:\n",
        "            return \"clean_up\"\n",
        "    else:\n",
        "        return \"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z81xpyGrgrtl"
      },
      "outputs": [],
      "source": [
        "ghz_sd[\"HazardStage\"] = ghz_sd.apply(infer_stage, axis=1)\n",
        "ghz_sd.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dQ97pMMgrtm"
      },
      "outputs": [],
      "source": [
        "hazard_mapping = {\n",
        "    1: \"Volcano\",\n",
        "    2: \"Landslide\",\n",
        "    3: \"Tsunami\",\n",
        "    4: \"Fault\",\n",
        "    5: \"Earthquake\"\n",
        "}\n",
        "\n",
        "ghz_sd[\"HazardType\"] = ghz_sd[\"HazardType\"].map(hazard_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEodaWu9grtn"
      },
      "outputs": [],
      "source": [
        "# Vectorized Version if the \"apply\" method take too long\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "haz = ghz_sd[\"HazardType\"]\n",
        "intensity = ghz_sd[\"intensity\"].fillna(0)\n",
        "duration = ghz_sd[\"duration_minutes\"].fillna(0)\n",
        "\n",
        "# Fault\n",
        "cond_fault = (haz == \"Fault\")\n",
        "\n",
        "# Earthquake: 0.1–5 min typical\n",
        "cond_eq_during = (haz == \"Earthquake\") & (duration <= 5)\n",
        "cond_eq_post = (haz == \"Earthquake\") & (duration > 5)\n",
        "\n",
        "# Volcano: days to months\n",
        "cond_volc_during = (haz == \"Volcano\") & (duration >= 1440)  # 1 day+\n",
        "cond_volc_pre = (haz == \"Volcano\") & (intensity <= 2) & (duration < 1440)\n",
        "cond_volc_dormant = (haz == \"Volcano\") & (intensity < 1.5) & (duration < 60)\n",
        "cond_volc_cleanup = (haz == \"Volcano\") & (intensity <= 3) & (duration >= 2880)  # 2+ days\n",
        "\n",
        "# Landslide: 10 min – 3 days\n",
        "cond_ls_during = (haz == \"Landslide\") & (duration <= 180)       # <= 3 hours\n",
        "cond_ls_post = (haz == \"Landslide\") & (duration > 180) & (duration <= 4320)  # <= 3 days\n",
        "cond_ls_cleanup = (haz == \"Landslide\") & (duration > 4320)\n",
        "\n",
        "# Tsunami: 10–360 min typical\n",
        "cond_ts_during = (haz == \"Tsunami\") & (duration <= 240)\n",
        "cond_ts_post = (haz == \"Tsunami\") & (duration > 240) & (duration <= 720)\n",
        "cond_ts_cleanup = (haz == \"Tsunami\") & (duration > 720)\n",
        "\n",
        "conditions = [\n",
        "    cond_fault,\n",
        "    cond_eq_during, cond_eq_post,\n",
        "    cond_volc_during, cond_volc_pre, cond_volc_dormant, cond_volc_cleanup,\n",
        "    cond_ls_during, cond_ls_post, cond_ls_cleanup,\n",
        "    cond_ts_during, cond_ts_post, cond_ts_cleanup\n",
        "]\n",
        "\n",
        "choices = [\n",
        "    \"pre_event\",        # Fault\n",
        "    \"during\", \"post_event\",           # Earthquake\n",
        "    \"during\", \"pre_event\", \"dormant\", \"clean_up\",  # Volcano\n",
        "    \"during\", \"post_event\", \"clean_up\",            # Landslide\n",
        "    \"during\", \"post_event\", \"clean_up\"             # Tsunami\n",
        "]\n",
        "\n",
        "# Apply vectorized selection\n",
        "ghz_sd[\"HazardStage\"] = np.select(conditions, choices, default=\"unknown\")\n",
        "\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 RF Training completed! Elapsed time: %s minutes\" % str(elapsed / 60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH8_Zmwmgrtp"
      },
      "source": [
        "# NEW ENTRY NUCLEAR ☢️\n",
        "power plant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KGDAjQZgrtq"
      },
      "outputs": [],
      "source": [
        "nuclear_stat = nuclear_df[['GEM unit ID','Project Name', 'Longitude', 'Latitude', 'Capacity (MW)', 'Country/Area', 'Owner', 'Status', 'Reactor Type', 'Wiki URL']].copy()\n",
        "nuclear_stat.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdRiccUfgrtr"
      },
      "source": [
        "## Economic loss:\n",
        "based on https://thecostguys.com/business/build-nuclear-power-plant and https://en.wikipedia.org/wiki/Economics_of_nuclear_power_plants#Construction_costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F26_elQVgrtr"
      },
      "outputs": [],
      "source": [
        "# Clean numeric Capacity\n",
        "nuclear_df[\"Capacity (MW)\"] = pd.to_numeric(nuclear_df[\"Capacity (MW)\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# Extract numeric ID from GEM unit ID\n",
        "gem_id_numeric = nuclear_df[\"GEM unit ID\"].astype(str).str.extract(r\"(\\d+)$\")[0]\n",
        "hazard_id = \"111\" + gem_id_numeric\n",
        "hazard_id = hazard_id.astype(\"int64\")\n",
        "\n",
        "# Assemble the cleaned nuclear hazard dataframe\n",
        "cleaned_nuclear_df = pd.DataFrame({\n",
        "    \"HazardID\": hazard_id,\n",
        "    \"latitude\": pd.to_numeric(nuclear_df[\"Latitude\"], errors=\"coerce\"),\n",
        "    \"longitude\": pd.to_numeric(nuclear_df[\"Longitude\"], errors=\"coerce\"),\n",
        "    \"HazardType\": \"Nuclear\",\n",
        "    \"distance\": np.nan,                 # To be filled by spatial ops\n",
        "    \"pop\": np.nan,                      # To be filled by ArcGIS later\n",
        "    \"intensity\": 5.0,                   # Default high severity\n",
        "    \"duration_minutes\": 1440.0,         # 1 day\n",
        "    \"economic_loss_million\": nuclear_df[\"Capacity (MW)\"] * 11.5,  # $11.5M per MW\n",
        "    \"travel_time\": np.nan,\n",
        "    \"monitor_time\": 120.0,              # Default 2 hours\n",
        "    \"cpm_total_time\": np.nan,\n",
        "    \"HazardStage\": \"pre_event\"\n",
        "})\n",
        "\n",
        "cleaned_nuclear_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-2JP6Czgrtr"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "# Parameters\n",
        "layer_name = \"nuclearpp\" # Feature class\n",
        "# Use geopandas to read it\n",
        "input_gdf = gpd.read_file(gdb_path, layer=layer_name)\n",
        "population_raster = r\"D:\\NDIS_Database\\12_Population_synthetic\\nasapopct.tif\"\n",
        "buffer_dist = 30000  # 30km\n",
        "chunk_size = 100\n",
        "std_threshold = 10\n",
        "\n",
        "# Load input as GeoDataFrame\n",
        "#gdf = gpd.read_file(input_fc)\n",
        "\n",
        "# Reproject to WGS84 (EPSG:4326) if needed\n",
        "if input_gdf.crs.to_epsg() != 4326:\n",
        "    input_gdf = input_gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Calculate approximate buffer distance in degrees\n",
        "buffer_deg = buffer_dist / 111320.0  # 1 degree ≈ 111.32 km at equator\n",
        "\n",
        "# Final results\n",
        "final_chunks = []\n",
        "\n",
        "total = len(input_gdf)\n",
        "print(f\"Total features: {total}\")\n",
        "\n",
        "for start in range(0, total, chunk_size):\n",
        "    print(f\"\\u23F3 Processing chunk {start+1} to {min(start + chunk_size, total)}\")\n",
        "\n",
        "    chunk = input_gdf.iloc[start:start + chunk_size].copy()\n",
        "\n",
        "    # Suppress geographic CRS buffer warning\n",
        "    warnings.filterwarnings(\"ignore\", message=\"Geometry is in a geographic CRS.*\")\n",
        "    chunk[\"geometry\"] = chunk.geometry.buffer(buffer_deg)\n",
        "\n",
        "    # Zonal statistics\n",
        "    stats = zonal_stats(chunk, population_raster,\n",
        "                        stats=[\"mean\", \"majority\", \"count\", \"std\"],\n",
        "                        geojson_out=False)\n",
        "\n",
        "    # Smart pop estimation\n",
        "    smart_pops = []\n",
        "    for row in stats:\n",
        "        count = row.get(\"count\", 0) or 0\n",
        "        mean = row.get(\"mean\", 0) or 0\n",
        "        majority = row.get(\"majority\", 0) or 0\n",
        "        std = row.get(\"std\", 0) or 0\n",
        "\n",
        "        if count == 0:\n",
        "            pop = 0\n",
        "        elif std < std_threshold:\n",
        "            pop = majority * count\n",
        "        else:\n",
        "            pop = mean * count\n",
        "\n",
        "        smart_pops.append(pop)\n",
        "\n",
        "    chunk[\"pop\"] = smart_pops\n",
        "    final_chunks.append(chunk)\n",
        "\n",
        "    print(f\"\\u2705 Chunk {start+1} processed.\")\n",
        "\n",
        "# Combine all results\n",
        "nuclear_gdf = pd.concat(final_chunks, ignore_index=True)\n",
        "\n",
        "# Optional: export to CSV\n",
        "# final_gdf.drop(columns=\"geometry\").to_csv(\"smart_population_estimate.csv\", index=False)\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All done. Result in dataframe: nuclear_gdf. Elapsed time: %s minutes\" % str(elapsed / 60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeudErTIgrts"
      },
      "outputs": [],
      "source": [
        "# Extract numeric part from GEM_unit_ID in population data\n",
        "nuclear_gdf[\"unit_id_digits\"] = nuclear_gdf[\"GEM_unit_ID\"].astype(str).str.extract(r\"(\\d+)$\")[0]\n",
        "nuclear_gdf[\"HazardID\"] = (\"111\" + nuclear_gdf[\"unit_id_digits\"]).astype(\"int64\")\n",
        "\n",
        "# Merge population into cleaned_nuclear_df using HazardID\n",
        "cleaned_nuclear_df = cleaned_nuclear_df.merge(\n",
        "    nuclear_gdf[[\"HazardID\", \"pop\"]],\n",
        "    on=\"HazardID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "cleaned_nuclear_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC5UPjEdgrtt"
      },
      "outputs": [],
      "source": [
        "# Replace original 'pop' with the merged one ('pop_y'), then drop the extra\n",
        "cleaned_nuclear_df[\"pop\"] = cleaned_nuclear_df[\"pop_y\"]\n",
        "cleaned_nuclear_df.drop(columns=[\"pop_x\", \"pop_y\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P3h_JNigrtt"
      },
      "source": [
        "## Distance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlapoxM6grtt"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "\n",
        "# Define paths\n",
        "project_folder = r\"D:\\ArcGISProjects\\GeohazardDB\"\n",
        "#gdb_path = os.path.join(project_folder, \"GeohazardDB.gdb\")\n",
        "\n",
        "# Input layers\n",
        "nuclear_layer = os.path.join(gdb_path, \"nuclearpp\")\n",
        "road_layer = os.path.join(gdb_path, \"roads\")\n",
        "country_layer = os.path.join(gdb_path, \"eez_country\")\n",
        "\n",
        "# Output lists\n",
        "nuc_list = []\n",
        "near_tables = []\n",
        "\n",
        "# Create GDB if it doesn't exist\n",
        "if not arcpy.Exists(gdb_path):\n",
        "    arcpy.CreateFileGDB_management(project_folder, \"GeohazardDB.gdb\")\n",
        "\n",
        "total_countries = int(arcpy.GetCount_management(country_layer)[0])\n",
        "\n",
        "# Iterate through countries\n",
        "with arcpy.da.SearchCursor(country_layer, [\"ISO_TER1\", \"SHAPE@\"]) as country_cursor:\n",
        "    for index, row in enumerate(country_cursor, start=1):\n",
        "        country_code = row[0]\n",
        "        country_geometry = row[1]\n",
        "        print(f\"\\u23F3 Processing country {index}/{total_countries}: {country_code}...\")\n",
        "\n",
        "        # Output names\n",
        "        nuc_clip = os.path.join(gdb_path, f\"nuc_{country_code}\")\n",
        "        road_clip = os.path.join(gdb_path, f\"road_{country_code}\")\n",
        "        near_output = os.path.join(gdb_path, f\"near_{country_code}\")\n",
        "\n",
        "        # Clip nuclear\n",
        "        if arcpy.Exists(nuc_clip):\n",
        "            arcpy.Delete_management(nuc_clip)\n",
        "        try:\n",
        "            arcpy.Clip_analysis(nuclear_layer, country_geometry, nuc_clip)\n",
        "            print(f\"  \\u2705 Clipped nuclear layer: {nuc_clip}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  \\u274C Error clipping nuclear: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Clip roads\n",
        "        if arcpy.Exists(road_clip):\n",
        "            arcpy.Delete_management(road_clip)\n",
        "        try:\n",
        "            arcpy.Clip_analysis(road_layer, country_geometry, road_clip)\n",
        "            print(f\"  \\u2705 Clipped road layer: {road_clip}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  \\u274C Error clipping roads: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Near analysis\n",
        "        if arcpy.Exists(near_output):\n",
        "            arcpy.Delete_management(near_output)\n",
        "        try:\n",
        "            arcpy.GenerateNearTable_analysis(\n",
        "                in_features=nuc_clip,\n",
        "                near_features=road_clip,\n",
        "                out_table=near_output,\n",
        "                search_radius=\"\",\n",
        "                location=\"LOCATION\",\n",
        "                angle=\"ANGLE\",\n",
        "                closest=\"CLOSEST\",\n",
        "                method=\"GEODESIC\"\n",
        "            )\n",
        "            print(f\"  \\u2705 Near analysis completed: {near_output}\")\n",
        "            near_tables.append(near_output)\n",
        "        except Exception as e:\n",
        "            print(f\"  \\u274C Error in Near Analysis: {e}\")\n",
        "\n",
        "        # Add distance field if missing\n",
        "        if \"distance\" not in [f.name for f in arcpy.ListFields(nuc_clip)]:\n",
        "            arcpy.AddField_management(nuc_clip, \"distance\", \"DOUBLE\")\n",
        "\n",
        "        with arcpy.da.UpdateCursor(nuc_clip, [\"OBJECTID\", \"distance\"]) as nuc_cursor:\n",
        "            for nuc_row in nuc_cursor:\n",
        "                nuc_id = nuc_row[0]\n",
        "                with arcpy.da.SearchCursor(near_output, [\"IN_FID\", \"NEAR_DIST\"]) as near_cursor:\n",
        "                    for near_row in near_cursor:\n",
        "                        if near_row[0] == nuc_id:\n",
        "                            nuc_row[1] = near_row[1]\n",
        "                            nuc_cursor.updateRow(nuc_row)\n",
        "                            break\n",
        "\n",
        "        print(f\"\\u2705 NEAR_DIST added to {nuc_clip} as 'distance' field.\")\n",
        "\n",
        "        # Add HazardID field to near table if missing\n",
        "        if \"HazardID\" not in [f.name for f in arcpy.ListFields(near_output)]:\n",
        "            arcpy.AddField_management(near_output, \"GEM_unit_ID\", \"TEXT\")\n",
        "\n",
        "        with arcpy.da.UpdateCursor(near_output, [\"IN_FID\", \"GEM_unit_ID\"]) as cursor:\n",
        "            for row in cursor:\n",
        "                with arcpy.da.SearchCursor(nuc_clip, [\"OBJECTID\", \"GEM_unit_ID\"]) as nuc_cursor:\n",
        "                    for nuc_row in nuc_cursor:\n",
        "                        if row[0] == nuc_row[0]:\n",
        "                            row[1] = nuc_row[1]\n",
        "                            cursor.updateRow(row)\n",
        "                            break\n",
        "\n",
        "        print(f\"\\u2705HazardID added to {near_output}, replacing NEAR_FID reference.\")\n",
        "        nuc_list.append(nuc_clip)\n",
        "\n",
        "# Merge all nuc_XXX datasets\n",
        "nuc_dist = os.path.join(gdb_path, \"nuc_dist\")\n",
        "if arcpy.Exists(nuc_dist):\n",
        "    arcpy.Delete_management(nuc_dist)\n",
        "arcpy.Merge_management(nuc_list, nuc_dist)\n",
        "print(f\"\\u2705 All nuclear data merged into {nuc_dist} successfully!\")\n",
        "\n",
        "# Compile all near tables\n",
        "compiled_near_table = os.path.join(gdb_path, \"compiled_nuclear_near_table\")\n",
        "if arcpy.Exists(compiled_near_table):\n",
        "    arcpy.Delete_management(compiled_near_table)\n",
        "\n",
        "arcpy.CreateTable_management(gdb_path, \"compiled_nuclear_near_table\")\n",
        "arcpy.AddField_management(compiled_near_table, \"FROM_X\", \"DOUBLE\")\n",
        "arcpy.AddField_management(compiled_near_table, \"FROM_Y\", \"DOUBLE\")\n",
        "arcpy.AddField_management(compiled_near_table, \"NEAR_X\", \"DOUBLE\")\n",
        "arcpy.AddField_management(compiled_near_table, \"NEAR_Y\", \"DOUBLE\")\n",
        "arcpy.AddField_management(compiled_near_table, \"NEAR_FID\", \"LONG\")\n",
        "arcpy.AddField_management(compiled_near_table, \"GEM_unit_ID\", \"TEXT\")\n",
        "\n",
        "with arcpy.da.InsertCursor(compiled_near_table, [\"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\", \"NEAR_FID\", \"GEM_unit_ID\"]) as insert_cursor:\n",
        "    for near_table in near_tables:\n",
        "        with arcpy.da.SearchCursor(near_table, [\"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\", \"NEAR_FID\", \"GEM_unit_ID\"]) as cursor:\n",
        "            for row in cursor:\n",
        "                insert_cursor.insertRow(row)\n",
        "\n",
        "print(f\"\\u2705 All nuclear near tables compiled into {compiled_near_table}!\")\n",
        "\n",
        "# Create polylines from near table\n",
        "polyline_output = os.path.join(gdb_path, \"compiled_nuclear_lines\")\n",
        "if arcpy.Exists(polyline_output):\n",
        "    arcpy.Delete_management(polyline_output)\n",
        "\n",
        "arcpy.XYToLine_management(\n",
        "    compiled_near_table,\n",
        "    polyline_output,\n",
        "    \"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\"\n",
        ")\n",
        "print(f\"\\u2705 Nuclear polylines created at {polyline_output}!\")\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(f\"\\u2705 All nuclear processing completed! Elapsed time: {elapsed / 60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WoykAXrgrtu"
      },
      "outputs": [],
      "source": [
        "nuc_dist = \"nuc_dist\"  # Geohazard feature class\n",
        "\n",
        "# Use arcpy to create a list of fields\n",
        "nuc_fields = [f.name for f in arcpy.ListFields(f\"{gdb_path}\\\\{nuc_dist}\")]\n",
        "\n",
        "# Use arcpy to create a search cursor and load the data into a list of dictionaries\n",
        "nuc_data = []\n",
        "with arcpy.da.SearchCursor(f\"{gdb_path}\\\\{nuc_dist}\", nuc_fields) as cursor:\n",
        "    for row in cursor:\n",
        "        nuc_data.append(dict(zip(nuc_fields, row)))\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "nucdf = pd.DataFrame(nuc_data)\n",
        "nucdf.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip9MQC8pgrtv"
      },
      "outputs": [],
      "source": [
        "nan_in_nuc = nucdf.isnull().sum().sum()\n",
        "\n",
        "# printing the number of values present in\n",
        "# the whole dataframe\n",
        "print('Number of NaN values present: ' + str(nan_in_nuc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQlHvK5Ggrtw"
      },
      "outputs": [],
      "source": [
        "# Extract numeric part from GEM_unit_ID in population data\n",
        "nucdf[\"unit_id_digits\"] = nucdf[\"GEM_unit_ID\"].astype(str).str.extract(r\"(\\d+)$\")[0]\n",
        "nucdf[\"HazardID\"] = (\"111\" + nucdf[\"unit_id_digits\"]).astype(\"int64\")\n",
        "\n",
        "# Merge population into cleaned_nuclear_df using HazardID\n",
        "cleaned_nuclear_df = cleaned_nuclear_df.merge(\n",
        "    nucdf[[\"HazardID\", \"distance\"]],\n",
        "    on=\"HazardID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "cleaned_nuclear_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRtjBe5Ggrtz"
      },
      "outputs": [],
      "source": [
        "# Custom hazard order to space out Volcano and Tsunami\n",
        "custom_order = ['Earthquake', 'Volcano','Landslide','Fault', 'Tsunami']  # Adjust as needed\n",
        "\n",
        "# Count hazard types\n",
        "hazard_counts = full_ghz_df['HazardType'].value_counts()\n",
        "hazard_counts = hazard_counts.reindex(custom_order).dropna()\n",
        "\n",
        "labels = hazard_counts.index.tolist()\n",
        "sizes = hazard_counts.values\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "wedges, texts = ax.pie(\n",
        "    sizes,\n",
        "    startangle=60,\n",
        "    wedgeprops=dict(width=0.3),\n",
        "    pctdistance=0.75\n",
        ")\n",
        "\n",
        "# Annotate\n",
        "for i, p in enumerate(wedges):\n",
        "    ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
        "    y = np.sin(np.deg2rad(ang))\n",
        "    x = np.cos(np.deg2rad(ang))\n",
        "    ha = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
        "    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n",
        "\n",
        "    # Custom tweak for 'Volcano'\n",
        "    if labels[i] == \"Volcano\":\n",
        "        x = 0.5  # Push label to the right\n",
        "        y = -1.0 # Drop downward\n",
        "        ha = \"left\"\n",
        "        connectionstyle = \"angle,angleA=0,angleB=90\"  # 270° arrow style\n",
        "\n",
        "    ax.annotate(f\"{labels[i]} ({sizes[i] / sizes.sum():.1%})\",\n",
        "                xy=(x, y),\n",
        "                xytext=(1.2*np.sign(x), 1.2*y),\n",
        "                horizontalalignment=ha,\n",
        "                arrowprops=dict(arrowstyle=\"-\", connectionstyle=connectionstyle))\n",
        "\n",
        "# Add center circle\n",
        "centre_circle = plt.Circle((0, 0), 0.55, fc='white')\n",
        "fig.gca().add_artist(centre_circle)\n",
        "\n",
        "ax.set_title(\"Geohazard Type Distribution\", pad=20)\n",
        "ax.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXBJt38kgrt0"
      },
      "outputs": [],
      "source": [
        "# Hazard Type Chart\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "hazard_counts = full_ghz_df['HazardType'].value_counts()\n",
        "labels = hazard_counts.index.tolist()\n",
        "sizes = hazard_counts.values\n",
        "\n",
        "# Plot only wedges and texts\n",
        "wedges, texts = ax.pie(\n",
        "    sizes,\n",
        "    startangle=45,\n",
        "    wedgeprops=dict(width=0.3),\n",
        "    pctdistance=0.75\n",
        ")\n",
        "\n",
        "# Add label lines and external labels\n",
        "for i, p in enumerate(wedges):\n",
        "    ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
        "    y = np.sin(np.deg2rad(ang))\n",
        "    x = np.cos(np.deg2rad(ang))\n",
        "    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
        "    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n",
        "\n",
        "    ax.annotate(f\"{labels[i]} ({sizes[i] / sizes.sum():.1%})\",\n",
        "                xy=(x, y),\n",
        "                xytext=(1.2*np.sign(x), 1.2*y),\n",
        "                horizontalalignment=horizontalalignment,\n",
        "                arrowprops=dict(arrowstyle=\"-\", connectionstyle=connectionstyle))\n",
        "\n",
        "centre_circle = plt.Circle((0, 0), 0.55, fc='white')\n",
        "fig.gca().add_artist(centre_circle)\n",
        "\n",
        "ax.set_title(\"Geohazard Type Distribution\", pad=20)\n",
        "ax.axis('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "# Save the doughnut chart as a PNG image with high resolution\n",
        "#plt.savefig(r'D:\\NDIS_Database\\ghz_type.png', dpi=300, transparent=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWUyHQwMgrt0"
      },
      "source": [
        "# STAGGERED DECISION STARTS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WWTrc8pgrt9"
      },
      "source": [
        "# STEP 1: Build Hazard-Stage-Survey Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUaiME-cgrt9"
      },
      "outputs": [],
      "source": [
        "# HazardStage decision mapping\n",
        "hazard_stage_survey_map = {\n",
        "    (\"Volcano\", \"pre_event\"): [\"Magnetometers\", \"Seismic\"],\n",
        "    (\"Volcano\", \"during\"): [\"Thermal_Camera\", \"Camera\", \"Multispectral\"],\n",
        "    (\"Volcano\", \"post_event\"): [\"Lidar\", \"Camera\"],\n",
        "    (\"Volcano\", \"clean_up\"): [\"Lidar\", \"Camera\"],\n",
        "\n",
        "    (\"Earthquake\", \"pre_event\"): [\"Seismic\"],\n",
        "    (\"Earthquake\", \"during\"): [\"Seismic\", \"Camera\"],\n",
        "    (\"Earthquake\", \"post_event\"): [\"Lidar\", \"Camera\"],\n",
        "\n",
        "    (\"Fault\", \"pre_event\"): [\"Seismic\", \"Magnetometers\"],\n",
        "    (\"Fault\", \"post_event\"): [\"Seismic\", \"Camera\"],\n",
        "\n",
        "    (\"Landslide\", \"pre_event\"): [\"Lidar\", \"GPR\"],\n",
        "    (\"Landslide\", \"during\"): [\"Camera\", \"Thermal_Camera\"],\n",
        "    (\"Landslide\", \"post_event\"): [\"Lidar\", \"Seismic\"],\n",
        "    (\"Landslide\", \"clean_up\"): [\"Camera\"],\n",
        "\n",
        "    (\"Tsunami\", \"during\"): [\"Camera\", \"Thermal_Camera\"],\n",
        "    (\"Tsunami\", \"post_event\"): [\"Camera\", \"Lidar\"],\n",
        "    (\"Tsunami\", \"clean_up\"): [\"Camera\"],\n",
        "\n",
        "    (\"Nuclear\", \"pre_event\"): [\"Thermal_Camera\", \"Camera\"],\n",
        "    (\"Nuclear\", \"during\"): [\"Thermal_Camera\", \"Camera\"],\n",
        "    (\"Nuclear\", \"post_event\"): [\"Camera\", \"Lidar\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoWbFQKhgrt-"
      },
      "outputs": [],
      "source": [
        "# Test dict to geohazard dataset\n",
        "full_ghz_df = ghz_nuc.copy()\n",
        "full_ghz_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLnt97j3grt_"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create haz_stage_key\n",
        "full_ghz_df[\"haz_stage_key\"] = list(zip(full_ghz_df[\"HazardType\"], full_ghz_df[\"HazardStage\"]))\n",
        "\n",
        "# Step 2: Map hazard_stage_survey_map\n",
        "full_ghz_df[\"SurveyOptions\"] = full_ghz_df[\"haz_stage_key\"].map(hazard_stage_survey_map)\n",
        "\n",
        "# Step 3: Use .where() to replace NaN with empty list []\n",
        "full_ghz_df[\"SurveyOptions\"] = full_ghz_df[\"SurveyOptions\"].where(full_ghz_df[\"SurveyOptions\"].notna(), [[]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IQtj53KgruA"
      },
      "source": [
        "# STEP 2: Build SurveyType → SensorName Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF5W1xBKgruA"
      },
      "outputs": [],
      "source": [
        "survey_sensor_map = {\n",
        "    \"Seismic\": \"Seismic\",\n",
        "    \"Magnetics\": \"Magnetometers\",\n",
        "    \"Thermal_Camera\": \"Thermal_Camera\",\n",
        "    \"Camera\": \"Camera\",\n",
        "    \"Multispectral\": \"Multispectral\",\n",
        "    \"Hyperspectral\": \"Hyperspectral\",\n",
        "    \"Lidar\": \"Lidar\",\n",
        "    \"GPR\": \"GPR\",\n",
        "    \"Electromagnetic\" : \"EM_Sensor\",\n",
        "    \"Gravity\" : \"Gravimeter\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxwsE6KLgruB"
      },
      "source": [
        "# Step 3: Attach Matched Sensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0xQz0kDgruB"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "# Step 1: Explode SurveyOptions\n",
        "exploded = full_ghz_df.explode(\"SurveyOptions\")\n",
        "\n",
        "# Step 2: Map surveys to sensors\n",
        "exploded[\"MatchedSensor\"] = exploded[\"SurveyOptions\"].map(survey_sensor_map)\n",
        "\n",
        "# Step 3: Group back without fillna that breaks (handle missing manually)\n",
        "matched_sensor_series = exploded.groupby(level=0)[\"MatchedSensor\"].apply(lambda x: list(x.dropna()))\n",
        "\n",
        "# Step 4: Reindex and replace NaN groups with empty lists manually\n",
        "matched_sensor_series = matched_sensor_series.reindex(full_ghz_df.index)\n",
        "matched_sensor_series = matched_sensor_series.apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# Step 5: Assign back\n",
        "full_ghz_df[\"MatchedSensors\"] = matched_sensor_series\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d_TD84PgruC"
      },
      "source": [
        "# STEP 4: Sensor --> Drone Matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag_Ok-odgruD"
      },
      "source": [
        "## Sensor Match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm7d9d9cgruE"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create lookup for sensor_name -> sensor_weight\n",
        "sensor_weight_lookup = sensor_df.set_index(\"sensor_name\")[\"sensor_weight\"].to_dict()\n",
        "\n",
        "# Step 2: Precompute for all sensors the eligible drones\n",
        "eligible_drones = {}\n",
        "\n",
        "for sensor_name, weight in sensor_weight_lookup.items():\n",
        "    eligible_drones[sensor_name] = list(\n",
        "        drone_df.loc[\n",
        "            (drone_df[\"max_payload_weight\"] >= weight) &\n",
        "            (drone_df[\"max_payload_weight\"].notna())\n",
        "        ][\"mfc_model\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JCXvi-FgruE"
      },
      "outputs": [],
      "source": [
        "# Step 3: Assign eligible drones for each MatchedSensor per hazard\n",
        "def find_eligible_drones(sensor_list):\n",
        "    drone_set = set()\n",
        "    for sensor in sensor_list:\n",
        "        drone_set.update(eligible_drones.get(sensor, []))\n",
        "    return list(drone_set)\n",
        "\n",
        "full_ghz_df[\"EligibleDrones\"] = full_ghz_df[\"MatchedSensors\"].apply(find_eligible_drones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFXQLtmHgruG"
      },
      "outputs": [],
      "source": [
        "# Step 1: Flatten all sensor lists into a single long list\n",
        "all_sensors_flat = [sensor for sensors in full_ghz_df[\"MatchedSensors\"] if isinstance(sensors, list) for sensor in sensors]\n",
        "\n",
        "# Step 2: Count frequency of each sensor\n",
        "sensor_counts = Counter(all_sensors_flat)\n",
        "\n",
        "# Step 3: Convert to DataFrame for plotting\n",
        "sensor_freq_df = pd.DataFrame(sensor_counts.items(), columns=[\"Sensor\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
        "\n",
        "# Step 4: Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(sensor_freq_df[\"Sensor\"], sensor_freq_df[\"Count\"], color='teal')\n",
        "plt.xlabel(\"Number of Hazards Matching Sensor\")\n",
        "plt.ylabel(\"Sensor Type\")\n",
        "plt.title(\"Matched Sensor Frequency Across All Hazards\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASwHDb3qgruH"
      },
      "outputs": [],
      "source": [
        "# From 'MatchedSensors', grab the sensor_weight\n",
        "sensor_weight_lookup = sensor_df.set_index(\"sensor_name\")[\"sensor_weight\"].to_dict()\n",
        "\n",
        "full_ghz_df[\"sensor_weight\"] = full_ghz_df[\"MatchedSensors\"].apply(\n",
        "    lambda sensors: sensor_weight_lookup.get(sensors[0], 1500) if sensors else 1500\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxkH8lpjgruH"
      },
      "outputs": [],
      "source": [
        "# Step 1: Choose the first suggested sensor from MatchedSensors list\n",
        "full_ghz_df[\"PrimarySensor\"] = full_ghz_df[\"MatchedSensors\"].apply(\n",
        "    lambda sensors: sensors[0] if isinstance(sensors, list) and sensors else None\n",
        ")\n",
        "\n",
        "# Step 2: Lookup the weight of the PrimarySensor\n",
        "sensor_weight_lookup = sensor_df.set_index(\"sensor_name\")[\"sensor_weight\"].to_dict()\n",
        "\n",
        "full_ghz_df[\"PrimarySensorWeight\"] = full_ghz_df[\"PrimarySensor\"].map(sensor_weight_lookup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMRqoV5FgruI"
      },
      "source": [
        "## Drone Match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nVAN3ffgruK"
      },
      "outputs": [],
      "source": [
        "# Categorize distance into buckets\n",
        "def categorize_distance(dist):\n",
        "    if dist <= 100:\n",
        "        return \"< 100m\"\n",
        "    elif dist <= 500:\n",
        "        return \"100m–500m\"\n",
        "    elif dist <= 5000:\n",
        "        return \"500m–5km\"\n",
        "    elif dist <= 50000:\n",
        "        return \"5km–50km\"\n",
        "    else:\n",
        "        return \"> 50km\"\n",
        "\n",
        "# Apply categorization\n",
        "full_ghz_df[\"distance_category\"] = full_ghz_df[\"distance\"].apply(categorize_distance)\n",
        "\n",
        "# Count each category\n",
        "distance_counts = full_ghz_df[\"distance_category\"].value_counts().sort_index()\n",
        "\n",
        "# Plot donut chart\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "wedges, texts, autotexts = ax.pie(\n",
        "    distance_counts,\n",
        "    labels=distance_counts.index,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    pctdistance=0.85,\n",
        "    wedgeprops=dict(width=0.4)\n",
        ")\n",
        "\n",
        "# Draw circle for donut hole\n",
        "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
        "fig.gca().add_artist(centre_circle)\n",
        "\n",
        "ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.title(\"Hazard Distance to Road Categories\")\n",
        "\n",
        "# Save the doughnut chart as a PNG image with high resolution\n",
        "plt.savefig(r'D:\\NDIS_Database\\distance_distribution.png', dpi=300, transparent=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYLJM7pSgruL"
      },
      "outputs": [],
      "source": [
        "# Categorize drones by max distance\n",
        "def categorize_drone_range(dist):\n",
        "    if dist <= 1000:\n",
        "        return \"< 1km\"\n",
        "    elif dist <= 5000:\n",
        "        return \"1–5km\"\n",
        "    elif dist <= 20000:\n",
        "        return \"5–20km\"\n",
        "    elif dist <= 50000:\n",
        "        return \"20–50km\"\n",
        "    else:\n",
        "        return \"> 50km\"\n",
        "\n",
        "drone_df[\"distance_category\"] = drone_df[\"distance_range\"].apply(categorize_drone_range)\n",
        "\n",
        "# Count per category\n",
        "drone_distance_counts = drone_df[\"distance_category\"].value_counts().sort_index()\n",
        "\n",
        "# Plot donut\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "wedges, texts, autotexts = ax.pie(\n",
        "    drone_distance_counts,\n",
        "    labels=drone_distance_counts.index,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    pctdistance=0.85,\n",
        "    wedgeprops=dict(width=0.4)\n",
        ")\n",
        "\n",
        "# Draw donut hole\n",
        "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
        "fig.gca().add_artist(centre_circle)\n",
        "\n",
        "ax.axis('equal')\n",
        "plt.title(\"Drone Maximum Distance Categories\")\n",
        "# Save the doughnut chart as a PNG image with high resolution\n",
        "\n",
        "plt.savefig(r'D:\\NDIS_Database\\dronedistance_distribution.png', dpi=300, transparent=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9-nUeM0gruM"
      },
      "outputs": [],
      "source": [
        "# Step 1: Select drones with distance range between 5km–20km\n",
        "cat1 = drone_df[\n",
        "    (drone_df[\"distance_range_m\"] <= 5000)\n",
        "]\n",
        "\n",
        "# Step 2: Pull out the model name and max payload weight\n",
        "cat1_summary = cat1[[\"drone_model\", \"distance_range_m\", \"payload_capacity_g\"]].sort_values(by=\"payload_capacity_g\", ascending=True)\n",
        "\n",
        "# Step 3: Show data\n",
        "cat1_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCbeYNP9gruM"
      },
      "outputs": [],
      "source": [
        "# Step 1: Select drones with distance range between 5km–20km\n",
        "mid_range_drones = drone_df[\n",
        "    (drone_df[\"distance_range\"] >= 5000) &\n",
        "    (drone_df[\"distance_range\"] <= 20000)\n",
        "]\n",
        "\n",
        "# Step 2: Pull out the model name and max payload weight\n",
        "mid_range_summary = mid_range_drones[[\"mfc_model\", \"distance_range\", \"max_payload_weight\"]].sort_values(by=\"max_payload_weight\", ascending=True)\n",
        "\n",
        "# Step 3: Show the top ones\n",
        "mid_range_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3qUXnxJgruN"
      },
      "outputs": [],
      "source": [
        "# Step 1: Select drones with distance range between 5km–20km\n",
        "cat3 = drone_df[\n",
        "    (drone_df[\"distance_range\"] >= 20000) &\n",
        "    (drone_df[\"distance_range\"] <= 50000) &\n",
        "    (drone_df[\"max_payload_weight\"] >= 800)\n",
        "]\n",
        "\n",
        "# Step 2: Pull out the model name and max payload weight\n",
        "cat3_summary = cat3[[\"mfc_model\", \"distance_range\", \"max_payload_weight\"]].sort_values(by=\"max_payload_weight\", ascending=True)\n",
        "\n",
        "# Step 3: Show the top ones\n",
        "cat3_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmvQ5GQfgruP"
      },
      "outputs": [],
      "source": [
        "def get_top_3_drones_by_payload(drones, sensor_weight):\n",
        "    candidates = []\n",
        "    for drone in drones:\n",
        "        specs = drone_specs_lookup.get(drone)\n",
        "        if specs:\n",
        "            max_payload = specs.get(\"max_payload_weight\", 0)\n",
        "            if max_payload >= sensor_weight:\n",
        "                candidates.append((drone, max_payload))\n",
        "    # Sort by how closely they match the sensor weight (lightest excess first)\n",
        "    candidates_sorted = sorted(candidates, key=lambda x: x[1])\n",
        "    return [drone for drone, _ in candidates_sorted[:3]]  # Top 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOkr6xrrgruQ"
      },
      "source": [
        "### Filtering based on Categorized Matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyFHS3B-gruR"
      },
      "outputs": [],
      "source": [
        "def flexible_filter(drones, hazard_distance):\n",
        "    result = []\n",
        "    for drone in drones:\n",
        "        dist = drone_range_lookup.get(drone, 0)\n",
        "        # Allow fallback based on tiers\n",
        "        if hazard_distance <= 100:  # almost anything\n",
        "            result.append(drone)\n",
        "        elif hazard_distance <= 500:\n",
        "            if dist >= hazard_distance * 0.5:\n",
        "                result.append(drone)\n",
        "        elif hazard_distance <= 50000:\n",
        "            if dist >= hazard_distance * 0.8:\n",
        "                result.append(drone)\n",
        "        else:  # distance > 50 km\n",
        "            # If no drone can reach, allow highest range anyway\n",
        "            if dist >= 50000 or dist == max(drone_range_lookup.values()):\n",
        "                result.append(drone)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcBGI2r7gruR"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "full_ghz_df[\"FilteredDrones\"] = full_ghz_df.apply(\n",
        "    lambda row: flexible_filter(row[\"EligibleDrones\"], row[\"distance\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZwKCDVOgruS"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "# Create new column 'Top3Drones'\n",
        "full_ghz_df[\"Top3Drones\"] = full_ghz_df.apply(\n",
        "    lambda row: get_top_3_drones_by_payload(row[\"FilteredDrones\"], row[\"PrimarySensorWeight\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(f\"✅ Top 3 drones assigned! Time: {elapsed / 60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRFXYhrFgruS"
      },
      "outputs": [],
      "source": [
        "# Optional: Lookup drone specs for top 3\n",
        "def fetch_drone_specs(drone_list):\n",
        "    return [drone_specs_lookup.get(d, {}) for d in drone_list]\n",
        "\n",
        "full_ghz_df[\"Top3Specs\"] = full_ghz_df[\"Top3Drones\"].apply(fetch_drone_specs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qOfeiCJgruU"
      },
      "outputs": [],
      "source": [
        "# Flatten all Top3Drones into one big list\n",
        "all_top_drones = [drone for drone_list in full_ghz_df[\"Top3Drones\"] if isinstance(drone_list, list) for drone in drone_list]\n",
        "\n",
        "# Count frequency\n",
        "drone_freq = Counter(all_top_drones)\n",
        "\n",
        "# Convert to DataFrame\n",
        "drone_freq_df = pd.DataFrame(drone_freq.items(), columns=[\"Drone\", \"Count\"]).sort_values(by=\"Count\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCb_ZgOHgruV"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(drone_freq_df[\"Drone\"], drone_freq_df[\"Count\"], color=\"steelblue\")\n",
        "plt.xlabel(\"Times Selected as Top-3\")\n",
        "plt.ylabel(\"Drone Model\")\n",
        "plt.title(\"Top Drones Selected Across All Geohazards\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caDbdZTzgruW"
      },
      "outputs": [],
      "source": [
        "# More flexible categorized rule for drone seleczionne\n",
        "start = timeit.default_timer()\n",
        "\n",
        "\n",
        "# Define function to assign category\n",
        "def categorize_distance(dist):\n",
        "    if dist <= 100:\n",
        "        return \"< 100m\"\n",
        "    elif dist <= 500:\n",
        "        return \"100m–500m\"\n",
        "    elif dist <= 5000:\n",
        "        return \"500m–5km\"\n",
        "    elif dist <= 50000:\n",
        "        return \"5km–50km\"\n",
        "    else:\n",
        "        return \"> 50km\"\n",
        "\n",
        "# Create distance_category\n",
        "full_ghz_df[\"distance_category\"] = full_ghz_df[\"distance\"].apply(categorize_distance)\n",
        "\n",
        "# Flexible filter function\n",
        "def filter_drones_flexible(drones, hazard_distance, category):\n",
        "    if category == \"< 100m\":\n",
        "        threshold = 1.0  # 100%\n",
        "    elif category == \"100m–500m\":\n",
        "        threshold = 0.8  # 80%\n",
        "    elif category == \"500m–5km\":\n",
        "        threshold = 0.6  # 60%\n",
        "    elif category == \"5km–50km\":\n",
        "        threshold = 0.4  # 40%\n",
        "    else:  # \"> 50km\"\n",
        "        threshold = 0.0  # any drone distance OK\n",
        "\n",
        "    good_drones = []\n",
        "    for drone in drones:\n",
        "        drone_range = drone_range_lookup.get(drone, 0)\n",
        "        if threshold == 0.0 or drone_range >= hazard_distance * threshold:\n",
        "            good_drones.append(drone)\n",
        "    return good_drones\n",
        "\n",
        "# Apply flexible filter\n",
        "full_ghz_df[\"FilteredDrones\"] = full_ghz_df.apply(\n",
        "    lambda row: filter_drones_flexible(row[\"EligibleDrones\"], row[\"distance\"], row[\"distance_category\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9uxrVVUgruX"
      },
      "outputs": [],
      "source": [
        "# Drone specs lookup\n",
        "drone_specs_lookup = drone_df.set_index(\"mfc_model\")[[\n",
        "    \"max_speed\", \"flight_time\", \"max_payload_weight\", \"distance_range\"\n",
        "]].to_dict(orient=\"index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izD7e7Q2gruX"
      },
      "outputs": [],
      "source": [
        "# Select best drone based on sensor payload compatibility (smallest excess capacity)\n",
        "def pick_best_drone_by_payload(drones, sensor_weight):\n",
        "    best_drone = None\n",
        "    best_excess = float('inf')  # Smallest extra payload capacity wins\n",
        "    for drone in drones:\n",
        "        specs = drone_specs_lookup.get(drone, None)\n",
        "        if specs:\n",
        "            payload_capacity = specs[\"max_payload_weight\"]\n",
        "            excess = payload_capacity - sensor_weight\n",
        "            if excess >= 0 and excess < best_excess:\n",
        "                best_drone = drone\n",
        "                best_excess = excess\n",
        "    if best_drone:\n",
        "        return best_drone, drone_specs_lookup[best_drone]\n",
        "    else:\n",
        "        return None, {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf1_o0uogruX"
      },
      "outputs": [],
      "source": [
        "# Define fallback mapping\n",
        "sensor_fallback_map = {\n",
        "    \"Gravimeter\": None,         # No fallback\n",
        "    \"EM_Sensor\": \"Camera\",       # Fallback to visual mapping\n",
        "    \"GPR\": \"Lidar\"               # Fallback to Lidar\n",
        "    # others don't need fallback\n",
        "}\n",
        "\n",
        "# Updated best drone picking with fallback logic\n",
        "def pick_best_drone_with_fallback(drones, primary_sensor, sensor_weight):\n",
        "    # First attempt: match primary sensor\n",
        "    best_drone = None\n",
        "    best_excess = float('inf')\n",
        "    for drone in drones:\n",
        "        specs = drone_specs_lookup.get(drone, None)\n",
        "        if specs:\n",
        "            payload_capacity = specs[\"max_payload_weight\"]\n",
        "            excess = payload_capacity - sensor_weight\n",
        "            if excess >= 0 and excess < best_excess:\n",
        "                best_drone = drone\n",
        "                best_excess = excess\n",
        "\n",
        "    # If primary fails, fallback to lighter sensor\n",
        "    if best_drone is None:\n",
        "        fallback_sensor = sensor_fallback_map.get(primary_sensor, None)\n",
        "        if fallback_sensor:\n",
        "            fallback_weight = sensor_df.loc[sensor_df[\"sensor_name\"] == fallback_sensor, \"sensor_weight\"].values[0]\n",
        "            for drone in drones:\n",
        "                specs = drone_specs_lookup.get(drone, None)\n",
        "                if specs:\n",
        "                    payload_capacity = specs[\"max_payload_weight\"]\n",
        "                    excess = payload_capacity - fallback_weight\n",
        "                    if excess >= 0 and excess < best_excess:\n",
        "                        best_drone = drone\n",
        "                        best_excess = excess\n",
        "            return fallback_sensor, best_drone, drone_specs_lookup[best_drone] if best_drone else None\n",
        "        else:\n",
        "            return None, None, None  # No fallback and no drone\n",
        "    else:\n",
        "        return primary_sensor, best_drone, drone_specs_lookup[best_drone]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mmvzLMVgruY"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "# Step 1: Prioritize payload match first\n",
        "def pick_best_drone_by_payload_only(drones, required_payload):\n",
        "    best_drone = None\n",
        "    min_excess_payload = float('inf')\n",
        "    for drone in drones:\n",
        "        specs = drone_specs_lookup.get(drone, None)\n",
        "        if specs:\n",
        "            max_payload = specs[\"max_payload_weight\"]\n",
        "            excess = max_payload - required_payload\n",
        "            if excess >= 0 and excess < min_excess_payload:\n",
        "                best_drone = drone\n",
        "                min_excess_payload = excess\n",
        "    return best_drone, drone_specs_lookup.get(best_drone, None) if best_drone else (None, None)\n",
        "\n",
        "# Step 2: Apply it\n",
        "full_ghz_df[[\"BestDrone\", \"BestDroneSpecs\"]] = full_ghz_df.apply(\n",
        "    lambda row: pd.Series(\n",
        "        pick_best_drone_by_payload_only(row[\"FilteredDrones\"], row[\"PrimarySensorWeight\"])\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTsTgL_KgruY"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "# Split into two columns: BestDrone and BestDroneSpecs\n",
        "full_ghz_df[[\"SelectedSensor\", \"BestDrone\", \"BestDroneSpecs\"]] = full_ghz_df.apply(\n",
        "    lambda row: pd.Series(\n",
        "        pick_best_drone_with_fallback(row[\"FilteredDrones\"], row[\"PrimarySensor\"], row[\"PrimarySensorWeight\"])\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKU3ljlSgruZ"
      },
      "outputs": [],
      "source": [
        "full_ghz_df[\"FilteredDrones\"].apply(len).hist(bins=20)\n",
        "plt.title(\"Number of Available Drones per Hazard (after flexible filtering)\")\n",
        "plt.xlabel(\"Available Drones\")\n",
        "plt.ylabel(\"Hazard Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjD0CNSygruZ"
      },
      "source": [
        "# Step 5: Travel Time + CPM Total Time Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tte8T0ejgrua"
      },
      "outputs": [],
      "source": [
        "# Step 1: Pull speed from BestDroneSpecs\n",
        "def extract_speed(specs):\n",
        "    if isinstance(specs, dict):\n",
        "        return specs.get(\"max_speed\", None)\n",
        "    return None\n",
        "\n",
        "full_ghz_df[\"DroneSpeed_mps\"] = full_ghz_df[\"BestDroneSpecs\"].apply(extract_speed)\n",
        "\n",
        "# Step 2: Calculate travel_time (minutes)\n",
        "# Note: Distance is assumed to be in meters, speed in meters/second\n",
        "full_ghz_df[\"travel_time\"] = (full_ghz_df[\"distance\"] / full_ghz_df[\"DroneSpeed_mps\"]) / 60\n",
        "\n",
        "# Step 3: Calculate cpm_total_time\n",
        "full_ghz_df[\"cpm_total_time\"] = (2 * full_ghz_df[\"travel_time\"]) + full_ghz_df[\"monitor_time\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w07tjazSgrua"
      },
      "outputs": [],
      "source": [
        "full_ghz_df.distance.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIvfhSGPgrub"
      },
      "outputs": [],
      "source": [
        "# Distance Histogram\n",
        "full_ghz_df[\"distance\"].hist(bins=100, figsize=(10,5))\n",
        "plt.xlabel(\"Distance to Road (meters)\")\n",
        "plt.ylabel(\"Count of Hazards\")\n",
        "plt.title(\"Hazard Distance to Road Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsotpJbsgrub"
      },
      "outputs": [],
      "source": [
        "# Drone Range Distribution\n",
        "drone_df[\"distance_range\"].hist(bins=20, figsize=(10,5))\n",
        "plt.xlabel(\"Drone Distance Range (meters)\")\n",
        "plt.ylabel(\"Number of Drones\")\n",
        "plt.title(\"Drone Max Distance Range Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwjqCvUUgruc"
      },
      "outputs": [],
      "source": [
        "# More flexible categorized rule for drone seleczionne\n",
        "start = timeit.default_timer()\n",
        "\n",
        "\n",
        "# Define function to assign category\n",
        "def categorize_distance(dist):\n",
        "    if dist <= 100:\n",
        "        return \"< 100m\"\n",
        "    elif dist <= 500:\n",
        "        return \"100m–500m\"\n",
        "    elif dist <= 5000:\n",
        "        return \"500m–5km\"\n",
        "    elif dist <= 50000:\n",
        "        return \"5km–50km\"\n",
        "    else:\n",
        "        return \"> 50km\"\n",
        "\n",
        "# Create distance_category\n",
        "full_ghz_df[\"distance_category\"] = full_ghz_df[\"distance\"].apply(categorize_distance)\n",
        "\n",
        "# Flexible filter function\n",
        "def filter_drones_flexible(drones, hazard_distance, category):\n",
        "    if category == \"< 100m\":\n",
        "        threshold = 1.0  # 100%\n",
        "    elif category == \"100m–500m\":\n",
        "        threshold = 0.8  # 80%\n",
        "    elif category == \"500m–5km\":\n",
        "        threshold = 0.6  # 60%\n",
        "    elif category == \"5km–50km\":\n",
        "        threshold = 0.4  # 40%\n",
        "    else:  # \"> 50km\"\n",
        "        threshold = 0.0  # any drone distance OK\n",
        "\n",
        "    good_drones = []\n",
        "    for drone in drones:\n",
        "        drone_range = drone_range_lookup.get(drone, 0)\n",
        "        if threshold == 0.0 or drone_range >= hazard_distance * threshold:\n",
        "            good_drones.append(drone)\n",
        "    return good_drones\n",
        "\n",
        "# Apply flexible filter\n",
        "full_ghz_df[\"FilteredDrones\"] = full_ghz_df.apply(\n",
        "    lambda row: filter_drones_flexible(row[\"EligibleDrones\"], row[\"distance\"], row[\"distance_category\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8M3w47-grud"
      },
      "outputs": [],
      "source": [
        "# Function based on balanced-fit\n",
        "def get_top_3_drones_by_payload_diverse(drones, sensor_weight):\n",
        "    eligible = []\n",
        "    for drone in drones:\n",
        "        specs = drone_specs_lookup.get(drone)\n",
        "        if specs and specs.get(\"max_payload_weight\", 0) >= sensor_weight:\n",
        "            eligible.append((drone, specs[\"max_payload_weight\"]))\n",
        "\n",
        "    eligible = sorted(eligible, key=lambda x: abs(x[1] - sensor_weight))\n",
        "    return [d[0] for d in eligible[:3]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNosGqOogrud"
      },
      "outputs": [],
      "source": [
        "# Apply the diverse top-3 drone selector\n",
        "start = timeit.default_timer()\n",
        "\n",
        "full_ghz_df[\"Top3Drones\"] = full_ghz_df.apply(\n",
        "    lambda row: get_top_3_drones_by_payload_diverse(row[\"FilteredDrones\"], row[\"PrimarySensorWeight\"]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Optional: Get full specs for Top3\n",
        "full_ghz_df[\"Top3Specs\"] = full_ghz_df[\"Top3Drones\"].apply(fetch_drone_specs)\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(f\"✅ Diverse Top-3 drones assigned! Time: {elapsed / 60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K7k3uJqgrud"
      },
      "outputs": [],
      "source": [
        "# Flatten all Top3Drones into one big list\n",
        "all_top_drones = [drone for drone_list in full_ghz_df[\"Top3Drones\"] if isinstance(drone_list, list) for drone in drone_list]\n",
        "\n",
        "# Count frequency\n",
        "drone_freq = Counter(all_top_drones)\n",
        "\n",
        "# Convert to DataFrame\n",
        "drone_freq_df = pd.DataFrame(drone_freq.items(), columns=[\"Drone\", \"Count\"]).sort_values(by=\"Count\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZfl8BV-grue"
      },
      "outputs": [],
      "source": [
        "# See Top 3 drone list\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(drone_freq_df[\"Drone\"], drone_freq_df[\"Count\"])\n",
        "plt.xlabel(\"Times Selected as Top-3\")\n",
        "plt.ylabel(\"Drone Model\")\n",
        "plt.title(\"Drone Distribution Using Balanced-Fit Payload Logic\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9yeaRvGgrue"
      },
      "outputs": [],
      "source": [
        "print(\"Unique Top-3 Drones:\", full_ghz_df[\"Top3Drones\"].explode().nunique())\n",
        "print(full_ghz_df[\"Top3Drones\"].explode().value_counts().head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UuX2uwagrue"
      },
      "outputs": [],
      "source": [
        "sample_row = full_ghz_df.iloc[0]  # or another random row\n",
        "print(\"Filtered Drones:\", sample_row[\"FilteredDrones\"])\n",
        "print(\"Sensor Weight:\", sample_row[\"PrimarySensorWeight\"])\n",
        "print(\"Top 3 Drones:\", sample_row[\"Top3Drones\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIm-jLw7gruf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBV0aTjNgruf"
      },
      "outputs": [],
      "source": [
        "def get_top_3_drones_payload_soft(drones, sensor_weight):\n",
        "    eligible = []\n",
        "    for drone in drones:\n",
        "        specs = drone_specs_lookup.get(drone)\n",
        "        if specs:\n",
        "            max_payload = specs.get(\"max_payload_weight\", 0)\n",
        "            # Allow drones with up to 3.5x sensor weight\n",
        "            if sensor_weight <= max_payload <= sensor_weight * 3.5:\n",
        "                eligible.append((drone, max_payload))\n",
        "\n",
        "    # Sort by closeness to the sensor weight (optional)\n",
        "    eligible.sort(key=lambda x: abs(x[1] - sensor_weight))\n",
        "\n",
        "    # If not enough drones, relax constraint even further\n",
        "    if len(eligible) < 3:\n",
        "        for drone in drones:\n",
        "            specs = drone_specs_lookup.get(drone)\n",
        "            if specs:\n",
        "                eligible.append((drone, specs.get(\"max_payload_weight\", 0)))\n",
        "        eligible = sorted(list(set(eligible)), key=lambda x: abs(x[1] - sensor_weight))\n",
        "\n",
        "    return [d[0] for d in eligible[:3]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4mYemYYgruf"
      },
      "outputs": [],
      "source": [
        "full_ghz_df[\"Top3Drones\"] = full_ghz_df.apply(\n",
        "    lambda row: get_top_3_drones_payload_soft(row[\"FilteredDrones\"], row[\"PrimarySensorWeight\"]),\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqKsQ3yugrug"
      },
      "outputs": [],
      "source": [
        "# Step 1: Flatten Top-3 Drones into a list\n",
        "all_top_drones = [\n",
        "    drone for drone_list in full_ghz_df[\"Top3Drones\"]\n",
        "    if isinstance(drone_list, list)\n",
        "    for drone in drone_list\n",
        "]\n",
        "\n",
        "# Step 2: Count the frequency of each drone\n",
        "drone_freq = Counter(all_top_drones)\n",
        "\n",
        "# Step 3: Build a DataFrame for plotting\n",
        "scatter_data = []\n",
        "for drone, count in drone_freq.items():\n",
        "    specs = drone_specs_lookup.get(drone, None)\n",
        "    if specs:\n",
        "        scatter_data.append({\n",
        "            \"Drone\": drone,\n",
        "            \"Payload_kg\": specs.get(\"max_payload_weight\", 0) / 1000,  # convert grams to kg if needed\n",
        "            \"Range_m\": specs.get(\"distance_range\", 0),\n",
        "            \"Count\": count\n",
        "        })\n",
        "\n",
        "scatter_df = pd.DataFrame(scatter_data)\n",
        "\n",
        "# Step 4: Plot\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.scatter(\n",
        "    scatter_df[\"Payload_kg\"],\n",
        "    scatter_df[\"Range_m\"],\n",
        "    s=scatter_df[\"Count\"] / 100,  # scale bubble size down a bit\n",
        "    alpha=0.7,\n",
        "    color='dodgerblue',\n",
        "    edgecolors='black'\n",
        ")\n",
        "\n",
        "for idx, row in scatter_df.iterrows():\n",
        "    plt.text(row[\"Payload_kg\"]+0.2, row[\"Range_m\"], row[\"Drone\"], fontsize=8)\n",
        "\n",
        "plt.title(\"Top-Selected Drones: Payload vs Distance Range\", fontsize=16)\n",
        "plt.xlabel(\"Payload Capacity (kg)\", fontsize=14)\n",
        "plt.ylabel(\"Distance Range (meters)\", fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAMP70yTgruh"
      },
      "outputs": [],
      "source": [
        "def score_drones_for_row(row, drone_df, w_distance=0.5, w_payload=0.4, w_penalty=0.3):\n",
        "    hazard_distance = row['distance']  # already in meters\n",
        "    sensor_weight_kg = row['sensor_weight'] / 1000.0  # convert grams → kg\n",
        "\n",
        "    best_drones = []\n",
        "    for _, drone in drone_df.iterrows():\n",
        "        d_model = drone['drone_model']\n",
        "        d_payload = drone['payload_capacity_kg']\n",
        "        d_range = drone['distance_range_m']\n",
        "\n",
        "        if pd.isna(d_payload) or pd.isna(d_range):\n",
        "            continue\n",
        "\n",
        "        # Convert drone payload to kg if needed\n",
        "        d_payload_kg = d_payload / 1000.0\n",
        "\n",
        "        # Matching scores\n",
        "        distance_score = max(0, 1 - abs(hazard_distance - d_range) / hazard_distance)\n",
        "        payload_score = 1 if d_payload_kg >= sensor_weight_kg else d_payload_kg / sensor_weight_kg\n",
        "\n",
        "        # Overkill penalty\n",
        "        payload_over = max(0, d_payload_kg - sensor_weight_kg)\n",
        "        distance_over = max(0, d_range - hazard_distance)\n",
        "        overkill_penalty = (payload_over / sensor_weight_kg + distance_over / hazard_distance) / 2\n",
        "\n",
        "        score = (\n",
        "            w_distance * distance_score +\n",
        "            w_payload * payload_score -\n",
        "            w_penalty * overkill_penalty\n",
        "        )\n",
        "\n",
        "        best_drones.append({\n",
        "            'drone_model': d_model,\n",
        "            'distance_score': distance_score,\n",
        "            'payload_score': payload_score,\n",
        "            'overkill_penalty': overkill_penalty,\n",
        "            'final_score': score,\n",
        "            'payload_capacity_kg': d_payload_kg,\n",
        "            'distance_range_m': d_range\n",
        "        })\n",
        "\n",
        "    if not best_drones:\n",
        "        return pd.Series({'BestDrone': None, 'BestDroneSpecs': None, 'Top3Drones': None, 'Top3Specs': None})\n",
        "\n",
        "    sorted_drones = sorted(best_drones, key=lambda x: x['final_score'], reverse=True)\n",
        "    top3 = sorted_drones[:3]\n",
        "    return pd.Series({\n",
        "        'BestDrone': top3[0]['drone_model'],\n",
        "        'BestDroneSpecs': top3[0],\n",
        "        'Top3Drones': ', '.join([d['drone_model'] for d in top3]),\n",
        "        'Top3Specs': str(top3)\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLI0Doy5gruh"
      },
      "outputs": [],
      "source": [
        "scored_cols = full_ghz_df.apply(lambda row: score_drones_for_row(row, drone_df), axis=1)\n",
        "full_ghz_df[['BestDrone', 'BestDroneSpecs', 'Top3Drones', 'Top3Specs']] = scored_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5OlSwM0gruh"
      },
      "outputs": [],
      "source": [
        "def cleanup_python_temp_files():\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "    patterns = ('joblib', 'tmp', '__MEI', 'pip-', 'matplotlib', 'MEI', 'sklearn')\n",
        "\n",
        "    for item in os.listdir(temp_dir):\n",
        "        item_path = os.path.join(temp_dir, item)\n",
        "        if any(p in item for p in patterns):\n",
        "            try:\n",
        "                if os.path.isdir(item_path):\n",
        "                    shutil.rmtree(item_path, ignore_errors=True)\n",
        "                elif os.path.isfile(item_path):\n",
        "                    os.remove(item_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Skipped {item_path}: {e}\")\n",
        "\n",
        "def chunked_score_and_merge(full_df, drone_df, chunk_size=50000):\n",
        "    drone_df = drone_df.rename(columns={\n",
        "        'mfc_model': 'drone_model',\n",
        "        'max_payload_weight': 'payload_capacity_g',\n",
        "        'distance_range': 'distance_range_m'\n",
        "    })\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for start in range(0, len(full_df), chunk_size):\n",
        "        print(f\"Processing chunk {start} to {start+chunk_size}...\")\n",
        "        chunk = full_df.iloc[start:start + chunk_size].copy()\n",
        "        chunk['key'] = 1\n",
        "        drone_df['key'] = 1\n",
        "\n",
        "        merged = chunk.merge(drone_df, on='key').drop('key', axis=1)\n",
        "\n",
        "        # Score calculation\n",
        "        hazard_distance = merged['distance']\n",
        "        sensor_weight = merged['sensor_weight']\n",
        "        d_payload = merged['payload_capacity_g']\n",
        "        d_range = merged['distance_range_m']\n",
        "\n",
        "        distance_score = (1 - (abs(hazard_distance - d_range) / hazard_distance)).clip(lower=0)\n",
        "        payload_score = np.where(d_payload >= sensor_weight, 1, d_payload / sensor_weight)\n",
        "        payload_over = np.maximum(0, d_payload - sensor_weight)\n",
        "        distance_over = np.maximum(0, d_range - hazard_distance)\n",
        "        overkill_penalty = (payload_over / sensor_weight + distance_over / hazard_distance) / 2\n",
        "\n",
        "        w_distance = 0.5\n",
        "        w_payload = 0.4\n",
        "        w_penalty = 0.3\n",
        "        final_score = (w_distance * distance_score) + (w_payload * payload_score) - (w_penalty * overkill_penalty)\n",
        "\n",
        "        merged['final_score'] = final_score\n",
        "        merged['drone_info'] = merged[['drone_model', 'distance_range_m', 'payload_capacity_g']].to_dict(orient='records')\n",
        "\n",
        "        top3 = (\n",
        "            merged.sort_values(['HazardID', 'final_score'], ascending=[True, False])\n",
        "            .groupby('HazardID')\n",
        "            .head(3)\n",
        "        )\n",
        "\n",
        "        # Aggregate per HazardID\n",
        "        def agg(group):\n",
        "            sorted_g = group.sort_values('final_score', ascending=False)\n",
        "            return pd.Series({\n",
        "                'BestDrone': sorted_g.iloc[0]['drone_model'],\n",
        "                'BestDroneSpecs': sorted_g.iloc[0]['drone_info'],\n",
        "                'Top3Drones': ', '.join(sorted_g['drone_model'].values),\n",
        "                'Top3Specs': str(list(sorted_g['drone_info'].values))\n",
        "            })\n",
        "\n",
        "        result = top3.groupby('HazardID').apply(agg).reset_index()\n",
        "        all_results.append(result)\n",
        "\n",
        "        # Cleanup RAM and temp\n",
        "        del merged, top3, result\n",
        "        gc.collect()\n",
        "        cleanup_python_temp_files()\n",
        "\n",
        "    return pd.concat(all_results, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qoIlVC4grui"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "# Use it\n",
        "top3_final = chunked_score_and_merge(full_ghz_df, drone_df, chunk_size=50000)\n",
        "full_ghz_df = full_ghz_df.merge(top3_final, on='HazardID', how='left')\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(f\"✅ Diverse Top-3 drones assigned! Time: {elapsed / 60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLty63n9gruj"
      },
      "outputs": [],
      "source": [
        "# Split Top3Drones column into separate entries\n",
        "top3_flat = full_ghz_df['Top3Drones_y'].dropna().str.split(', ').explode()\n",
        "\n",
        "top3_counts = top3_flat.value_counts()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "top3_counts.head(20).plot(kind='bar', color='skyblue')\n",
        "plt.title('Top 3 Drone Appearances Across Missions')\n",
        "plt.ylabel('Number of Appearances')\n",
        "plt.xlabel('Drone Model')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsEBc4cZgruj"
      },
      "outputs": [],
      "source": [
        "pivot = (\n",
        "    full_ghz_df\n",
        "    .groupby(['HazardType', 'BestDrone_y'])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "pivot.plot(kind='bar', stacked=True, figsize=(14,7))\n",
        "plt.title('Best Drone Distribution per Hazard Type')\n",
        "plt.ylabel('Number of Missions')\n",
        "plt.xlabel('Hazard Type')\n",
        "plt.legend(title='Best Drone', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZY3Wttjgrum"
      },
      "outputs": [],
      "source": [
        "full_ghz_df = full_ghz_df.drop(columns=[\n",
        "    'MatchedSensors', 'EligibleDrones', 'FilteredDrones',\n",
        "    'BestDrone_x', 'BestDroneSpecs_x', 'Top3Drones_x', 'Top3Specs_x',\n",
        "    'BestDrone_y', 'BestDroneSpecs_y', 'Top3Drones_y', 'Top3Specs_y',\n",
        "    'key', 'PrimarySensor', 'PrimarySensorWeight', 'SelectedSensor'\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P43oDdH0grun"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yQSlKrdgrun"
      },
      "source": [
        "# STAGGERED DECISION VERSION 2 - BALANCED DRONE BEST FIT WITH MATCHED TOP 3 SENSORS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svvkWTajgrun"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pNNFFlxgrun"
      },
      "source": [
        "# STEP 2: Top 3 Sensor Lookup (with Lidar substituted) --> Based on Scientometrics Method done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9PE5F3Kgruo"
      },
      "outputs": [],
      "source": [
        "top3_sensor_lookup = {\n",
        "    'Earthquake': ['Seismic', 'Lidar', 'GPS'],\n",
        "    'Volcano': ['Seismic', 'Lidar', 'Multispectral'],\n",
        "    'Tsunami': ['Seismic', 'GPS', 'Multispectral'],\n",
        "    'Landslide': ['Lidar', 'Seismic', 'Multispectral'],\n",
        "    'Fault': ['Seismic', 'EM_Sensor', 'Gravimeter']\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j43xbp8Vgrup"
      },
      "outputs": [],
      "source": [
        "missing_hazards = full_ghz_df[full_ghz_df['Top3Sensors'].isna()]['HazardType'].unique()\n",
        "print(\"Missing hazard types:\", missing_hazards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp7sC-UNgruq"
      },
      "source": [
        "# STEP 3: Define the scoring function for Matching Sensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r1zIriOgruq"
      },
      "outputs": [],
      "source": [
        "# Rebuild normalized_df from scientometrics counts\n",
        "method_df = pd.DataFrame({\n",
        "    'Geophysical Methods': [\n",
        "        'Seismic', 'Radar', 'Remote Sensing', 'GPS', 'Imagery',\n",
        "        'Electrical Resistivity', 'Gravity', 'Thermal', 'Electromagnetic', 'Magnetic'\n",
        "    ],\n",
        "    'Earthquake': [27136, 1762, 653, 712, 0, 0, 0, 0, 0, 0],\n",
        "    'Volcano': [2493, 488, 375, 123, 173, 146, 106, 102, 88, 44],\n",
        "    'Tsunami': [1137, 52, 58, 149, 0, 0, 0, 0, 0, 0],\n",
        "    'Landslide': [756, 813, 673, 61, 468, 440, 0, 0, 31, 0],\n",
        "    'Fault': [15686, 310, 363, 317, 0, 791, 461, 0, 343, 205],\n",
        "    'Nuclear': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  # Placeholder if needed\n",
        "})\n",
        "\n",
        "normalized_df = method_df.copy()\n",
        "for col in method_df.columns[1:]:\n",
        "    max_val = method_df[col].max()\n",
        "    normalized_df[col] = method_df[col] / max_val if max_val else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F-vvf5cgrur"
      },
      "outputs": [],
      "source": [
        "# sensor_map must already exist\n",
        "sensor_map = {\n",
        "    'Seismic': 'Seismic',\n",
        "    'Radar': 'Radarpod',\n",
        "    'Remote Sensing': 'Multispectral',\n",
        "    'GPS': 'GPS',\n",
        "    'Imagery': 'Camera',\n",
        "    'Electrical Resistivity': 'EM_Sensor',\n",
        "    'Gravity': 'Gravimeter',\n",
        "    'Thermal': 'Thermal_Camera',\n",
        "    'Electromagnetic': 'EM_Sensor',\n",
        "    'Magnetic': 'Magnetometers'\n",
        "}\n",
        "\n",
        "# Build (HazardType, Sensor) → Normalized Score from normalized_df\n",
        "sensor_weight_lookup = {}\n",
        "\n",
        "for _, row in normalized_df.iterrows():\n",
        "    method = row['Geophysical Methods']\n",
        "    sensor = sensor_map.get(method)\n",
        "    if sensor:\n",
        "        for hazard in normalized_df.columns[1:]:\n",
        "            sensor_weight_lookup[(hazard, sensor)] = row[hazard]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6MI24Wqgrur"
      },
      "outputs": [],
      "source": [
        "# Safe Top3 generator using SurveyOptions + scientometric weights\n",
        "def generate_top3sensors_stage_aware(row):\n",
        "    hazard = row['HazardType']\n",
        "    options = row['SurveyOptions']\n",
        "\n",
        "    if not isinstance(options, list) or len(options) == 0:\n",
        "        return []\n",
        "\n",
        "    scores = []\n",
        "    for sensor in options:\n",
        "        score = sensor_weight_lookup.get((hazard, sensor), 0)\n",
        "        scores.append((sensor, score))\n",
        "\n",
        "    top3 = [s[0] for s in sorted(scores, key=lambda x: x[1], reverse=True)[:3]]\n",
        "    return top3\n",
        "\n",
        "# Apply again\n",
        "full_ghz_df['Top3Sensors'] = full_ghz_df.apply(generate_top3sensors_stage_aware, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB_Rqdvdgrus"
      },
      "outputs": [],
      "source": [
        "# Fast, chunked, vectorized staggered drone scorer per Top3Sensors\n",
        "def batch_staggered_score(full_ghz_df, drone_df, sensor_df, chunk_size=50000):\n",
        "    drone_df = drone_df[['mfc_model', 'max_payload_weight', 'distance_range']].dropna()\n",
        "    sensor_df = sensor_df[['sensor_name', 'sensor_weight']].dropna()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for start in range(0, len(full_ghz_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(full_ghz_df))\n",
        "        chunk = full_ghz_df.iloc[start:start+chunk_size].copy().reset_index(drop=True)\n",
        "        print(f\"\\U0001f373 Processing chunk {start} to {end}...\")  # <-- Sanity check line\n",
        "\n",
        "        for sensor_col in ['s1', 's2', 's3']:\n",
        "            sensor_array = chunk['Top3Sensors'].to_numpy(object)\n",
        "            sensor_names = pd.Series([row[sensor_col_idx(sensor_col)] if isinstance(row, list) and len(row) > sensor_col_idx(sensor_col) else None for row in sensor_array], index=chunk.index)\n",
        "            chunk[f'BestDrone_{sensor_col}'] = None\n",
        "\n",
        "            for sensor_name in sensor_names.dropna().unique():\n",
        "                sensor_weight = sensor_df[sensor_df['sensor_name'] == sensor_name]['sensor_weight'].values\n",
        "                if len(sensor_weight) == 0:\n",
        "                    continue\n",
        "                sensor_weight = sensor_weight[0]\n",
        "\n",
        "                # Subset of chunk using this sensor\n",
        "                sub_idx = sensor_names[sensor_names == sensor_name].index\n",
        "                sub_chunk = chunk.loc[sub_idx].copy()\n",
        "\n",
        "                # Cross-join sub_chunk × drones\n",
        "                sub_chunk['key'] = 1\n",
        "                drone_df['key'] = 1\n",
        "                merged = sub_chunk.merge(drone_df, on='key').drop(columns='key')\n",
        "\n",
        "                h_dist = merged['distance']\n",
        "                d_range = merged['distance_range']\n",
        "                d_payload = merged['max_payload_weight']\n",
        "\n",
        "                dist_score = (1 - abs(h_dist - d_range) / h_dist).clip(lower=0)\n",
        "                payload_score = np.where(d_payload >= sensor_weight, 1, d_payload / sensor_weight)\n",
        "                overkill_penalty = (np.maximum(0, d_payload - sensor_weight) / sensor_weight + np.maximum(0, d_range - h_dist) / h_dist) / 2\n",
        "                tight_fit_bonus = np.where(d_payload >= sensor_weight, 1 - ((d_payload - sensor_weight) / d_payload), 0)\n",
        "\n",
        "                score = 0.5 * dist_score + 0.4 * payload_score - 0.3 * overkill_penalty + 0.1 * tight_fit_bonus\n",
        "                merged['final_score'] = score\n",
        "\n",
        "                # Get best drone per HazardID\n",
        "                best_per_hazard = (\n",
        "                    merged.sort_values(['HazardID', 'final_score'], ascending=[True, False])\n",
        "                    .groupby('HazardID').first()\n",
        "                )\n",
        "\n",
        "                # Map back best drone names to the main chunk\n",
        "                # Join back using HazardID\n",
        "                best_drone_map = best_per_hazard[['mfc_model']].reset_index()\n",
        "\n",
        "                colname = f'BestDrone_{sensor_col}'\n",
        "                if colname in chunk.columns:\n",
        "                    chunk = chunk.drop(columns=[colname])\n",
        "                chunk = chunk.merge(best_drone_map, on='HazardID', how='left')\n",
        "                chunk.rename(columns={'mfc_model': colname}, inplace=True)\n",
        "\n",
        "        results.append(chunk)\n",
        "\n",
        "    return pd.concat(results, ignore_index=True)\n",
        "\n",
        "def sensor_col_idx(col):\n",
        "    return {'s1': 0, 's2': 1, 's3': 2}[col]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gwzX91Xgrus"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "full_ghz_df = batch_staggered_score(full_ghz_df, drone_df, sensor_df, chunk_size=50000)\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(f\"✅ Diverse Top-3 drones assigned! Time: {elapsed / 60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njt_k7VYgrut"
      },
      "outputs": [],
      "source": [
        "full_ghz_df['BestDrone_s1'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCxObTvGgruu"
      },
      "outputs": [],
      "source": [
        "def batch_staggered_score(full_ghz_df, drone_df, sensor_df, chunk_size=50000):\n",
        "    drone_df = drone_df[['mfc_model', 'max_payload_weight', 'distance_range']].dropna()\n",
        "    sensor_df = sensor_df[['sensor_name', 'sensor_weight']].dropna()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    def filter_drones_flexible(drones, hazard_distance, category):\n",
        "        if category == \"< 100m\":\n",
        "            threshold = 1.0\n",
        "        elif category == \"100m–500m\":\n",
        "            threshold = 0.8\n",
        "        elif category == \"500m–5km\":\n",
        "            threshold = 0.6\n",
        "        elif category == \"5km–50km\":\n",
        "            threshold = 0.4\n",
        "        else:\n",
        "            threshold = 0.0\n",
        "        return drones[(drones['distance_range'] >= hazard_distance * threshold)]\n",
        "\n",
        "    for start in range(0, len(full_ghz_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(full_ghz_df))\n",
        "        print(f\"\\U0001f373 Processing chunk {start} to {end}...\")\n",
        "        chunk = full_ghz_df.iloc[start:end].copy().reset_index(drop=True)\n",
        "\n",
        "        for sensor_col in ['s1', 's2', 's3']:\n",
        "            sensor_array = chunk['Top3Sensors'].to_numpy(object)\n",
        "            sensor_names = pd.Series(\n",
        "                [row[sensor_col_idx(sensor_col)] if isinstance(row, list) and len(row) > sensor_col_idx(sensor_col) else None for row in sensor_array],\n",
        "                index=chunk.index\n",
        "            )\n",
        "            chunk[f'BestDrone_{sensor_col}'] = None\n",
        "\n",
        "            for sensor_name in sensor_names.dropna().unique():\n",
        "                sensor_weight = sensor_df[sensor_df['sensor_name'] == sensor_name]['sensor_weight'].values\n",
        "                if len(sensor_weight) == 0:\n",
        "                    continue\n",
        "                sensor_weight = sensor_weight[0]\n",
        "\n",
        "                sub_idx = sensor_names[sensor_names == sensor_name].index\n",
        "                sub_chunk = chunk.loc[sub_idx].copy()\n",
        "                hazard_dist = sub_chunk['distance']\n",
        "                hazard_cat = sub_chunk['distance_category']\n",
        "\n",
        "                sub_chunk['key'] = 1\n",
        "                drone_df['key'] = 1\n",
        "                merged = sub_chunk.merge(drone_df, on='key').drop(columns='key')\n",
        "\n",
        "                h_dist = merged['distance']\n",
        "                d_range = merged['distance_range']\n",
        "                d_payload = merged['max_payload_weight']\n",
        "\n",
        "                dist_score = (1 - abs(h_dist - d_range) / h_dist).clip(lower=0)\n",
        "                payload_score = np.where(d_payload >= sensor_weight, 1, d_payload / sensor_weight)\n",
        "                overkill_penalty = (\n",
        "                    (np.maximum(0, d_payload - sensor_weight) / sensor_weight +\n",
        "                     np.maximum(0, d_range - h_dist) / h_dist) / 2\n",
        "                )\n",
        "                tight_fit_bonus = np.where(d_payload >= sensor_weight,\n",
        "                                           1 - ((d_payload - sensor_weight) / d_payload), 0)\n",
        "\n",
        "                score = (0.5 * dist_score +\n",
        "                         0.4 * payload_score -\n",
        "                         0.3 * overkill_penalty +\n",
        "                         0.1 * tight_fit_bonus)\n",
        "\n",
        "                merged['final_score'] = score\n",
        "\n",
        "                if merged.empty:\n",
        "                    fallback_drone = (\n",
        "                        filter_drones_flexible(drone_df, hazard_dist.median(), hazard_cat.mode()[0])\n",
        "                        .assign(score=np.abs(drone_df['max_payload_weight'] - sensor_weight))\n",
        "                        .sort_values('score')\n",
        "                    )\n",
        "                    fallback = fallback_drone.iloc[0]['mfc_model'] if not fallback_drone.empty else None\n",
        "                    chunk.loc[sub_idx, f'BestDrone_{sensor_col}'] = fallback\n",
        "                    continue\n",
        "\n",
        "                best_per_hazard = (\n",
        "                    merged.sort_values(['HazardID', 'final_score'], ascending=[True, False])\n",
        "                    .groupby('HazardID').first()\n",
        "                )\n",
        "\n",
        "                best_drone_map = best_per_hazard[['mfc_model']].reset_index()\n",
        "                colname = f'BestDrone_{sensor_col}'\n",
        "                if colname in chunk.columns:\n",
        "                    chunk = chunk.drop(columns=[colname])\n",
        "                chunk = chunk.merge(best_drone_map, on='HazardID', how='left')\n",
        "                chunk.rename(columns={'mfc_model': colname}, inplace=True)\n",
        "\n",
        "        results.append(chunk)\n",
        "\n",
        "    return pd.concat(results, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "837dAW-ogruu"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "full_ghz_df = batch_staggered_score(full_ghz_df, drone_df, sensor_df, chunk_size=50000)\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(f\"\\u2705 Diverse Top-3 drones assigned! Time: {elapsed / 60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAoe-i_ngrux"
      },
      "outputs": [],
      "source": [
        "full_ghz_df['BestDrone_s1'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz1qGQNrgrux"
      },
      "outputs": [],
      "source": [
        "# Count each category\n",
        "distance_counts = full_ghz_df[\"distance_category\"].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1LeV_eNgruy"
      },
      "outputs": [],
      "source": [
        "# Categorize drones by max distance\n",
        "def categorize_drone_range(dist):\n",
        "    if dist <= 1000:\n",
        "        return \"< 1km\"\n",
        "    elif dist <= 5000:\n",
        "        return \"1–5km\"\n",
        "    elif dist <= 20000:\n",
        "        return \"5–20km\"\n",
        "    elif dist <= 50000:\n",
        "        return \"20–50km\"\n",
        "    else:\n",
        "        return \"> 50km\"\n",
        "\n",
        "drone_df[\"distance_category\"] = drone_df[\"distance_range\"].apply(categorize_drone_range)\n",
        "\n",
        "# Count per category\n",
        "drone_distance_counts = drone_df[\"distance_category\"].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06jfEdNggruz"
      },
      "outputs": [],
      "source": [
        "# Categorize drones by max distance\n",
        "def categorize_drone_weight(weight):\n",
        "    if weight <= 500:\n",
        "        return \"< 500gr\"\n",
        "    elif weight <= 1000:\n",
        "        return \"500gr – 1kg\"\n",
        "    elif weight <= 2000:\n",
        "        return \"1–2kg\"\n",
        "    elif weight <= 5000:\n",
        "        return \"2–5kg\"\n",
        "    else:\n",
        "        return \"> 5kg\"\n",
        "\n",
        "drone_df[\"payloadweight_cat\"] = drone_df[\"max_payload_weight\"].apply(categorize_drone_weight)\n",
        "\n",
        "# Count per category\n",
        "drone_weight_counts = drone_df[\"payloadweight_cat\"].value_counts().sort_index()\n",
        "drone_weight_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVF7pcQjgruz"
      },
      "outputs": [],
      "source": [
        "# Plot donut\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "wedges, texts, autotexts = ax.pie(\n",
        "    drone_weight_counts,\n",
        "    labels=drone_weight_counts.index,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    pctdistance=0.85,\n",
        "    wedgeprops=dict(width=0.4)\n",
        ")\n",
        "\n",
        "# Draw donut hole\n",
        "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
        "fig.gca().add_artist(centre_circle)\n",
        "\n",
        "ax.axis('equal')\n",
        "plt.title(\"Drone Maximum Payload Categories\")\n",
        "# Save the doughnut chart as a PNG image with high resolution\n",
        "\n",
        "#plt.savefig(r'D:\\NDIS_Database\\dronedistance_distribution.png', dpi=300, transparent=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2To_jsegruz"
      },
      "outputs": [],
      "source": [
        "full_ghz_df['Top3Sensors'] = full_ghz_df['Top3Sensors'].apply(\n",
        "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLBwiHvygru0"
      },
      "outputs": [],
      "source": [
        "print(full_ghz_df['Top3Sensors'].explode().unique())\n",
        "print(sensor_df['sensor_name'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn5qGitHgru0"
      },
      "outputs": [],
      "source": [
        "sensor_weight_map = sensor_df.set_index('sensor_name')['sensor_weight'].to_dict()\n",
        "sensor_weight_map.get(\"Camera\")  # should return 100 or similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MKqSJ2qgru1"
      },
      "outputs": [],
      "source": [
        "def simple_drone_selector(sensor_name, sensor_weight, hazard_distance, drone_df):\n",
        "    if sensor_name == 'Camera':\n",
        "        eligible = drone_df.copy()\n",
        "    else:\n",
        "        eligible = drone_df[drone_df['max_payload_weight'] >= sensor_weight]\n",
        "\n",
        "    match = eligible[eligible['distance_range'] >= hazard_distance]\n",
        "    if not match.empty:\n",
        "        return match.sort_values(by=['max_payload_weight', 'distance_range']).iloc[0]['mfc_model']\n",
        "\n",
        "    if not eligible.empty:\n",
        "        fallback = eligible.copy()\n",
        "        fallback['dist_diff'] = (fallback['distance_range'] - hazard_distance).abs()\n",
        "        return fallback.sort_values(by='dist_diff').iloc[0]['mfc_model']\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30RxLua5gru2"
      },
      "outputs": [],
      "source": [
        "def assign_best_drones_chunked(df, drone_df, sensor_df, chunk_size=50000):\n",
        "    sensor_weight_map = sensor_df.set_index('sensor_name')['sensor_weight'].to_dict()\n",
        "    results = []\n",
        "\n",
        "    for start in range(0, len(df), chunk_size):\n",
        "        end = min(start + chunk_size, len(df))\n",
        "        print(f\"\\U0001f373 Processing chunk {start} to {end}...\")\n",
        "        chunk = df.iloc[start:end].copy()\n",
        "\n",
        "        for i, sensor_pos in enumerate(['s1', 's2', 's3']):\n",
        "            colname = f'BestDrone_{sensor_pos}'\n",
        "            chunk[colname] = None\n",
        "\n",
        "            for idx, row in chunk.iterrows():\n",
        "                sensors = row['Top3Sensors']\n",
        "                if not isinstance(sensors, list) or len(sensors) <= i:\n",
        "                    continue\n",
        "\n",
        "                sensor_name = sensors[i]\n",
        "                sensor_weight = sensor_weight_map.get(sensor_name)\n",
        "                if sensor_weight is None:\n",
        "                    continue\n",
        "\n",
        "                hazard_distance = row['distance']\n",
        "                # print(f\"{sensor_pos.upper()} | {sensor_name} | {sensor_weight}g | dist = {hazard_distance}\")\n",
        "                selected = simple_drone_selector(sensor_name, sensor_weight, hazard_distance, drone_df)\n",
        "                chunk.at[idx, colname] = selected\n",
        "\n",
        "        results.append(chunk)\n",
        "\n",
        "    return pd.concat(results, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Joh2xh8_gru2"
      },
      "outputs": [],
      "source": [
        "full_ghz_df = assign_best_drones_chunked(full_ghz_df, drone_df, sensor_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxx76HoVgru3"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "# Parse Top3Sensors\n",
        "full_ghz_df['Top3Sensors'] = full_ghz_df['Top3Sensors'].apply(\n",
        "    lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else x\n",
        ")\n",
        "\n",
        "# Extract first sensor\n",
        "full_ghz_df['s1'] = full_ghz_df['Top3Sensors'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
        "\n",
        "# Map sensor weight\n",
        "sensor_weight_map = sensor_df.set_index('sensor_name')['sensor_weight'].to_dict()\n",
        "full_ghz_df['s1_weight'] = full_ghz_df['s1'].map(sensor_weight_map)\n",
        "\n",
        "# Output column\n",
        "full_ghz_df['BestDrone_s1'] = None\n",
        "\n",
        "# Match sensor → drones by payload → then distance\n",
        "for sensor in full_ghz_df['s1'].dropna().unique():\n",
        "    w = sensor_weight_map.get(sensor, 0)\n",
        "    sensor_mask = full_ghz_df['s1'] == sensor\n",
        "    chunk = full_ghz_df[sensor_mask].copy()\n",
        "\n",
        "    for idx, row in chunk.iterrows():\n",
        "        hazard_distance = row['distance']\n",
        "\n",
        "        # Step 1: payload match (or all drones for camera)\n",
        "        if sensor == 'Camera':\n",
        "            eligible = drone_df.copy()\n",
        "        else:\n",
        "            eligible = drone_df[drone_df['max_payload_weight'] >= w]\n",
        "\n",
        "        # Step 2: match distance\n",
        "        dist_match = eligible[eligible['distance_range'] >= hazard_distance]\n",
        "        if not dist_match.empty:\n",
        "            best = dist_match.sort_values(by=['max_payload_weight', 'distance_range']).iloc[0]['mfc_model']\n",
        "        elif not eligible.empty:\n",
        "            fallback = eligible.copy()\n",
        "            fallback['dist_diff'] = (fallback['distance_range'] - hazard_distance).abs()\n",
        "            best = fallback.sort_values(by='dist_diff').iloc[0]['mfc_model']\n",
        "        else:\n",
        "            best = None\n",
        "\n",
        "        full_ghz_df.at[idx, 'BestDrone_s1'] = best\n",
        "\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(f\"\\u2705 BestDrone for Sensor 1 is assigned! Time: {elapsed / 60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hvdnalagru4"
      },
      "source": [
        "# --- Ready for API --- ver 3.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMW0Z8S7gru4"
      },
      "source": [
        "# STEP 1: Build Hazard-Stage-Survey Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeQUNC1ugru4"
      },
      "outputs": [],
      "source": [
        "hazard_stage_survey_map = {\n",
        "    (\"Volcano\", \"pre_event\"): [\"Magnetometers\", \"Seismic\", \"Camera\"],\n",
        "    (\"Volcano\", \"during\"): [\"Thermal_Camera\", \"Camera\", \"Lidar\"],\n",
        "    (\"Volcano\", \"post_event\"): [\"Lidar\", \"Camera\", \"Seismic\"],\n",
        "    (\"Volcano\", \"clean_up\"): [\"Lidar\", \"Camera\", \"Seismic\"],\n",
        "\n",
        "    (\"Earthquake\", \"pre_event\"): [\"Seismic\", \"Magnetometers\", \"Camera\"],\n",
        "    (\"Earthquake\", \"during\"): [\"Seismic\", \"Camera\", \"Lidar\"],\n",
        "    (\"Earthquake\", \"post_event\"): [\"Lidar\", \"Camera\", \"Seismic\"],\n",
        "\n",
        "    (\"Fault\", \"pre_event\"): [\"Seismic\", \"Magnetometers\", \"Camera\"],\n",
        "    (\"Fault\", \"post_event\"): [\"Seismic\", \"Camera\", \"Lidar\"],\n",
        "\n",
        "    (\"Landslide\", \"pre_event\"): [\"Lidar\", \"GPR\", \"Camera\"],\n",
        "    (\"Landslide\", \"during\"): [\"Camera\", \"Thermal_Camera\", \"Lidar\"],\n",
        "    (\"Landslide\", \"post_event\"): [\"Lidar\", \"Seismic\", \"Camera\"],\n",
        "    (\"Landslide\", \"clean_up\"): [\"Camera\", \"Lidar\", \"Seismic\"],\n",
        "\n",
        "    (\"Tsunami\", \"during\"): [\"Camera\", \"Thermal_Camera\", \"Lidar\"],\n",
        "    (\"Tsunami\", \"post_event\"): [\"Camera\", \"Lidar\", \"Seismic\"],\n",
        "    (\"Tsunami\", \"clean_up\"): [\"Camera\", \"Lidar\", \"Seismic\"],\n",
        "\n",
        "    (\"Nuclear\", \"pre_event\"): [\"Thermal_Camera\", \"Camera\", \"Lidar\"],\n",
        "    (\"Nuclear\", \"during\"): [\"Thermal_Camera\", \"Camera\", \"Lidar\"],\n",
        "    (\"Nuclear\", \"post_event\"): [\"Camera\", \"Lidar\", \"Seismic\"],\n",
        "}\n",
        "\n",
        "def get_top3_sensors_from_stage(stage_key):\n",
        "    return hazard_stage_survey_map.get(stage_key, [\"Seismic\", \"Camera\", \"Lidar\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ5zrex8gru4"
      },
      "outputs": [],
      "source": [
        "sensor_weight_map = sensor_df.set_index('sensor_name')['sensor_weight'].to_dict()\n",
        "sensor_weight_map.get(\"Camera\")  # should return 100 or similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKPSZDX0gru5"
      },
      "outputs": [],
      "source": [
        "def fast_drone_match(sensor_name, hazard_distance, drone_df):\n",
        "    sensor_weight = sensor_df[sensor_df['sensor_name'] == sensor_name]['sensor_weight']\n",
        "    if sensor_weight.empty:\n",
        "        return None\n",
        "    sensor_weight = sensor_weight.values[0]\n",
        "\n",
        "    # Step 1: Filter by payload\n",
        "    eligible = drone_df[drone_df['max_payload_weight'] >= sensor_weight]\n",
        "\n",
        "    # Step 2: Filter by flexible distance rule\n",
        "    if hazard_distance < 100:\n",
        "        threshold = 1.0\n",
        "    elif hazard_distance < 500:\n",
        "        threshold = 0.8\n",
        "    elif hazard_distance < 5000:\n",
        "        threshold = 0.6\n",
        "    elif hazard_distance < 50000:\n",
        "        threshold = 0.4\n",
        "    else:\n",
        "        threshold = 0.0  # Any drone OK\n",
        "\n",
        "    eligible = eligible[\n",
        "        (eligible['distance_range'] >= hazard_distance * threshold) |\n",
        "        (hazard_distance > 50000)  # catch-all fallback\n",
        "    ]\n",
        "\n",
        "    if eligible.empty:\n",
        "        # No match → fallback to closest by payload only\n",
        "        fallback = drone_df[drone_df['max_payload_weight'] >= sensor_weight]\n",
        "        if fallback.empty:\n",
        "            return None\n",
        "        return fallback.sort_values(by='max_payload_weight').iloc[0]\n",
        "\n",
        "    # Return the closest fit by smallest \"overkill\"\n",
        "    eligible['overkill'] = (eligible['max_payload_weight'] - sensor_weight).abs()\n",
        "    return eligible.sort_values(by='overkill').iloc[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtj14y8egru5"
      },
      "outputs": [],
      "source": [
        "def calculate_urgency(pop, intensity, econ_loss, distance):\n",
        "    # Normalize each component to 0–1 scale\n",
        "    norm_pop = min(pop / 1_000_000, 1.0)\n",
        "    norm_intensity = min(intensity / 5, 1.0)\n",
        "    norm_econ = min(econ_loss / 1000, 1.0)  # assume 1000M = extreme\n",
        "    norm_dist = min(distance / 100_000, 1.0)  # cap at 100 km\n",
        "\n",
        "    # Weighted combo (tweak as needed)\n",
        "    raw_score = 0.4 * norm_pop + 0.3 * norm_intensity + 0.2 * norm_econ + 0.1 * norm_dist\n",
        "\n",
        "    # Convert to 1–10 scale\n",
        "    urgency_scaled = round(raw_score * 10)\n",
        "    return min(max(urgency_scaled, 1), 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56jxehyAgru6"
      },
      "source": [
        "# Step 3: Mission Recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0DLY1eBgru6"
      },
      "outputs": [],
      "source": [
        "def recommend_mission(\n",
        "    lat: float,\n",
        "    lon: float,\n",
        "    radius: int = 10000,\n",
        "    hazard_type: Optional[str] = None,\n",
        "    sensors: Optional[List[str]] = None,\n",
        "    stage: Optional[str] = None\n",
        "):\n",
        "    # 1. Find geohazards within buffer\n",
        "    buffer_results = find_ghz_within_buffer(lat, lon, radius)\n",
        "    if buffer_results.empty:\n",
        "        return {\"error\": \"No geohazards found in this radius.\"}\n",
        "\n",
        "    # 2. Choose hazard (user-selected or highest intensity nearby)\n",
        "    ghz_user = buffer_results[buffer_results['HazardType'] == hazard_type]\n",
        "    if not ghz_user.empty:\n",
        "        ghz = ghz_user.sort_values(by='intensity', ascending=False).iloc[0]\n",
        "        alt_hazard = buffer_results['HazardType'].value_counts().idxmax()\n",
        "        alt_note = None if alt_hazard == hazard_type else f\"Note: This area shows more frequent {alt_hazard.lower()} hazards historically.\"\n",
        "    else:\n",
        "        ghz = buffer_results.sort_values(by='intensity', ascending=False).iloc[0]\n",
        "        alt_note = f\"No {hazard_type} found. Suggesting mission for nearby {ghz['HazardType'].lower()}.\"\n",
        "\n",
        "    # 3. Determine Top3Sensors\n",
        "    if sensors:\n",
        "        top3_sensors = sensors[:3]\n",
        "    else:\n",
        "        stage_key = (ghz['HazardType'], stage or 'pre_event')\n",
        "        top3_sensors = get_top3_sensors_from_stage(stage_key)\n",
        "\n",
        "    # 4. Get sensor weights\n",
        "    sensor_weights = [sensor_weight_map.get(s, 0) for s in top3_sensors]\n",
        "\n",
        "    # 5. Find top 3 drones per sensor\n",
        "    top3_drones = []\n",
        "    for s_name, s_weight in zip(top3_sensors, sensor_weights):\n",
        "        d = fast_drone_match(s_name, ghz['distance'], drone_df)\n",
        "        if d is not None: top3_drones.append(d['mfc_model'])\n",
        "\n",
        "    # 6. Select best drone + sensor combo\n",
        "    best_pair = top3_drones[0] + \" + \" + top3_sensors[0] if top3_drones else None\n",
        "\n",
        "    # 7. Get specs\n",
        "    drone_specs = drone_df[drone_df['mfc_model'] == top3_drones[0]].iloc[0].to_dict() if top3_drones else {}\n",
        "\n",
        "    # 8. Calculate CPM (travel + monitor time) → dummy for now\n",
        "    mission_summary = {\"travel_time_min\": 12, \"monitor_time_min\": 18, \"total_eta_min\": 30}\n",
        "\n",
        "    # 9. Urgency (population, intensity, economic loss, distance)\n",
        "    urgency = calculate_urgency(\n",
        "    ghz['pop'],\n",
        "    ghz['intensity'],\n",
        "    ghz['economic_loss_million'],\n",
        "    ghz['distance']\n",
        ")\n",
        "\n",
        "    # 10. Logistics\n",
        "    if ghz['distance'] > 50000:\n",
        "        logistics = \"BVLOS recommended. Ocean-capable or hybrid land-sea drone setup.\"\n",
        "    elif ghz['distance'] > 20000:\n",
        "        logistics = \"Deploy from closest land vehicle, recommend long-range multirotor.\"\n",
        "    else:\n",
        "        logistics = \"Standard RPAS deployment possible.\"\n",
        "\n",
        "    return {\n",
        "        \"mission_summary\": mission_summary,\n",
        "        \"top3_drones\": top3_drones,\n",
        "        \"top3_sensors\": top3_sensors,\n",
        "        \"best_pair\": best_pair,\n",
        "        \"drone_specs\": drone_specs,\n",
        "        \"urgency_score\": urgency,\n",
        "        \"logistics_recommendation\": logistics,\n",
        "        \"hazard_id\": ghz['HazardID'],\n",
        "        \"hazard_type\": ghz['HazardType'],\n",
        "        \"hazard_distance_m\": ghz['distance'],\n",
        "        \"note\": alt_note\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayipXNVugru6"
      },
      "outputs": [],
      "source": [
        "def find_ghz_within_buffer(lat: float, lon: float, radius: float) -> pd.DataFrame:\n",
        "    # Create GeoDataFrame of user point\n",
        "    user_point = gpd.GeoDataFrame(\n",
        "        geometry=[Point(lon, lat)],\n",
        "        crs=\"EPSG:4326\"  # global data is in WGS84\n",
        "    ).to_crs(epsg=3857)  # Project to meters for buffer\n",
        "\n",
        "    # Create a buffer around user point (radius in meters)\n",
        "    buffer = user_point.buffer(radius).iloc[0]\n",
        "\n",
        "    # Convert geohazard DataFrame to GeoDataFrame\n",
        "    ghz_gdf = gpd.GeoDataFrame(\n",
        "        full_ghz_df,\n",
        "        geometry=gpd.points_from_xy(full_ghz_df.longitude, full_ghz_df.latitude),\n",
        "        crs=\"EPSG:4326\"\n",
        "    ).to_crs(epsg=3857)\n",
        "\n",
        "    # Filter points within buffer\n",
        "    within = ghz_gdf[ghz_gdf.geometry.within(buffer)]\n",
        "\n",
        "    return within.to_crs(epsg=4326).drop(columns=\"geometry\")  # Convert back to WGS84"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn4KSnw1gru7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hZUPrOqgru7"
      },
      "outputs": [],
      "source": [
        "start = timeit.default_timer()\n",
        "\n",
        "# Example test\n",
        "lat, lon = 49.3, -123.1  # or any valid coordinate\n",
        "radius = 10000\n",
        "hazard_type = \"Volcano\"\n",
        "stage = \"during\"\n",
        "sensors = None  # or manually pass e.g. [\"Thermal_Camera\", \"Camera\"]\n",
        "\n",
        "result = recommend_mission(lat, lon, radius, hazard_type, sensors, stage)\n",
        "\n",
        "elapsed = timeit.default_timer() - start\n",
        "print(f\"\\u2705 Diverse Top-3 drones assigned! Time: {elapsed / 60:.2f} minutes\")\n",
        "\n",
        "pprint(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7rS7atNgru7"
      },
      "source": [
        "# Statistics of Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY4jQcNigru8"
      },
      "outputs": [],
      "source": [
        "# Define urgency score calculator\n",
        "def calculate_urgency(pop, intensity, econ_loss, distance):\n",
        "    norm_pop = np.minimum(pop / 1_000_000, 1.0)\n",
        "    norm_intensity = np.minimum(intensity / 5, 1.0)\n",
        "    norm_econ = np.minimum(econ_loss / 1000, 1.0)\n",
        "    norm_dist = np.minimum(distance / 100_000, 1.0)\n",
        "    raw_score = 0.4 * norm_pop + 0.3 * norm_intensity + 0.2 * norm_econ + 0.1 * norm_dist\n",
        "    urgency_scaled = np.round(raw_score * 10)\n",
        "    return np.clip(urgency_scaled, 1, 10)\n",
        "\n",
        "# Top3 sensor selector\n",
        "def get_top3_sensors_from_stage(stage_key):\n",
        "    default = [\"Camera\", \"Lidar\", \"Seismic\"]\n",
        "    return hazard_stage_survey_map.get(stage_key, default)\n",
        "\n",
        "# Fast drone selector with fallback\n",
        "def fast_drone_match(sensor_name, sensor_weight, hazard_distance, drone_df):\n",
        "    subset = drone_df[drone_df['max_payload_weight'] >= sensor_weight]\n",
        "    if sensor_name == \"Camera\":\n",
        "        subset = pd.concat([subset, drone_df[drone_df['max_payload_weight'] == 0]])\n",
        "\n",
        "    if subset.empty:\n",
        "        return None\n",
        "\n",
        "    subset['dist_diff'] = np.abs(subset['distance_range'] - hazard_distance)\n",
        "    subset['weight_diff'] = subset['max_payload_weight'] - sensor_weight\n",
        "    best = subset.sort_values(by=['dist_diff', 'weight_diff']).head(3)\n",
        "    return best\n",
        "\n",
        "# Main batch processor\n",
        "def apply_recommender_batch(full_df, drone_df, sensor_df):\n",
        "    records = []\n",
        "    for _, row in full_df.iterrows():\n",
        "        htype = row['HazardType']\n",
        "        stage = row.get('HazardStage', 'pre_event')\n",
        "        stage_key = (htype, stage)\n",
        "        top3_sensors = get_top3_sensors_from_stage(stage_key)\n",
        "        weights = [sensor_weight_map.get(s, 0) for s in top3_sensors]\n",
        "\n",
        "        top3_drone_models = []\n",
        "        for s, w in zip(top3_sensors, weights):\n",
        "            top = fast_drone_match(s, w, row['distance'], drone_df)\n",
        "            if top is not None:\n",
        "                top3_drone_models.append(top.iloc[0]['mfc_model'])\n",
        "            else:\n",
        "                top3_drone_models.append(None)\n",
        "\n",
        "        urgency = calculate_urgency(row['pop'], row['intensity'], row['economic_loss_million'], row['distance'])\n",
        "        best_pair = f\"{top3_drone_models[0]} + {top3_sensors[0]}\" if top3_drone_models[0] else None\n",
        "        records.append({\n",
        "            'HazardID': row['HazardID'],\n",
        "            'Top3Sensors': top3_sensors,\n",
        "            'Top3Drones': top3_drone_models,\n",
        "            'BestPair': best_pair,\n",
        "            'UrgencyScore': urgency\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame.from_records(records)\n",
        "\n",
        "# Example chart after processing\n",
        "def plot_drone_frequency(result_df):\n",
        "    all_drones = pd.Series(sum(result_df['Top3Drones'].dropna().tolist(), []))\n",
        "    counts = all_drones.value_counts().sort_values(ascending=False)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    counts.head(20).plot(kind='bar')\n",
        "    plt.title(\"Top 20 Drones Used Across All Hazards\")\n",
        "    plt.ylabel(\"Usage Count\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.grid(axis='y')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPPcb42Fgru8"
      },
      "outputs": [],
      "source": [
        "result_df = apply_recommender_batch(full_ghz_df, drone_df, sensor_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL3fG6j9gru8"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(models, counts)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Top Best Drones Selected')\n",
        "plt.xlabel('Drone Model')\n",
        "plt.ylabel('Number of Missions')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ArcGISPro",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}