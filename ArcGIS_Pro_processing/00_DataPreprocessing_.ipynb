{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd0J3LTrWg3R"
      },
      "source": [
        "# NDIS Database Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GIIQX9ZWg3V"
      },
      "source": [
        "## Geohazard & Infrastructure database compilation\n",
        "\n",
        "This notebook documents the preprocessing pipeline for the Natural Disaster Information System (NDIS), a decision-support tool for planning RPAS (Remotely Piloted Aircraft Systems) missions in geohazard monitoring.\n",
        "\n",
        "The preprocessing integrates six core datasets, each representing a distinct geohazard type:\n",
        "1. Volcano\n",
        "2. Landslide\n",
        "3. Tsunami\n",
        "4. Fault\n",
        "5. Earthquake\n",
        "6. Nuclear Power Plant\n",
        "\n",
        "To ensure consistency across hazard types, raw datasets were parsed and harmonized into a unified schema. This involved:\n",
        "- Converting spatial geometries to a common coordinate system\n",
        "- Standardizing key fields such as intensity, duration, and economic loss\n",
        "- Mapping hazard-specific attributes (e.g., magnitude, VEI) into shared analytical fields\n",
        "- Handling missing values using severity-based proxies or simulation logic\n",
        "\n",
        "Geospatial preprocessing steps include:\n",
        "- Buffering and clipping to Exclusive Economic Zone (EEZ) boundaries\n",
        "- Zonal statistics for population exposure within hazard zones\n",
        "- Distance-based feature engineering (e.g., proximity to roads or coastlines)\n",
        "\n",
        "All spatial analysis was performed in **ArcGIS Pro**, which also supported early-stage logic testing and scenario validation. The final decision logic, including RPAS and sensor recommendations, was implemented using a staggered rule-based workflow. Although machine learning models (e.g., decision trees) were evaluated, they were not used in the operational version due to limited generalizability.\n",
        "\n",
        "This notebook serves as a reproducible reference for harmonized data preparation and logic integration in the NDIS system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfsa3e7uWg3Y"
      },
      "outputs": [],
      "source": [
        "from arcgis.gis import GIS\n",
        "gis = GIS(\"home\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBbBjNR7Wg3a"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "#ArcGIS packages\n",
        "import arcpy\n",
        "#from arcgis.mapping import WebScene\n",
        "from arcgis.gis import GIS\n",
        "from arcgis.features import FeatureLayer\n",
        "from IPython.display import display\n",
        "from arcgis.features import GeoAccessor\n",
        "from arcgis import *\n",
        "from arcpy.sa import Int\n",
        "# Raster processing for dataframe\n",
        "from rasterstats import zonal_stats\n",
        "import rasterio\n",
        "\n",
        "# basic packages\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import timeit\n",
        "import random\n",
        "import string\n",
        "from playsound import playsound\n",
        "import gc # Force Garbage Collection. This helps reduce memory leaks in long loops.\n",
        "import warnings\n",
        "\n",
        "# Data management\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point  # to get points from long lat\n",
        "\n",
        "# Request service\n",
        "#from requests import Request\n",
        "import json\n",
        "import re\n",
        "from functools import reduce\n",
        "#from owslib.wfs import WebFeatureService\n",
        "import sqlite3\n",
        "\n",
        "# Plotting packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK-mzvB3Wg3b"
      },
      "source": [
        "# Get Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPq0KNioWg3c"
      },
      "source": [
        "## <font color='red'> 1. Volcano data </font>\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Note:</b>\n",
        "    Citation: Global Volcanism Program, 2013. Volcanoes of the World, v. 4.11.0 (08 Jul 2022). Venzke, E (ed.). Smithsonian Institution. Downloaded 13 Jul 2022. https://doi.org/10.5479/si.GVP.VOTW4-2013.\n",
        "\n",
        "Further info: https://volcano.si.edu/database/webservices.cfm\n",
        "\n",
        "Service Layer: http://webservices.volcano.si.edu/geoserver/GVP-VOTW/ows?service=WFS&version=1.0.0&request=describefeaturetype&typeName=GVP-VOTW:E3WebApp_HoloceneVolcanoes\n",
        "    \n",
        "    Significant Volcano Eruption:\n",
        "Citation: National Geophysical Data Center / World Data Service (NGDC/WDS): NCEI/WDS Global Significant Volcanic Eruptions Database. NOAA National Centers for Environmental Information. doi:10.7289/V5JW8BSH [8 July 2022]\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNkLZhSkWg3e"
      },
      "outputs": [],
      "source": [
        "# Set the path to this geodatabase\n",
        "gdb_path = r\"D:\\ArcGISProjects\\GeohazardDB\\GeohazardDB.gdb\"  # Update the path accordingly\n",
        "\n",
        "# Set the workspace to geodatabase\n",
        "arcpy.env.workspace = gdb_path\n",
        "\n",
        "# Specify the feature class name\n",
        "feature_class_name = \"volc84\"\n",
        "feature_class_path = f\"{gdb_path}\\\\{feature_class_name}\"\n",
        "\n",
        "# Describe the feature class\n",
        "desc = arcpy.Describe(feature_class_path)\n",
        "\n",
        "# Check the spatial reference\n",
        "spatial_reference = desc.spatialReference\n",
        "\n",
        "# Print the spatial reference details\n",
        "print(f\"Spatial Reference of {feature_class_name}:\")\n",
        "print(f\"  Name: {spatial_reference.name}\")\n",
        "print(f\"  WKID: {spatial_reference.factoryCode}\")\n",
        "print(f\"  WKT: {spatial_reference.exportToString()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFjSDMs-Wg3f"
      },
      "outputs": [],
      "source": [
        "# Use arcpy to create a list of fields\n",
        "fields = [f.name for f in arcpy.ListFields(f\"{gdb_path}\\\\{feature_class_name}\")]\n",
        "\n",
        "# Use arcpy to create a search cursor and load the data into a list of dictionaries\n",
        "data = []\n",
        "with arcpy.da.SearchCursor(f\"{gdb_path}\\\\{feature_class_name}\", fields) as cursor:\n",
        "    for row in cursor:\n",
        "        data.append(dict(zip(fields, row)))\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "volc = pd.DataFrame(data)\n",
        "volc.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9d-wVojWg3f"
      },
      "outputs": [],
      "source": [
        "# Assuming 'Shape' column contains tuples (x, y), get the geometry for volcano database\n",
        "volc['geometry'] = volc['Shape'].apply(lambda geom: Point(geom) if geom is not None else None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tRXcZUFWg3g"
      },
      "outputs": [],
      "source": [
        "# Set the coordinate reference system (CRS) to WGS 84 (EPSG:4326)\n",
        "#volc.set_crs(epsg=4326, inplace=True)\n",
        "#print(volc.crs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0mJ-dkSWg3g"
      },
      "outputs": [],
      "source": [
        "# Create a GeoDataFrame\n",
        "volc = gpd.GeoDataFrame(volc, geometry='geometry')\n",
        "volc.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc5QUYRaWg3g"
      },
      "outputs": [],
      "source": [
        "# Extract column needed for the basic NDIS database to compile with other geohazards dataset.\n",
        "# Volcano Number --> HazardID, longitude, latitude, HazardType (in numerical coded added after extraction). Volcano is 1.\n",
        "vo_df = volc[['VolcanoNumber','LatitudeDecimal','LongitudeDecimal','geometry']].copy()\n",
        "vo_df.rename(columns = {'VolcanoNumber':'HazardID','LatitudeDecimal':'latitude','LongitudeDecimal':'longitude'}, inplace = True)\n",
        "vo_df['HazardType'] = 1\n",
        "vo_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MqKTVOVWg3h"
      },
      "source": [
        "## <font color='red'> 2. Landslide Data </font>\n",
        "Retrieved from NASA\n",
        "\n",
        "Title: Global Landslide Catalog | Type: Feature Service | Owner: krolikie@unhcr.org_unhcr\n",
        "https://maps.nccs.nasa.gov/arcgis/apps/MapAndAppGallery/index.html?appid=574f26408683485799d02e857e5d9521\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Note:</b>\n",
        "    Citation: Kirschbaum, D.B., Stanley, T., & Zhou, Y. (2015). Spatial and temporal analysis of a global landslide catalog. Geomorphology, 249, 4-15. doi:10.1016/j.geomorph.2015.03.016\n",
        "\n",
        "    Kirschbaum, D.B., Adler, R., Hong, Y., Hill, S., & Lerner-Lam, A. (2010). A global landslide catalog for hazard applications: method, results, and limitations. Natural Hazards, 52, 561-575. doi:10.1007/s11069-009-9401-4\n",
        "\n",
        "Further info:\n",
        "https://gpm.nasa.gov/landslides/data.html\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fe12OzMWg3h"
      },
      "outputs": [],
      "source": [
        "# After the file is downloaded, load the following from local path and check the GCS\n",
        "ls_spatial_ref = arcpy.Describe(r\"D:/NDIS_Database/04_Landslide/nasa_coolr_reports_point.shp\").spatialReference\n",
        "ls_spatial_ref"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYEZVhdaWg3h"
      },
      "outputs": [],
      "source": [
        "# Read the shapefile from local disk\n",
        "landslide_df = gpd.read_file(r\"D:/NDIS_Database/04_Landslide/nasa_coolr_reports_point.shp\")\n",
        "landslide_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMeolhaCWg3i"
      },
      "outputs": [],
      "source": [
        "# Extract column needed for the basic NDIS database to compile with other geohazards dataset.\n",
        "# Volcano Number --> HazardID, longitude, latitude, HazardType (in numerical coded added after extraction). Volcano is 1.\n",
        "ls_df = landslide_df[['ev_id','latitude','longitude', 'geometry']].copy()\n",
        "ls_df.rename(columns = {'ev_id':'HazardID'}, inplace = True)\n",
        "ls_df['HazardType'] = 2\n",
        "ls_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7KP-TG-Wg3i"
      },
      "outputs": [],
      "source": [
        "ls_df.crs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR0tnrbmWg3j"
      },
      "source": [
        "## <font color='red'> 3. Tsunami Data </font>\n",
        "\n",
        "Data retrieved from NCEI NOAA - Global Historical Tsunami Database\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Note:</b>\n",
        "    A feature layer displaying historical tsunami events from NCEI's Global Historical Tsunami Database. The Global Historical Tsunami Database consists of two related files containing information on tsunami events from 2000 B.C. to the present in the Atlantic, Indian, and Pacific Oceans; and the Mediterranean and Caribbean Seas.\n",
        "    \n",
        "    Citation: National Geophysical Data Center / World Data Service: NCEI/WDS Global Historical Tsunami Database. NOAA National Centers for Environmental Information. doi:10.7289/V5PN93H7 [4 August 2023]\n",
        "\n",
        "Further info: https://ngdc.noaa.gov/hazard/hazards.shtml\n",
        "\n",
        "Documentation: https://data.noaa.gov/metaview/page?xml=NOAA/NESDIS/NGDC/MGG/Hazards/iso/xml/G02151.xml&view=getDataView\n",
        "\n",
        "Layer info: https://www.arcgis.com/home/item.html?id=5a44c3d4d465498993120b70ab568876\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg4a9TznWg3j"
      },
      "outputs": [],
      "source": [
        "# Specify the feature class name\n",
        "feature_class_tsunami = \"tsunami\"  # Check for content pane\n",
        "tsunami_path = f\"{gdb_path}\\\\{feature_class_tsunami}\"\n",
        "\n",
        "# Describe the feature class\n",
        "desc_tsun = arcpy.Describe(tsunami_path)\n",
        "\n",
        "# Check the spatial reference\n",
        "tsunami_sr = desc_tsun.spatialReference\n",
        "\n",
        "# Print the spatial reference details\n",
        "print(f\"Spatial Reference of {feature_class_tsunami}:\")\n",
        "print(f\"  Name: {tsunami_sr.name}\")\n",
        "print(f\"  WKID: {tsunami_sr.factoryCode}\")\n",
        "print(f\"  WKT:  {tsunami_sr.exportToString()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9jCxo7KWg3k"
      },
      "outputs": [],
      "source": [
        "# Use arcpy to create a list of fields\n",
        "ts_fields = [f.name for f in arcpy.ListFields(f\"{gdb_path}\\\\{feature_class_tsunami}\")]\n",
        "\n",
        "# Use arcpy to create a search cursor and load the data into a list of dictionaries\n",
        "ts_data = []\n",
        "with arcpy.da.SearchCursor(f\"{gdb_path}\\\\{feature_class_tsunami}\", ts_fields) as cursor:\n",
        "    for row in cursor:\n",
        "        ts_data.append(dict(zip(ts_fields, row)))\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "tsun = pd.DataFrame(ts_data)\n",
        "tsun.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEaaaDusWg3l"
      },
      "outputs": [],
      "source": [
        "ts_df = tsun[['ID','LONGITUDE', 'LATITUDE']].copy()\n",
        "ts_df.rename(columns = {'ID':'HazardID','LONGITUDE':'longitude', 'LATITUDE':'latitude'}, inplace=True)\n",
        "ts_df['HazardType'] = 3\n",
        "ts_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xggilEQTWg3l"
      },
      "outputs": [],
      "source": [
        "# Generate points geometry from longitude and latitude\n",
        "ts_df['geometry'] = ts_df.apply(lambda x: Point(x['longitude'], x['latitude']), axis=1)\n",
        "\n",
        "# Create a GeoDataFrame\n",
        "ts_df = gpd.GeoDataFrame(ts_df, geometry='geometry')\n",
        "\n",
        "# Set the coordinate reference system (CRS) to WGS 84 (EPSG:4326)\n",
        "ts_df.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "ts_df.crs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDCB90tsWg3m"
      },
      "source": [
        "## <font color='red'> 4. Fault Data </font>\n",
        "\n",
        "GEM Global Active Faults Database (GAF-DB)\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Note:</b>\n",
        "    A feature layer displaying historical tsunami events from NCEI's Global Historical Tsunami Database. The Global Historical Tsunami Database consists of two related files containing information on tsunami events from 2000 B.C. to the present in the Atlantic, Indian, and Pacific Oceans; and the Mediterranean and Caribbean Seas.\n",
        "    \n",
        "    Citation: The GEM GAF-DB has been published in Earthquake Spectra.\n",
        "    Styron, Richard, and Marco Pagani. ‚ÄúThe GEM Global Active Faults Database.‚Äù Earthquake Spectra, vol. 36, no. 1_suppl, Oct. 2020, pp. 160‚Äì180, doi:10.1177/8755293020944182.\n",
        "\n",
        "The link to the publication is here: https://journals.sagepub.com/doi/abs/10.1177/8755293020944182\n",
        "\n",
        "Documentation: https://github.com/GEMScienceTools/gem-global-active-faults\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ypqMsefWg3m"
      },
      "outputs": [],
      "source": [
        "# Obtain Spatial Reference information for harmonized processing\n",
        "\n",
        "af_spatial_ref = arcpy.Describe(r\"D:/NDIS_Database/05_Fault/fault_points_wgs84.shp\").spatialReference\n",
        "af_spatial_ref"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJwnZCtFWg3n"
      },
      "outputs": [],
      "source": [
        "# Prior to loading this file, fault data has been processes using Geoprocessing (via GUI) using Generate Points Along Lines tool.\n",
        "# Read the shapefile from local disk\n",
        "fault_df = gpd.read_file(r\"D:/NDIS_Database/05_Fault/fault_points_wgs84.shp\")\n",
        "fault_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6cyuWhPWg3n"
      },
      "outputs": [],
      "source": [
        "# extract the longitude and latitude from the geometry field using the .x and .y attributes of the geometry column\n",
        "if fault_df.geometry is not None:\n",
        "    # Create new fields for longitude and latitude\n",
        "    fault_df[\"longitude\"] = fault_df.geometry.x\n",
        "    fault_df[\"latitude\"]  = fault_df.geometry.y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml0W_nq8Wg3n"
      },
      "outputs": [],
      "source": [
        "faultdb = fault_df[['ORIG_FID', 'longitude','latitude', 'geometry']].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYTi-YZGWg3o"
      },
      "source": [
        "### Assign ID\n",
        "\n",
        "Add numeric prefix (177) and zero-padding the original index\n",
        "\n",
        "Preserves traceability to original ORIG_FID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQTJQu0zWg3o"
      },
      "outputs": [],
      "source": [
        "print(max(fault_df['ORIG_FID'].unique()))\n",
        "print(min(fault_df['ORIG_FID'].unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE6qatT1Wg3o"
      },
      "outputs": [],
      "source": [
        "fault_df = fault_df.copy()\n",
        "\n",
        "# Use 7-digit global index padded with zeros ‚Üí ensures 10-digit result with '177' prefix\n",
        "fault_df[\"HazardID\"] = fault_df.reset_index().index.map(lambda i: f\"177{str(i).zfill(7)}\")\n",
        "\n",
        "# Check length and uniqueness\n",
        "assert fault_df[\"HazardID\"].str.len().max() == 10, \"HazardID exceeds 10 digits!\"\n",
        "assert fault_df[\"HazardID\"].is_unique, \"HazardID values are not unique!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ57AbmVWg3p"
      },
      "outputs": [],
      "source": [
        "len(fault_df['HazardID'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEp62VYOWg3p"
      },
      "outputs": [],
      "source": [
        "print(max(fault_df['HazardID'].unique()))\n",
        "print(min(fault_df['HazardID'].unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96bGL6QNWg3q"
      },
      "outputs": [],
      "source": [
        "# Mapping table to preserve tracability to ORIG_ID\n",
        "fault_df[[\"HazardID\", \"ORIG_FID\"]].to_csv(r\"D:\\NDIS_Database\\05_Fault\\fault_id_mapping.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK01RFDHWg3q"
      },
      "outputs": [],
      "source": [
        "# Extract column needed for the basic NDIS database to compile with other geohazards dataset.\n",
        "# Volcano Number --> HazardID, longitude, latitude, HazardType (in numerical coded added after extraction). Volcano is 1.\n",
        "faultdb = fault_df[['HazardID', 'longitude','latitude', 'geometry']].copy()\n",
        "faultdb['HazardType'] = 4\n",
        "faultdb.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdFbDkP6Wg3q"
      },
      "source": [
        "## <font color='red'> 5. Earthquake Data </font>\n",
        "\n",
        "### <font color='green'> Historcal part: </font>\n",
        "\n",
        "### GHEC Catalog\n",
        "\n",
        "GEM provides Global Historical Earthquake Catalogue (GHEC) from 1000 to 1903 (Albini, 2014). (https://platform.openquake.org/maps/80/download)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfeYGjHiWg3r"
      },
      "source": [
        "### SHEEC (SHARE European Earthquake Catalogue)\n",
        "\n",
        "SHEEC catalogue covers the year 1000-1899 for Europe region specifically. It consists of data with magnitude ranges from 1.7 to 8.5 (Stucchi et al., 2012). The data could be downloaded at https: //www.emidius.eu/SHEEC/sheec_1000_1899.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b19zjnhmWg39"
      },
      "source": [
        "### <font color='green'> Instrumental part: </font>\n",
        "\n",
        "### ISC Bulletin/ISC Global\n",
        "The ISC Bulletin has now been completely rebuilt for the period 1964-2010. As a result, the ISC hypocentre solutions and magnitudes for the entire period of 1964-latest are based on the ak135 velocity model and the location procedure that is currently used in operations.\n",
        "(http://www.isc.ac.uk/iscbulletin/search/catalogue/)\n",
        "\n",
        "The Bulletin of the International Seismological Centre relies on contributions from seismological agencies around the world. To date, a total of 573 agencies have contributed to the ISC Bulletin, throughout its history. For more information about the agency (http://www.isc.ac.uk/iscbulletin/agencies/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27UpMagMWg39"
      },
      "source": [
        "### ISC-GEM Catalogue\n",
        "\n",
        "The ISC-GEM Global Instrumental Earthquake Catalogue (1904-2016) is the result of a special effort to adapt and substantially extend and improve currently existing bulletin data for large earthquakes (magnitude 5.5 and above, plus continental events down to magnitude 5.0).\n",
        "(http://www.isc.ac.uk/iscgem/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gecA4-gMWg39"
      },
      "source": [
        "### Load data from local layer file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2f0C-pRWg3-"
      },
      "outputs": [],
      "source": [
        "# Fix ID (identifier) of the earthquake events ID\n",
        "# Load CSV files\n",
        "df1 = pd.read_csv(r\"D:\\NDIS_Database\\02_Earthquake\\eq_1000_2023.csv\") # Dataset with incorrect eventID\n",
        "df2 = pd.read_csv(r\"D:\\NDIS_Database\\02_Earthquake\\GHEC1000_1903\\eq1000.csv\") # Dataset with correct GEHid\n",
        "\n",
        "# Rename columns in df2 to match df1 (update the mapping if needed)\n",
        "rename_mapping = {\n",
        "    \"Lat\": \"latitude\",\n",
        "    \"Lon\": \"longitude\",\n",
        "    \"Year\": \"year\",\n",
        "    \"Mo\": \"month\",\n",
        "    \"Da\": \"day\"\n",
        "}\n",
        "\n",
        "df2.rename(columns=rename_mapping, inplace=True)\n",
        "\n",
        "# Define matching fields\n",
        "key_fields = [\"longitude\", \"latitude\", \"year\", \"month\", \"day\"]\n",
        "\n",
        "# Merge on key fields\n",
        "merged_df = df1.merge(df2[key_fields + [\"GEHid\"]], on=key_fields, how=\"left\")\n",
        "\n",
        "# Replace eventID with GEHid where a match is found\n",
        "merged_df[\"eventID\"] = merged_df[\"GEHid\"].combine_first(merged_df[\"eventID\"])\n",
        "\n",
        "# Drop the temporary GEHid column\n",
        "merged_df.drop(columns=[\"GEHid\"], inplace=True)\n",
        "\n",
        "merged_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5isKuENWg3-"
      },
      "outputs": [],
      "source": [
        "eq_df = merged_df[['eventID','longitude', 'latitude']].copy()\n",
        "eq_df.rename(columns = {'eventID':'HazardID'}, inplace = True)\n",
        "eq_df['HazardType'] = 5\n",
        "eq_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GelhC2YyWg3_"
      },
      "outputs": [],
      "source": [
        "nan_in_eq = eq_df.isnull().sum().sum()\n",
        "\n",
        "# printing the number of values present in\n",
        "# the whole dataframe\n",
        "print('Number of NaN values present: ' + str(nan_in_eq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZDPf1jtWg3_"
      },
      "outputs": [],
      "source": [
        "# Generate points geometry from longitude and latitude\n",
        "eq_df['geometry'] = eq_df.apply(lambda x: Point(x['longitude'], x['latitude']), axis=1)\n",
        "\n",
        "# Create a GeoDataFrame\n",
        "eq_df = gpd.GeoDataFrame(eq_df, geometry='geometry')\n",
        "\n",
        "# Set the coordinate reference system (CRS) to WGS 84 (EPSG:4326)\n",
        "eq_df.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "eq_df.crs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "517G0zY9Wg4A"
      },
      "source": [
        "-------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjvqiHDIWg4A"
      },
      "source": [
        "# NEW ENTRY NUCLEAR POWER PLANT ‚ò¢Ô∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE3fYNp_Wg4A"
      },
      "outputs": [],
      "source": [
        "# Read data\n",
        "nuclear_df = pd.read_csv(r\"D:\\NDIS_Database\\15_NuclearPower\\nuclearpp.csv\")\n",
        "nuclear_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BdmlE0UWg4B"
      },
      "outputs": [],
      "source": [
        "nuclear_df['GEM unit ID']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QSdG43eWg4B"
      },
      "outputs": [],
      "source": [
        "# Assemble the cleaned nuclear hazard dataframe\n",
        "cleaned_nuclear_df = pd.DataFrame({\n",
        "    \"HazardID\": nuclear_df['GEM unit ID'],\n",
        "    \"latitude\": pd.to_numeric(nuclear_df[\"Latitude\"], errors=\"coerce\"),\n",
        "    \"longitude\": pd.to_numeric(nuclear_df[\"Longitude\"], errors=\"coerce\"),\n",
        "    \"HazardType\": \"Nuclear\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rah3ARHoWg4D"
      },
      "outputs": [],
      "source": [
        "cleaned_nuclear_df = cleaned_nuclear_df.copy()\n",
        "\n",
        "# Extract last 6 digits of original G-code\n",
        "cleaned_nuclear_df[\"unit_id_digits\"] = cleaned_nuclear_df[\"HazardID\"].str[-6:]\n",
        "\n",
        "# Add prefix '1110' to make a uniform 10-digit numeric HazardID\n",
        "cleaned_nuclear_df[\"HazardID\"] = (\"1110\" + cleaned_nuclear_df[\"unit_id_digits\"]).astype(\"int64\")\n",
        "\n",
        "# Optional: drop helper column\n",
        "cleaned_nuclear_df.drop(columns=[\"unit_id_digits\"], inplace=True)\n",
        "cleaned_nuclear_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgEzYtjtWg4E"
      },
      "source": [
        "# Concatenate all data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pEu9isDWg4F"
      },
      "outputs": [],
      "source": [
        "#Concatenate fault data to the rest of the datasets\n",
        "ghz_df = pd.concat([vo_df, ls_df, ts_df, eq_df, faultdb, cleaned_nuclear_df]) #All Geohazards dataframe\n",
        "ghz_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQmk93e8Wg4F"
      },
      "outputs": [],
      "source": [
        "ghz_df.set_crs(epsg=4326, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNL-ztfhWg4G"
      },
      "outputs": [],
      "source": [
        "# Save locally in a supported format\n",
        "output_path = \"D:/NDIS_Database/ghz.gpkg\"  # Adjust as needed\n",
        "ghz_df.to_file(output_path, driver=\"GPKG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4hxKhRTWg4G"
      },
      "outputs": [],
      "source": [
        "ghz_df.drop('geometry',axis=1).to_csv(\"D:/NDIS_Database/ghz84.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnUPapk3Wg4G"
      },
      "outputs": [],
      "source": [
        "# For query purpose save it into sql\n",
        "# Step 1: Create or open the SQLite database\n",
        "conn = sqlite3.connect(\"ghz_data.sqlite\")\n",
        "\n",
        "# Step 2: Save the full multi-hazard DataFrame\n",
        "ghz_df.to_sql(\"hazards\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "# Step 3: Close the connection\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehxjnwxrWg4H"
      },
      "source": [
        "## <font color='red'> Countries EEZ Data </font>\n",
        "Retrieved from Maritime Boundaries and Exclusive Economic Zones (200NM), version 12\n",
        "\n",
        "https://www.marineregions.org/. https://doi.org/10.14284/632\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Citation:</b>\n",
        "    Flanders Marine Institute (2024). Union of the ESRI Country shapefile and the Exclusive Economic Zones (version 4). Available online at https://www.marineregions.org/. https://doi.org/10.14284/698. Consulted on 2025-02-20\n",
        "Further info:\n",
        "https://www.marineregions.org/downloads.php#unioneezcountry\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjCXr__PWg4H"
      },
      "source": [
        "## Test Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCWGNGm8Wg4H"
      },
      "outputs": [],
      "source": [
        "ghz_gpkg = r\"D:\\NDIS_Database\\ghz84.gpkg\"\n",
        "# Load the GeoPackage\n",
        "ghz = gpd.read_file(ghz_gpkg, layer=\"ghz84\")\n",
        "ghz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WyqraipWg4H"
      },
      "outputs": [],
      "source": [
        "ghz.crs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajPgPvhoWg4I"
      },
      "source": [
        "----------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo1QrS_cWg4I"
      },
      "source": [
        "# <font color='red'> Clip and Near Analysis </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCn3ELgBWg4I"
      },
      "source": [
        "### CLIP and NEAR Analysis for the entire region"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoNWpjqhWg4J"
      },
      "source": [
        "Divide the dataset into regions EEZ (country+ocean territory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo5SAv9sWg4J"
      },
      "outputs": [],
      "source": [
        "# Read the shapefile from local disk\n",
        "eez = gpd.read_file(r\"D:\\NDIS_Database\\00_ClippingRegion\\EEZ_land_union_v4_202410\\EEZ_land_union_v4_202410.shp\")\n",
        "eez.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tajb0TDWg4J"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGYNr_bDWg4J"
      },
      "source": [
        "### EEZ Data Treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSfzdzXrWg4K"
      },
      "outputs": [],
      "source": [
        "# Set the path to this geodatabase\n",
        "gdb_path = r\"D:\\ArcGISProjects\\GeohazardDB\\GeohazardDB.gdb\"  # Update the path accordingly\n",
        "gdb_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25O1PodSWg4K"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNDio-kSWg4L"
      },
      "outputs": [],
      "source": [
        "# Define feature classes\n",
        "eez_country = \"EEZ_country\"\n",
        "eez_union = \"EEZ_union\"\n",
        "\n",
        "# Step 1: Generate Near Table\n",
        "near_table = \"EEZ_near_table\"\n",
        "if not arcpy.Exists(near_table):\n",
        "    print(\"‚ö†Ô∏è Generating Near Table...\")\n",
        "    arcpy.GenerateNearTable_analysis(eez_union, eez_country, near_table, closest=\"CLOSEST\", method=\"PLANAR\")\n",
        "\n",
        "# Step 2: Add NEAR_FID to EEZ_union\n",
        "arcpy.JoinField_management(eez_union, \"OBJECTID\", near_table, \"IN_FID\", [\"NEAR_FID\"])\n",
        "\n",
        "# Step 3: Merge EEZ_union into EEZ_country using NEAR_FID\n",
        "with arcpy.da.UpdateCursor(eez_country, [\"OBJECTID\", \"SHAPE@\"]) as country_cursor:\n",
        "    for country_row in country_cursor:\n",
        "        country_id = country_row[0]\n",
        "        country_geom = country_row[1]\n",
        "\n",
        "        # Collect matching EEZ_union geometries\n",
        "        union_geoms = []\n",
        "        with arcpy.da.SearchCursor(eez_union, [\"NEAR_FID\", \"SHAPE@\"]) as union_cursor:\n",
        "            for union_row in union_cursor:\n",
        "                if union_row[0] == country_id and union_row[1] is not None:\n",
        "                    union_geoms.append(union_row[1])\n",
        "\n",
        "        # Merge all geometries if matches exist\n",
        "        if union_geoms:\n",
        "            merged_geom = reduce(lambda x, y: x.union(y), [country_geom] + union_geoms)\n",
        "            country_row[1] = merged_geom\n",
        "            country_cursor.updateRow(country_row)\n",
        "\n",
        "print(\"‚úÖ Finished merging polygons. EEZ_country now contains 275 polygons.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPDhBto3Wg4L"
      },
      "outputs": [],
      "source": [
        "# Define feature classes\n",
        "eez = \"EEZ24\"\n",
        "# Field to check (change if needed)\n",
        "unique_field = \"FID\"\n",
        "\n",
        "# Get unique values\n",
        "unique_values = set()\n",
        "with arcpy.da.SearchCursor(eez, [unique_field]) as cursor:\n",
        "    for row in cursor:\n",
        "        if row[0]:  # Ignore Null values\n",
        "            unique_values.add(row[0])\n",
        "\n",
        "# Print results\n",
        "print(f\"‚úÖ Unique {unique_field} count: {len(unique_values)}\")\n",
        "if len(unique_values) == 327:\n",
        "    print(\"üéâ The field has exactly 327 unique values, matching the number of polygons!\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Mismatch! Expected 327 but found {len(unique_values)}. Check for duplicates or missing values.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iL8LCo6Wg4M"
      },
      "outputs": [],
      "source": [
        "# Field to check\n",
        "field_name = \"ISO_SOV1\"\n",
        "search_value = \"AAT\"\n",
        "\n",
        "# Check if \"ANT\" exists\n",
        "exists = False\n",
        "with arcpy.da.SearchCursor(eez, [field_name]) as cursor:\n",
        "    for row in cursor:\n",
        "        if row[0] == search_value:\n",
        "            exists = True\n",
        "            break  # No need to continue once found\n",
        "\n",
        "# Print result\n",
        "if exists:\n",
        "    print(f\"‚úÖ '{search_value}' exists in the field '{field_name}'.\")\n",
        "else:\n",
        "    print(f\"‚ùå '{search_value}' was NOT found in '{field_name}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdHt8AE2Wg4M"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D34bU8gEWg4N"
      },
      "outputs": [],
      "source": [
        "input_eez = \"EEZ24\"\n",
        "\n",
        "# Create a set to track used ISO_TER1 values and if NaN or duplicates, rename it from TERRITORY1 abbreviation\n",
        "used_names = set()\n",
        "\n",
        "# Function to generate a unique 3-letter code\n",
        "def generate_unique_code(existing_names):\n",
        "    while True:\n",
        "        # Generate a random 3-letter combination\n",
        "        code = ''.join(random.choices(string.ascii_uppercase, k=3))\n",
        "        if code not in existing_names:  # Ensure it's unique\n",
        "            return code\n",
        "\n",
        "# Step 1: Update the ISO_TER1 values\n",
        "with arcpy.da.UpdateCursor(input_eez, ['ISO_TER1', 'TERRITORY1']) as cursor:\n",
        "    for row in cursor:\n",
        "        iso_name = row[0]\n",
        "        territory_name = row[1]\n",
        "\n",
        "        # Check if this ISO_TER1 value is empty or not valid\n",
        "        if not iso_name or len(iso_name) != 3 or not iso_name.isalpha():\n",
        "            # Generate a unique 3-letter code\n",
        "            unique_code = generate_unique_code(used_names)\n",
        "            row[0] = unique_code  # Update ISO_TER1 value\n",
        "            used_names.add(unique_code)  # Add to the set of used names\n",
        "\n",
        "        # If it is valid and unique, keep the original value\n",
        "        cursor.updateRow(row)\n",
        "\n",
        "print(\"‚úÖ ISO_TER1 values have been updated to unique 3-letter names.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvvuTpYGWg4N"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Set input dataset\n",
        "input_eez = \"EEZ\"\n",
        "\n",
        "# Track used ISO_TER1 values\n",
        "used_names = set()\n",
        "\n",
        "# Track duplicates: {ISO_TER1: [rows]}\n",
        "iso_to_rows = defaultdict(list)\n",
        "\n",
        "# Step 1: First Pass - Identify duplicates and invalid values\n",
        "with arcpy.da.UpdateCursor(input_eez, ['ISO_TER1', 'UNION']) as cursor:\n",
        "    for row in cursor:\n",
        "        iso_name = row[0]\n",
        "        territory_name = row[1]\n",
        "\n",
        "        # Store existing values for duplication checks\n",
        "        if iso_name and len(iso_name) == 3 and iso_name.isalpha():\n",
        "            if iso_name in used_names:\n",
        "                iso_to_rows[iso_name].append(row)  # Mark as duplicate\n",
        "            else:\n",
        "                used_names.add(iso_name)  # Add valid unique code\n",
        "        else:\n",
        "            iso_to_rows[\"INVALID\"].append(row)  # Mark invalid values\n",
        "\n",
        "# Function to generate a unique 3-letter code\n",
        "def generate_unique_code(prefix, existing_names):\n",
        "    while True:\n",
        "        # Generate a code using the first 2 letters of UNION + 1 random letter\n",
        "        if prefix and len(prefix) >= 2:\n",
        "            code = prefix[:2].upper() + random.choice(string.ascii_uppercase)\n",
        "        else:\n",
        "            code = ''.join(random.choices(string.ascii_uppercase, k=3))\n",
        "\n",
        "        if code not in existing_names:  # Ensure uniqueness\n",
        "            return code\n",
        "\n",
        "# Step 2: Second Pass - Fix duplicates and invalid values\n",
        "with arcpy.da.UpdateCursor(input_eez, ['ISO_TER1', 'UNION']) as cursor:\n",
        "    for row in cursor:\n",
        "        iso_name = row[0]\n",
        "        territory_name = row[1]\n",
        "\n",
        "        # If this row was marked as a duplicate or invalid, update it\n",
        "        if iso_name in iso_to_rows or iso_name == \"INVALID\":\n",
        "            prefix = territory_name[:2] if territory_name else \"\"\n",
        "            unique_code = generate_unique_code(prefix, used_names)\n",
        "            row[0] = unique_code\n",
        "            used_names.add(unique_code)\n",
        "\n",
        "        cursor.updateRow(row)\n",
        "\n",
        "print(\"‚úÖ ISO_TER1 values updated: duplicates fixed, invalid values replaced.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiJCJsfnWg4O"
      },
      "source": [
        "---------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIurtSTgWg4O"
      },
      "source": [
        "### Test Using Smaller area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk3rqtX8Wg4P"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkKh0ixEWg4P"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "# Define paths\n",
        "project_folder = r\"D:\\ArcGISProjects\\GeohazardDB\"\n",
        "gdb_path = os.path.join(project_folder, \"GeohazardDB.gdb\")  # File Geodatabase\n",
        "\n",
        "# Initialize a list to store processed geohazard datasets\n",
        "ghz_list = []\n",
        "\n",
        "# Input datasets\n",
        "geohazard_layer = os.path.join(gdb_path, \"g84_Clip\")  # Geohazard dataset\n",
        "road_layer = os.path.join(gdb_path, \"grip_Clip\")  # Global road dataset\n",
        "country_layer = os.path.join(gdb_path, \"eez3\")  # Country boundaries\n",
        "\n",
        "# Check if GDB exists, otherwise create it\n",
        "if not arcpy.Exists(gdb_path):\n",
        "    arcpy.CreateFileGDB_management(project_folder, \"GeohazardDB.gdb\")\n",
        "\n",
        "# Iterate through each country\n",
        "with arcpy.da.SearchCursor(country_layer, [\"ISO_TER1\", \"SHAPE@\"]) as country_cursor:\n",
        "    for row in country_cursor:\n",
        "        country_code = row[0]  # Country code (e.g., THA, USA)\n",
        "        country_geometry = row[1]  # Country boundary geometry\n",
        "\n",
        "        print(f\"\\u23F3 Processing country: {country_code}...\")\n",
        "\n",
        "        # Define output names inside the GDB\n",
        "        ghz_clip = os.path.join(gdb_path, f\"ghz_{country_code}\")\n",
        "        road_clip = os.path.join(gdb_path, f\"road_{country_code}\")\n",
        "        near_output = os.path.join(gdb_path, f\"near_{country_code}\")\n",
        "\n",
        "        # ---- Step 1: Clip Geohazard Data ----\n",
        "        if arcpy.Exists(ghz_clip):\n",
        "            arcpy.Delete_management(ghz_clip)\n",
        "        try:\n",
        "            arcpy.Clip_analysis(geohazard_layer, country_geometry, ghz_clip)\n",
        "            print(f\"  \\u2705 Clipped geohazard layer: {ghz_clip}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  \\u274C Error clipping geohazard: {e}\")\n",
        "            continue  # Skip to next country if error occurs\n",
        "\n",
        "        # ---- Step 2: Clip Road Data ----\n",
        "        if arcpy.Exists(road_clip):\n",
        "            arcpy.Delete_management(road_clip)\n",
        "        try:\n",
        "            arcpy.Clip_analysis(road_layer, country_geometry, road_clip)\n",
        "            print(f\"  \\u2705 Clipped road layer: {road_clip}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  {chr(0x274C)} Error clipping roads: {e}\")\n",
        "            continue  # Skip to next country if error occurs\n",
        "\n",
        "        # ---- Step 3: Perform Near Analysis ----\n",
        "        if arcpy.Exists(near_output):\n",
        "            arcpy.Delete_management(near_output)\n",
        "        try:\n",
        "            arcpy.GenerateNearTable_analysis(\n",
        "                in_features=ghz_clip,\n",
        "                near_features=road_clip,\n",
        "                out_table=near_output,\n",
        "                search_radius=\"100000 Meters\",  # Keep it in 100 Km\n",
        "                location=\"LOCATION\",  # Include X, Y coordinates\n",
        "                angle=\"ANGLE\",\n",
        "                closest=\"CLOSEST\",\n",
        "                method=\"GEODESIC\"\n",
        "            )\n",
        "            print(f\"  \\u2705 Near analysis completed: {near_output}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  \\u274C Error in Near Analysis: {e}\")\n",
        "\n",
        "\n",
        "        # ---- Step 4: Add distance to geohazard ----\n",
        "        # Add \"distance\" field if it doesn't exist\n",
        "        if \"distance\" not in [f.name for f in arcpy.ListFields(ghz_clip)]:\n",
        "            arcpy.AddField_management(ghz_clip, \"distance\", \"DOUBLE\")\n",
        "\n",
        "        # Update \"distance\" field with NEAR_DIST from the near table\n",
        "        with arcpy.da.UpdateCursor(ghz_clip, [\"OBJECTID\", \"distance\"]) as ghz_cursor:\n",
        "            for ghz_row in ghz_cursor:\n",
        "                ghz_id = ghz_row[0]  # OBJECTID of ghz_XXX\n",
        "\n",
        "                # Find matching NEAR_DIST from the near table\n",
        "                with arcpy.da.SearchCursor(near_output, [\"IN_FID\", \"NEAR_DIST\"]) as near_cursor:\n",
        "                    for near_row in near_cursor:\n",
        "                        if near_row[0] == ghz_id:  # Match OBJECTID\n",
        "                            ghz_row[1] = near_row[1]  # Assign NEAR_DIST\n",
        "                            ghz_cursor.updateRow(ghz_row)\n",
        "                            break  # Stop once found\n",
        "\n",
        "        print(f\"\\u2705 NEAR_DIST added to {ghz_clip} as 'distance' field.\")\n",
        "\n",
        "        # Add \"HazardID\" field to near table if it doesn't exist\n",
        "        if \"HazardID\" not in [f.name for f in arcpy.ListFields(near_output)]:\n",
        "            arcpy.AddField_management(near_output, \"HazardID\", \"TEXT\")\n",
        "\n",
        "        # Since this uses \"CLOSEST\" method, so it will only take 1 closest disance,\n",
        "        # and discard the NEAR_FID as the 1:N will no longer needed. Instead it can be replaced with HazardID\n",
        "        # to obtain the relationship with the ghz dataset\n",
        "        # Use UpdateCursor to populate HazardID from ghz_XXX\n",
        "        with arcpy.da.UpdateCursor(near_output, [\"IN_FID\", \"HazardID\"]) as cursor:\n",
        "            for row in cursor:\n",
        "                # Fetch the corresponding HazardID from ghz_XXX\n",
        "                with arcpy.da.SearchCursor(ghz_clip, [\"OBJECTID\", \"HazardID\"]) as ghz_cursor:\n",
        "                    for ghz_row in ghz_cursor:\n",
        "                        if row[0] == ghz_row[0]:  # Match OBJECTID\n",
        "                            row[1] = ghz_row[1]  # Assign HazardID\n",
        "                            cursor.updateRow(row)\n",
        "                            break  # Exit loop once matched\n",
        "\n",
        "        print(f\"\\u2705 HazardID added to {near_output}, replacing NEAR_FID reference.\")\n",
        "\n",
        "        # Add processed dataset to list for final merge\n",
        "        ghz_list.append(ghz_clip)\n",
        "\n",
        "# Merge all processed ghz_XXX datasets into one: \"ghz_dist\"\n",
        "ghz_dist = os.path.join(gdb_path, \"ghz_dist\")\n",
        "\n",
        "if arcpy.Exists(ghz_dist):\n",
        "    arcpy.Delete_management(ghz_dist)  # Ensure a fresh start\n",
        "\n",
        "arcpy.Merge_management(ghz_list, ghz_dist)\n",
        "\n",
        "print(f\" All geohazard data merged into {ghz_dist} successfully!\")\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efA7XdPyWg4P"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy0znauTWg4Q"
      },
      "source": [
        "# <h1 style=\"background-color:#d5f2e1; color: #bc6ee0;\">Clip and Near Analysis Including Outside EEZ</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgfZQ3e3Wg4Q"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "HYfvixqhWg4Q"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "\n",
        "# Define paths\n",
        "project_folder = r\"D:\\ArcGISProjects\\GeohazardDB\"\n",
        "gdb_path = os.path.join(project_folder, \"GeohazardDB.gdb\")  # File Geodatabase\n",
        "\n",
        "# Initialize lists to store processed geohazard datasets and near tables\n",
        "ghz_list = []\n",
        "near_tables = []\n",
        "outside_eez_list = []  # List to store datasets with points outside EEZ\n",
        "\n",
        "# Input datasets\n",
        "geohazard_layer = os.path.join(gdb_path, \"ghz84\")  # Geohazard dataset\n",
        "road_layer = os.path.join(gdb_path, \"roads\")  # Global road dataset\n",
        "country_layer = os.path.join(gdb_path, \"eez_country\")  # Country boundaries\n",
        "\n",
        "# Check if GDB exists, otherwise create it\n",
        "if not arcpy.Exists(gdb_path):\n",
        "    arcpy.CreateFileGDB_management(project_folder, \"GeohazardDB.gdb\")\n",
        "\n",
        "# Get total number of countries to process\n",
        "total_countries = int(arcpy.GetCount_management(country_layer)[0])\n",
        "\n",
        "# Iterate through each country\n",
        "with arcpy.da.SearchCursor(country_layer, [\"ISO_TER1\", \"SHAPE@\"]) as country_cursor:\n",
        "    for index, row in enumerate(country_cursor, start=1):\n",
        "        country_code = row[0]  # Country code (e.g., THA, USA)\n",
        "        country_geometry = row[1]  # Country boundary geometry\n",
        "\n",
        "        print(f\"\\u23F3 Processing country {index}/{total_countries}: {country_code}...\")\n",
        "\n",
        "        # Define output names inside the GDB\n",
        "        ghz_clip = os.path.join(gdb_path, f\"ghz_{country_code}\")\n",
        "        road_clip = os.path.join(gdb_path, f\"road_{country_code}\")\n",
        "        near_output = os.path.join(gdb_path, f\"near_{country_code}\")\n",
        "\n",
        "        # ---- Step 1: Clip Geohazard Data ----\n",
        "        if arcpy.Exists(ghz_clip):\n",
        "            arcpy.Delete_management(ghz_clip)\n",
        "        try:\n",
        "            arcpy.Clip_analysis(geohazard_layer, country_geometry, ghz_clip)\n",
        "            print(f\"  \\u2705 Clipped geohazard layer: {ghz_clip}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  \\u274C Error clipping geohazard: {e}\")\n",
        "            continue  # Skip to next country if error occurs\n",
        "\n",
        "        # ---- Step 2: Clip Road Data ----\n",
        "        if arcpy.Exists(road_clip):\n",
        "            arcpy.Delete_management(road_clip)\n",
        "        try:\n",
        "            arcpy.Clip_analysis(road_layer, country_geometry, road_clip)\n",
        "            print(f\"  \\u2705 Clipped road layer: {road_clip}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  {chr(0x274C)} Error clipping roads: {e}\")\n",
        "            continue  # Skip to next country if error occurs\n",
        "\n",
        "        # ---- Step 3: Perform Near Analysis ----\n",
        "        if arcpy.Exists(near_output):\n",
        "            arcpy.Delete_management(near_output)\n",
        "        try:\n",
        "            arcpy.GenerateNearTable_analysis(\n",
        "                in_features=ghz_clip,\n",
        "                near_features=road_clip,\n",
        "                out_table=near_output,\n",
        "                search_radius=\"\",  # No search radius\n",
        "                location=\"LOCATION\",  # Include X, Y coordinates\n",
        "                angle=\"ANGLE\",\n",
        "                closest=\"CLOSEST\",\n",
        "                method=\"GEODESIC\"\n",
        "            )\n",
        "            print(f\"  \\u2705 Near analysis completed: {near_output}\")\n",
        "            near_tables.append(near_output)  # Store near table\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  \\u274C Error in Near Analysis: {e}\")\n",
        "\n",
        "        # ---- Step 4: Add distance to geohazard ----\n",
        "        # Add \"distance\" field if it doesn't exist\n",
        "        if \"distance\" not in [f.name for f in arcpy.ListFields(ghz_clip)]:\n",
        "            arcpy.AddField_management(ghz_clip, \"distance\", \"DOUBLE\")\n",
        "\n",
        "        # Update \"distance\" field with NEAR_DIST from the near table\n",
        "        with arcpy.da.UpdateCursor(ghz_clip, [\"OBJECTID\", \"distance\"]) as ghz_cursor:\n",
        "            for ghz_row in ghz_cursor:\n",
        "                ghz_id = ghz_row[0]  # OBJECTID of ghz_XXX\n",
        "\n",
        "                # Find matching NEAR_DIST from the near table\n",
        "                found_match = False\n",
        "                with arcpy.da.SearchCursor(near_output, [\"IN_FID\", \"NEAR_DIST\"]) as near_cursor:\n",
        "                    for near_row in near_cursor:\n",
        "                        if near_row[0] == ghz_id:  # Match OBJECTID\n",
        "                            ghz_row[1] = near_row[1]  # Assign NEAR_DIST\n",
        "                            ghz_cursor.updateRow(ghz_row)\n",
        "                            found_match = True\n",
        "                            break  # Stop once found\n",
        "\n",
        "                if not found_match:  # If no match found, add to outside_eez list\n",
        "                    outside_eez_list.append(ghz_clip)\n",
        "\n",
        "        print(f\"\\u2705 NEAR_DIST added to {ghz_clip} as 'distance' field.\")\n",
        "\n",
        "        # Add \"HazardID\" field to near table if it doesn't exist\n",
        "        if \"HazardID\" not in [f.name for f in arcpy.ListFields(near_output)]:\n",
        "            arcpy.AddField_management(near_output, \"HazardID\", \"TEXT\")\n",
        "\n",
        "        # Use UpdateCursor to populate HazardID from ghz_XXX\n",
        "        with arcpy.da.UpdateCursor(near_output, [\"IN_FID\", \"HazardID\"]) as cursor:\n",
        "            for row in cursor:\n",
        "                # Fetch the corresponding HazardID from ghz_XXX\n",
        "                with arcpy.da.SearchCursor(ghz_clip, [\"OBJECTID\", \"HazardID\"]) as ghz_cursor:\n",
        "                    for ghz_row in ghz_cursor:\n",
        "                        if row[0] == ghz_row[0]:  # Match OBJECTID\n",
        "                            row[1] = ghz_row[1]  # Assign HazardID\n",
        "                            cursor.updateRow(row)\n",
        "                            break  # Exit loop once matched\n",
        "\n",
        "        print(f\"\\u2705 HazardID added to {near_output}, replacing NEAR_FID reference.\")\n",
        "\n",
        "        # Add processed dataset to list for final merge\n",
        "        ghz_list.append(ghz_clip)\n",
        "\n",
        "# Merge all processed ghz_XXX datasets into one: \"ghz_dist\"\n",
        "ghz_dist = os.path.join(gdb_path, \"ghz_dist\")\n",
        "\n",
        "if arcpy.Exists(ghz_dist):\n",
        "    arcpy.Delete_management(ghz_dist)  # Ensure a fresh start\n",
        "\n",
        "arcpy.Merge_management(ghz_list, ghz_dist)\n",
        "\n",
        "print(f\" All geohazard data merged into {ghz_dist} successfully!\")\n",
        "\n",
        "# ---- Step 5: Compile all near tables into one ----\n",
        "compiled_near_table = os.path.join(gdb_path, \"compiled_near_table\")\n",
        "if arcpy.Exists(compiled_near_table):\n",
        "    arcpy.Delete_management(compiled_near_table)\n",
        "\n",
        "# Create empty table to store results\n",
        "arcpy.CreateTable_management(gdb_path, \"compiled_near_table\")\n",
        "arcpy.AddField_management(compiled_near_table, \"FROM_X\", \"DOUBLE\")\n",
        "arcpy.AddField_management(compiled_near_table, \"FROM_Y\", \"DOUBLE\")\n",
        "arcpy.AddField_management(compiled_near_table, \"NEAR_X\", \"DOUBLE\")\n",
        "arcpy.AddField_management(compiled_near_table, \"NEAR_Y\", \"DOUBLE\")\n",
        "arcpy.AddField_management(compiled_near_table, \"NEAR_FID\", \"LONG\")\n",
        "arcpy.AddField_management(compiled_near_table, \"HazardID\", \"TEXT\")\n",
        "\n",
        "# Insert cursor for compiled near table\n",
        "with arcpy.da.InsertCursor(compiled_near_table, [\"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\", \"NEAR_FID\", \"HazardID\"]) as insert_cursor:\n",
        "    for near_table in near_tables:\n",
        "        with arcpy.da.SearchCursor(near_table, [\"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\", \"NEAR_FID\", \"HazardID\"]) as cursor:\n",
        "            for row in cursor:\n",
        "                insert_cursor.insertRow(row)\n",
        "\n",
        "print(f\"\\u270 All near tables compiled into {compiled_near_table} successfully!\")\n",
        "\n",
        "# ---- Step 6: Handle points outside EEZ ----\n",
        "outside_eez_output = os.path.join(gdb_path, \"outside_eez\")\n",
        "\n",
        "# Try deleting the feature class if it exists (avoid locking issues)\n",
        "if arcpy.Exists(outside_eez_output):\n",
        "    try:\n",
        "        arcpy.Delete_management(outside_eez_output)\n",
        "        print(f\"\\u270 Deleted existing {outside_eez_output}\")\n",
        "    except Exception as e:\n",
        "        print(f\"&#9888 Error deleting {outside_eez_output}: {e}\")\n",
        "        time.sleep(2)  # Wait a bit and retry\n",
        "        arcpy.Delete_management(outside_eez_output)\n",
        "\n",
        "# Get all field names from geohazard_layer (excluding OBJECTID & Shape)\n",
        "fields = [f.name for f in arcpy.ListFields(geohazard_layer) if f.type not in (\"OID\", \"Geometry\")]\n",
        "fields.insert(0, \"SHAPE@\")  # Ensure geometry is included\n",
        "\n",
        "# Create a new feature class with the same schema\n",
        "try:\n",
        "    arcpy.CreateFeatureclass_management(\n",
        "        gdb_path, \"outside_eez\", \"POINT\",\n",
        "        template=geohazard_layer,  # Preserve schema\n",
        "        spatial_reference=arcpy.Describe(geohazard_layer).spatialReference\n",
        "    )\n",
        "    print(\"\\u270 Successfully created outside_eez feature class\")\n",
        "except Exception as e:\n",
        "    print(f\"\\u274C Failed to create outside_eez: {e}\")\n",
        "\n",
        "# ---- Step 7: Create Polyline Feature Class from Compiled Near Table ----\n",
        "polyline_output = os.path.join(gdb_path, \"compiled_near_lines\")\n",
        "if arcpy.Exists(polyline_output):\n",
        "    arcpy.Delete_management(polyline_output)\n",
        "\n",
        "# Use XY To Line tool to create lines from compiled near table\n",
        "arcpy.XYToLine_management(\n",
        "    compiled_near_table,\n",
        "    polyline_output,\n",
        "    \"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\",\n",
        "    \"\",  # Optional Line ID field\n",
        ")\n",
        "\n",
        "print(f\"\\u270 Polylines created from compiled near table at {polyline_output}!\")\n",
        "\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\" % str(elapsed / 60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nTxUh7MWg4R"
      },
      "outputs": [],
      "source": [
        "# ---- Step 6: Handle points outside EEZ ----\n",
        "outside_eez_output = os.path.join(gdb_path, \"outside_eez\")\n",
        "\n",
        "# Try deleting the feature class if it exists (avoid locking issues)\n",
        "if arcpy.Exists(outside_eez_output):\n",
        "    try:\n",
        "        arcpy.Delete_management(outside_eez_output)\n",
        "        print(f\"‚úÖ Deleted existing {outside_eez_output}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error deleting {outside_eez_output}: {e}\")\n",
        "        time.sleep(2)  # Wait a bit and retry\n",
        "        arcpy.Delete_management(outside_eez_output)\n",
        "\n",
        "# Get all field names from geohazard_layer (excluding OBJECTID & Shape)\n",
        "fields = [f.name for f in arcpy.ListFields(geohazard_layer) if f.type not in (\"OID\", \"Geometry\")]\n",
        "fields.insert(0, \"SHAPE@\")  # Ensure geometry is included\n",
        "\n",
        "# Create a new feature class with the same schema\n",
        "try:\n",
        "    arcpy.CreateFeatureclass_management(\n",
        "        gdb_path, \"outside_eez\", \"POINT\",\n",
        "        template=geohazard_layer,  # Preserve schema\n",
        "        spatial_reference=arcpy.Describe(geohazard_layer).spatialReference\n",
        "    )\n",
        "    print(\"‚úÖ Successfully created outside_eez feature class\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to create outside_eez: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbzqRsCUWg4S"
      },
      "outputs": [],
      "source": [
        "# ---- Step 7: Create Polyline Feature Class from Compiled Near Table ----\n",
        "polyline_output = os.path.join(gdb_path, \"compiled_near_lines\")\n",
        "if arcpy.Exists(polyline_output):\n",
        "    arcpy.Delete_management(polyline_output)\n",
        "\n",
        "# Use XY To Line tool to create lines from compiled near table\n",
        "arcpy.XYToLine_management(\n",
        "    compiled_near_table,\n",
        "    polyline_output,\n",
        "    \"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\",\n",
        "    \"\",  # Optional Line ID field\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Polylines created from compiled near table at {polyline_output}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJi_b929Wg4S"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Paths\n",
        "source_gdb = r\"D:\\ArcGISProjects\\GeohazardDB\\GeohazardDB.gdb\"\n",
        "output_gdb = r\"D:\\ArcGISProjects\\GeohazardDB\\NDIS.gdb\"\n",
        "\n",
        "# Make sure output GDB exists\n",
        "if not arcpy.Exists(output_gdb):\n",
        "    arcpy.CreateFileGDB_management(os.path.dirname(output_gdb), os.path.basename(output_gdb))\n",
        "\n",
        "# Example test country codes ‚Äî replace or expand this list as needed\n",
        "test_country_codes = [\"THA\", \"IDN\", \"JPN\"]\n",
        "\n",
        "for country_code in test_country_codes:\n",
        "    ghz_fc = os.path.join(source_gdb, f\"ghz_{country_code}\")\n",
        "    road_fc = os.path.join(source_gdb, f\"road_{country_code}\")\n",
        "    near_table = os.path.join(output_gdb, f\"near_{country_code}\")\n",
        "    point_output = os.path.join(output_gdb, f\"near_points_{country_code}\")\n",
        "\n",
        "    print(f\"\\n--- Processing {country_code} ---\")\n",
        "\n",
        "    # Check if input features exist\n",
        "    if not arcpy.Exists(ghz_fc) or not arcpy.Exists(road_fc):\n",
        "        print(f\"  ‚õî Skipped: Missing clipped layers for {country_code}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Step 1: Generate Near Table\n",
        "        if arcpy.Exists(near_table):\n",
        "            arcpy.Delete_management(near_table)\n",
        "        arcpy.GenerateNearTable_analysis(\n",
        "            in_features=ghz_fc,\n",
        "            near_features=road_fc,\n",
        "            out_table=near_table,\n",
        "            location=\"LOCATION\",\n",
        "            angle=\"ANGLE\",\n",
        "            closest=\"CLOSEST\",\n",
        "            method=\"GEODESIC\"\n",
        "        )\n",
        "        print(f\"  ‚úÖ Near table created: {near_table}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error generating near table: {e}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Step 2: Convert XY to Point\n",
        "        if arcpy.Exists(point_output):\n",
        "            arcpy.Delete_management(point_output)\n",
        "        arcpy.XYTableToPoint_management(\n",
        "            in_table=near_table,\n",
        "            out_feature_class=point_output,\n",
        "            x_field=\"NEAR_X\",\n",
        "            y_field=\"NEAR_Y\",\n",
        "            coordinate_system=arcpy.Describe(ghz_fc).spatialReference\n",
        "        )\n",
        "        print(f\"  ‚úÖ XY to Point completed: {point_output}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error converting to point: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBSUyFf6Wg4S"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "# Paths\n",
        "source_gdb = r\"D:\\ArcGISProjects\\GeohazardDB\\GeohazardDB.gdb\"\n",
        "output_gdb = r\"D:\\ArcGISProjects\\GeohazardDB\\NDIS.gdb\"\n",
        "\n",
        "# Field schema for compiled table\n",
        "fields = [\n",
        "    (\"FROM_X\", \"DOUBLE\"),\n",
        "    (\"FROM_Y\", \"DOUBLE\"),\n",
        "    (\"NEAR_X\", \"DOUBLE\"),\n",
        "    (\"NEAR_Y\", \"DOUBLE\"),\n",
        "    (\"NEAR_FID\", \"LONG\"),\n",
        "    (\"HazardID\", \"TEXT\")\n",
        "]\n",
        "\n",
        "# Create output GDB if needed\n",
        "if not arcpy.Exists(output_gdb):\n",
        "    arcpy.CreateFileGDB_management(os.path.dirname(output_gdb), os.path.basename(output_gdb))\n",
        "\n",
        "# Prepare compiled near table\n",
        "compiled_table = os.path.join(output_gdb, \"compiled_near_table\")\n",
        "if arcpy.Exists(compiled_table):\n",
        "    arcpy.Delete_management(compiled_table)\n",
        "arcpy.CreateTable_management(output_gdb, \"compiled_near_table\")\n",
        "for name, ftype in fields:\n",
        "    arcpy.AddField_management(compiled_table, name, ftype)\n",
        "\n",
        "# Load all country codes from EEZ feature class\n",
        "eez_fc = os.path.join(source_gdb, \"eez_country\")\n",
        "country_codes = [row[0] for row in arcpy.da.SearchCursor(eez_fc, [\"ISO_TER1\"])]\n",
        "\n",
        "# Loop through all countries\n",
        "for code in country_codes:\n",
        "    ghz_fc = os.path.join(source_gdb, f\"ghz_{code}\")\n",
        "    road_fc = os.path.join(source_gdb, f\"road_{code}\")\n",
        "    near_table = os.path.join(output_gdb, f\"near_{code}\")\n",
        "\n",
        "    print(f\"\\U0001f373 Processing {code}...\")\n",
        "\n",
        "    if not arcpy.Exists(ghz_fc) or not arcpy.Exists(road_fc):\n",
        "        print(f\"  ‚õî Missing input for {code}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Generate Near Table\n",
        "        if arcpy.Exists(near_table):\n",
        "            arcpy.Delete_management(near_table)\n",
        "        arcpy.GenerateNearTable_analysis(\n",
        "            in_features=ghz_fc,\n",
        "            near_features=road_fc,\n",
        "            out_table=near_table,\n",
        "            location=\"LOCATION\",\n",
        "            angle=\"ANGLE\",\n",
        "            closest=\"CLOSEST\",\n",
        "            method=\"GEODESIC\"\n",
        "        )\n",
        "        print(f\"  ‚úÖ Near table created: {near_table}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error generating near table for {code}: {e}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Append rows to compiled table\n",
        "        with arcpy.da.InsertCursor(compiled_table, [f[0] for f in fields]) as insert_cursor:\n",
        "            with arcpy.da.SearchCursor(near_table, [f[0] for f in fields]) as read_cursor:\n",
        "                for row in read_cursor:\n",
        "                    insert_cursor.insertRow(row)\n",
        "        print(f\"  ‚ûï Appended {code} to compiled table.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error appending rows for {code}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Final step: XY To Line from compiled table\n",
        "polyline_output = os.path.join(output_gdb, \"compiled_near_lines\")\n",
        "if arcpy.Exists(polyline_output):\n",
        "    arcpy.Delete_management(polyline_output)\n",
        "\n",
        "try:\n",
        "    arcpy.XYToLine_management(\n",
        "        in_table=compiled_table,\n",
        "        out_feature_class=polyline_output,\n",
        "        startx_field=\"FROM_X\",\n",
        "        starty_field=\"FROM_Y\",\n",
        "        endx_field=\"NEAR_X\",\n",
        "        endy_field=\"NEAR_Y\",\n",
        "        line_type=\"GEODESIC\"\n",
        "    )\n",
        "    print(f\"\\n‚úÖ Global polyline layer created: {polyline_output}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error creating polyline layer: {e}\")\n",
        "\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\" % str(elapsed / 60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frdjHOm3Wg4T"
      },
      "outputs": [],
      "source": [
        "compiled_lines = r\"D:\\ArcGISProjects\\GeohazardDB\\GeohazardDB.gdb\\compiled_near_lines\"\n",
        "\n",
        "# Add new field for length in meters\n",
        "if \"length_m\" not in [f.name for f in arcpy.ListFields(compiled_lines)]:\n",
        "    arcpy.AddField_management(compiled_lines, \"length_m\", \"DOUBLE\")\n",
        "\n",
        "# Calculate geodesic length in meters\n",
        "arcpy.CalculateGeometryAttributes_management(\n",
        "    in_features=compiled_lines,\n",
        "    geometry_property=[[\"distance_m\", \"LENGTH_GEODESIC\"]],\n",
        "    length_unit=\"METERS\"\n",
        ")\n",
        "print(\"‚úÖ length_m field populated with geodesic length in meters.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpFjUnJhWg4T"
      },
      "outputs": [],
      "source": [
        "compiled_table = r\"D:\\ArcGISProjects\\GeohazardDB\\GeohazardDB.gdb\\compiled_near_table\"\n",
        "arcpy.JoinField_management(\n",
        "    in_data=compiled_lines,\n",
        "    in_field=\"OID\",\n",
        "    join_table=compiled_table,\n",
        "    join_field=\"OBJECTID\",  # the join key\n",
        "    fields=[\"HazardID\"]\n",
        ")\n",
        "print(\"‚úÖ HazardID injected into compiled_near_lines.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33WqQAuSWg4U"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doWpWSCCWg4U"
      },
      "source": [
        "# Population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJKgTK_jWg4U"
      },
      "outputs": [],
      "source": [
        "ghz_pap = pd.read_csv(r\"D:\\NDIS_Database\\ghz_paper.csv\")\n",
        "ghz_pap.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "nNfa0I1yWg4V"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "# Parameters\n",
        "population_raster = r\"D:\\NDIS_Database\\12_Population_synthetic\\nasapopct.tif\"\n",
        "buffer_dist = 30000  # 30 km\n",
        "chunk_size = 10000\n",
        "\n",
        "output_dir = r\"D:\\NDIS_Database\\zonal_chunks\"\n",
        "\n",
        "# Check if it's a file, not a folder\n",
        "if os.path.exists(output_dir) and not os.path.isdir(output_dir):\n",
        "    os.remove(output_dir)\n",
        "\n",
        "# Now safely create the directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Convert DataFrame to GeoDataFrame\n",
        "ghz_pap[\"geometry\"] = ghz_pap.apply(\n",
        "    lambda row: Point(row[\"longitude\"], row[\"latitude\"]), axis=1\n",
        ")\n",
        "input_gdf = gpd.GeoDataFrame(ghz_pap, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
        "\n",
        "# Buffer in degrees (WGS84 safe approx)\n",
        "buffer_deg = buffer_dist / 111320.0\n",
        "\n",
        "# Tracking\n",
        "total = len(input_gdf)\n",
        "print(f\"üîπ Total features: {total}\")\n",
        "final_chunks = []\n",
        "suspicious_chunks = []\n",
        "\n",
        "for start in range(0, total, chunk_size):\n",
        "    end = min(start + chunk_size, total)\n",
        "    print(f\"‚è≥ Processing chunk {start+1} to {end}\")\n",
        "\n",
        "    chunk = input_gdf.iloc[start:end].copy()\n",
        "\n",
        "    # Suppress geographic CRS buffer warning\n",
        "    warnings.filterwarnings(\"ignore\", message=\"Geometry is in a geographic CRS.*\")\n",
        "    chunk[\"geometry\"] = chunk.geometry.buffer(buffer_deg)\n",
        "\n",
        "    # Drop bad geometries\n",
        "    chunk = chunk[chunk[\"geometry\"].notnull()]\n",
        "    chunk = chunk[chunk.is_valid]\n",
        "\n",
        "    if chunk.empty:\n",
        "        print(f\"‚ö†Ô∏è Skipping empty/invalid chunk {start+1} to {end}\")\n",
        "        continue\n",
        "\n",
        "    # Compute zonal stats with overflow suppression\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "        try:\n",
        "            stats = zonal_stats(\n",
        "                chunk,\n",
        "                population_raster,\n",
        "                stats=[\"sum\"],\n",
        "                geojson_out=False,\n",
        "                nodata=-9999  # Or None if unknown\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in chunk {start+1} to {end}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not stats or all(row.get(\"sum\") is None for row in stats):\n",
        "        print(f\"‚ö†Ô∏è No stats found for chunk {start+1} to {end}\")\n",
        "        chunk[\"pop\"] = 0\n",
        "    else:\n",
        "        chunk[\"pop\"] = [float(row.get(\"sum\", 0) or 0) for row in stats]\n",
        "\n",
        "    # Flag suspicious high-pop zones\n",
        "    chunk[\"pop_flagged\"] = chunk[\"pop\"].apply(lambda x: \"‚ö†Ô∏è check\" if x > 20_000_000 else \"\")\n",
        "    max_pop = chunk[\"pop\"].max()\n",
        "    if max_pop > 20_000_000:\n",
        "        print(f\"‚ö†Ô∏è High population in chunk {start+1}-{end}: {max_pop:,.0f}\")\n",
        "        suspicious_chunks.append(chunk[chunk[\"pop\"] > 20_000_000].copy())\n",
        "\n",
        "    # Save chunk to CSV immediately\n",
        "    chunk_out_path = os.path.join(output_dir, f\"chunk_{start+1}_{end}.csv\")\n",
        "    chunk.drop(columns=\"geometry\").to_csv(chunk_out_path, index=False)\n",
        "    print(f\"üìÅ Saved chunk to: {chunk_out_path}\")\n",
        "\n",
        "    print(f\"‚úÖ Chunk {start+1} to {end} processed.\")\n",
        "    # Append to final result in memory\n",
        "    final_chunks.append(chunk.copy())  # keep geometry for now\n",
        "\n",
        "    # Save chunk to disk (crash recovery)\n",
        "    chunk_out_path = os.path.join(output_dir, f\"chunk_{start+1}_{end}.csv\")\n",
        "    chunk.drop(columns=\"geometry\").to_csv(chunk_out_path, index=False)\n",
        "\n",
        "    # Memory cleanup\n",
        "    del chunk, stats\n",
        "    gc.collect()\n",
        "\n",
        "# Save suspicious rows summary\n",
        "if suspicious_chunks:\n",
        "    suspicious_df = pd.concat(suspicious_chunks, ignore_index=True)\n",
        "    suspicious_df.to_csv(os.path.join(output_dir, \"suspicious_population_zones.csv\"), index=False)\n",
        "    print(f\"üü° Saved {len(suspicious_df)} suspicious rows for manual review.\")\n",
        "else:\n",
        "    print(\"‚úÖ No suspicious population zones found.\")\n",
        "\n",
        "print(\"üéâ All done.\")\n",
        "\n",
        "\n",
        "# Combine all chunks in memory\n",
        "final_gdf = pd.concat(final_chunks, ignore_index=True)\n",
        "print(\"‚úÖ Combined all chunks into final_gdf\")\n",
        "\n",
        "final_gdf.to_csv(r\"D:\\NDIS_Database\\ghz_pop.csv\", index=False)\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aLrCIoQWg4W"
      },
      "outputs": [],
      "source": [
        "suspicious_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzsjV_qMWg4X"
      },
      "outputs": [],
      "source": [
        "# Step 1: Reload or reuse original input GeoDataFrame\n",
        "chunk = input_gdf.iloc[1930000:1938350].copy()  # Python is 0-based\n",
        "\n",
        "# Step 2: Buffer and clean geometry\n",
        "warnings.filterwarnings(\"ignore\", message=\"Geometry is in a geographic CRS.*\")\n",
        "chunk[\"geometry\"] = chunk.geometry.buffer(buffer_deg)\n",
        "chunk = chunk[chunk[\"geometry\"].notnull()]\n",
        "chunk = chunk[~chunk[\"geometry\"].is_empty]\n",
        "chunk = chunk[chunk.is_valid]\n",
        "\n",
        "# Step 3: Rerun zonal stats safely\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "    stats = zonal_stats(\n",
        "        chunk,\n",
        "        population_raster,\n",
        "        stats=[\"sum\"],\n",
        "        geojson_out=False,\n",
        "        nodata=-9999\n",
        "    )\n",
        "\n",
        "# Step 4: Fill population values\n",
        "chunk[\"pop\"] = [float(row.get(\"sum\", 0) or 0) for row in stats]\n",
        "chunk[\"pop_flagged\"] = chunk[\"pop\"].apply(lambda x: \"‚ö†Ô∏è check\" if x > 20_000_000 else \"\")\n",
        "\n",
        "# Step 5: Save the corrected chunk\n",
        "chunk_out_path = os.path.join(output_dir, \"chunk_1930001_1938350.csv\")\n",
        "chunk.drop(columns=\"geometry\").to_csv(chunk_out_path, index=False)\n",
        "print(f\"‚úÖ Patched chunk saved to {chunk_out_path}\")\n",
        "\n",
        "# Optional: append to final in-memory result\n",
        "#final_chunks.append(chunk.copy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uX448lxWg4Y"
      },
      "outputs": [],
      "source": [
        "# ArcGIS ObjectID starts at 1, but pandas index starts at 0\n",
        "# So subtract 1\n",
        "pandas_row_ids = [i - 1 for i in null_geom_ids]\n",
        "\n",
        "# Isolate the rows from final_gdf\n",
        "null_rows = final_gdf.iloc[pandas_row_ids].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxMYZjFKWg4Y"
      },
      "outputs": [],
      "source": [
        "print(min(null_rows.longitude))\n",
        "print(max(null_rows.longitude))\n",
        "print(min(null_rows.latitude))\n",
        "print(max(null_rows.latitude))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUgLcfhgWg4Y"
      },
      "outputs": [],
      "source": [
        "# List of ArcGIS-reported IDs\n",
        "arcgis_oids = [\n",
        "    1045, 14943, 14999, 15003, 15037, 15052, 15108, 18189, 64503, 72741,\n",
        "    99537, 102208, 103016, 105219, 114251, 114516, 115189, 116052, 116932,\n",
        "    116991, 117447, 117792, 118519, 121364, 139022, 355328, 359478, 370280,\n",
        "    370413, 371531, 386363, 386367, 386387, 388549, 389557, 390083, 393224,\n",
        "    393645, 394422, 394774, 403304, 404977, 476193, 539023, 867092, 895514,\n",
        "    927888, 987286, 988385, 1110501, 1112965, 1112970, 1163972, 1178798,\n",
        "    1183599, 1289453, 1289586, 1338653, 1340317, 1340620, 1520597, 1527977,\n",
        "    1527982, 1582745, 1585801, 1619277, 1620106, 1621234, 1622439, 1630938,\n",
        "    1633828, 1633854, 1648844, 1821680, 1888743, 1888745, 1888748, 1888750,\n",
        "    1888752, 1888754, 1888926, 1903415, 1903418, 1903419, 1903420\n",
        "]\n",
        "\n",
        "# Extract suspicious rows\n",
        "sus_rows = final_gdf.iloc[arcgis_oids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "u5mDOlevWg4Z"
      },
      "outputs": [],
      "source": [
        "at_origin_coords = final_gdf[\n",
        "    (final_gdf[\"longitude\"] == 0.0) & (final_gdf[\"latitude\"] == 0.0)\n",
        "]\n",
        "at_origin_coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybeb80iTWg4Z"
      },
      "outputs": [],
      "source": [
        "at_origin_coords.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIWOl1OaWg4Z"
      },
      "outputs": [],
      "source": [
        "invalid_coords = final_gdf[\n",
        "    (final_gdf[\"latitude\"] > 90) | (final_gdf[\"latitude\"] < -90) |\n",
        "    (final_gdf[\"longitude\"] > 180) | (final_gdf[\"longitude\"] < -180)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btkZ-WBEWg4a"
      },
      "outputs": [],
      "source": [
        "# Step 1: Get the final chunk (assuming input_gdf is still in memory)\n",
        "final_chunk = input_gdf.iloc[1930000:1938350].copy()\n",
        "\n",
        "# Step 2: Extract rows with inf population from final_gdf\n",
        "inf_rows = final_gdf[np.isinf(final_gdf[\"pop\"])].copy()\n",
        "\n",
        "# Step 3: Drop geometry and rebuild clean GeoDataFrame\n",
        "inf_rows = inf_rows.drop(columns=\"geometry\", errors=\"ignore\")\n",
        "inf_rows[\"geometry\"] = inf_rows.apply(lambda row: Point(row[\"longitude\"], row[\"latitude\"]), axis=1)\n",
        "inf_gdf = gpd.GeoDataFrame(inf_rows, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
        "\n",
        "# Step 4: Combine both sets (resetting index not strictly necessary but keeps things tidy)\n",
        "redo_gdf = pd.concat([final_chunk, inf_gdf], ignore_index=True)\n",
        "\n",
        "print(f\"üîÅ Total to reprocess: {len(redo_gdf)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbdMPIwSWg4b"
      },
      "outputs": [],
      "source": [
        "null_or_empty_geom_df = ghz_pap[ghz_pap[\"latitude\"].isna() | ghz_pap[\"longitude\"].isna()]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82Lys8VDWg4b"
      },
      "source": [
        "# Tsunami Reprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVWbIKE2Wg4c"
      },
      "outputs": [],
      "source": [
        "# Select only the desired columns from merged_df_final\n",
        "tsun_2concat = tsunami[[\n",
        "    \"UNIQUE_ID\",\n",
        "    \"LATITUDE\",\n",
        "    \"LONGITUDE\",\n",
        "    \"intensity\",\n",
        "    \"economic_loss_million\",\n",
        "    \"duration_minutes\"\n",
        "]].copy()\n",
        "tsun_2concat.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6afHsgpWg4c"
      },
      "outputs": [],
      "source": [
        "# Rename fields to match geohazard schema\n",
        "tsun_2concat = tsunami[[\n",
        "    \"UNIQUE_ID\",\n",
        "    \"LATITUDE\",\n",
        "    \"LONGITUDE\",\n",
        "    \"intensity\",\n",
        "    \"economic_loss_million\",\n",
        "    \"duration_minutes\"\n",
        "]].copy()\n",
        "\n",
        "# Rename columns\n",
        "tsun_2concat.rename(columns={\n",
        "    \"UNIQUE_ID\": \"HazardID\",\n",
        "    \"LATITUDE\": \"latitude\",\n",
        "    \"LONGITUDE\": \"longitude\"\n",
        "}, inplace=True)\n",
        "tsun_2concat.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFdUQIUNWg4d"
      },
      "outputs": [],
      "source": [
        "# Save to CSV for ArcGIS processing\n",
        "csv_path = \"D:/ArcGISProjects/GeohazardDB/tsunami_standardized.csv\"\n",
        "tsun_2concat.to_csv(csv_path, index=False)\n",
        "\n",
        "tsun_2concat.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPrxykg8Wg4d"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "gdb_path = r\"D:\\ArcGISProjects\\GeohazardDB\\GeohazardDB.gdb\"\n",
        "country_layer = os.path.join(gdb_path, \"eez_country\")\n",
        "road_layer = os.path.join(gdb_path, \"roads\")  # Assuming the full global road dataset is here\n",
        "output_clip = os.path.join(gdb_path, \"road_MNE\")\n",
        "\n",
        "# Get geometry of MNE\n",
        "where_clause = \"ISO_TER1 = 'MNE'\"\n",
        "\n",
        "# Temporary layer\n",
        "arcpy.MakeFeatureLayer_management(country_layer, \"country_lyr\", where_clause)\n",
        "\n",
        "# Clip road layer\n",
        "arcpy.Clip_analysis(\n",
        "    in_features=road_layer,\n",
        "    clip_features=\"country_lyr\",\n",
        "    out_feature_class=output_clip\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Clipped road_MNE saved at: {output_clip}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "cno1dXJFWg4e"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "\n",
        "# Paths\n",
        "project_folder = r\"D:\\ArcGISProjects\\GeohazardDB\"\n",
        "input_csv = os.path.join(project_folder, \"tsunami_standardized.csv\")\n",
        "ndis_gdb = os.path.join(project_folder, \"NDIS.gdb\")\n",
        "road_gdb = os.path.join(project_folder, \"GeohazardDB.gdb\")\n",
        "road_layer_template = os.path.join(road_gdb, \"roads\")\n",
        "country_layer = os.path.join(project_folder, \"GeohazardDB.gdb\", \"eez_country\")\n",
        "\n",
        "# Convert CSV to point layer\n",
        "tsun_layer = os.path.join(ndis_gdb, \"tsun_input\")\n",
        "arcpy.management.XYTableToPoint(\n",
        "    in_table=input_csv,\n",
        "    out_feature_class=tsun_layer,\n",
        "    x_field=\"longitude\",\n",
        "    y_field=\"latitude\",\n",
        "    coordinate_system=arcpy.SpatialReference(4326)\n",
        ")\n",
        "\n",
        "# Near analysis preparation\n",
        "ghz_list = []\n",
        "near_tables = []\n",
        "\n",
        "total_countries = int(arcpy.GetCount_management(country_layer)[0])\n",
        "\n",
        "with arcpy.da.SearchCursor(country_layer, [\"ISO_TER1\", \"SHAPE@\"]) as country_cursor:\n",
        "    for index, row in enumerate(country_cursor, start=1):\n",
        "        iso = row[0]\n",
        "        shape = row[1]\n",
        "        print(f\"‚è≥ Processing {iso} ({index}/{total_countries})...\")\n",
        "\n",
        "        tsun_clip = os.path.join(ndis_gdb, f\"tsun_{iso}\")\n",
        "        road_clip = os.path.join(road_gdb, f\"road_{iso}\")\n",
        "        near_table = os.path.join(ndis_gdb, f\"near_{iso}\")\n",
        "\n",
        "        # Clip tsunami points\n",
        "        if arcpy.Exists(tsun_clip):\n",
        "            arcpy.Delete_management(tsun_clip)\n",
        "        arcpy.analysis.Clip(tsun_layer, shape, tsun_clip)\n",
        "\n",
        "        # Run Near (roads already pre-clipped)\n",
        "        if arcpy.Exists(near_table):\n",
        "            arcpy.Delete_management(near_table)\n",
        "        arcpy.analysis.GenerateNearTable(\n",
        "            in_features=tsun_clip,\n",
        "            near_features=road_clip,\n",
        "            out_table=near_table,\n",
        "            location=\"LOCATION\",\n",
        "            angle=\"ANGLE\",\n",
        "            closest=\"CLOSEST\",\n",
        "            method=\"GEODESIC\"\n",
        "        )\n",
        "        print(f\"  ‚úÖ Near table created: {near_table}\")\n",
        "        near_tables.append(near_table)\n",
        "\n",
        "        # Add 'distance' to tsun_clip\n",
        "        if \"distance\" not in [f.name for f in arcpy.ListFields(tsun_clip)]:\n",
        "            arcpy.AddField_management(tsun_clip, \"distance\", \"DOUBLE\")\n",
        "\n",
        "        with arcpy.da.UpdateCursor(tsun_clip, [\"OBJECTID\", \"distance\"]) as up_cursor:\n",
        "            for up_row in up_cursor:\n",
        "                oid = up_row[0]\n",
        "                with arcpy.da.SearchCursor(near_table, [\"IN_FID\", \"NEAR_DIST\"]) as near_cursor:\n",
        "                    for near_row in near_cursor:\n",
        "                        if near_row[0] == oid:\n",
        "                            up_row[1] = near_row[1]\n",
        "                            up_cursor.updateRow(up_row)\n",
        "                            break\n",
        "\n",
        "        # Add 'HazardID' to near table\n",
        "        if \"HazardID\" not in [f.name for f in arcpy.ListFields(near_table)]:\n",
        "            arcpy.AddField_management(near_table, \"HazardID\", \"TEXT\")\n",
        "\n",
        "        with arcpy.da.UpdateCursor(near_table, [\"IN_FID\", \"HazardID\"]) as cursor:\n",
        "            for row in cursor:\n",
        "                with arcpy.da.SearchCursor(tsun_clip, [\"OBJECTID\", \"HazardID\"]) as src:\n",
        "                    for src_row in src:\n",
        "                        if row[0] == src_row[0]:\n",
        "                            row[1] = src_row[1]\n",
        "                            cursor.updateRow(row)\n",
        "                            break\n",
        "\n",
        "        ghz_list.append(tsun_clip)\n",
        "\n",
        "# Merge tsunami point results\n",
        "merged_output = os.path.join(ndis_gdb, \"tsun_dist\")\n",
        "if arcpy.Exists(merged_output):\n",
        "    arcpy.Delete_management(merged_output)\n",
        "arcpy.Merge_management(ghz_list, merged_output)\n",
        "print(f\"‚úÖ All tsunami points merged into {merged_output}\")\n",
        "\n",
        "# Merge near tables\n",
        "compiled_near_table = os.path.join(ndis_gdb, \"compiled_near_table_tsun\")\n",
        "if arcpy.Exists(compiled_near_table):\n",
        "    arcpy.Delete_management(compiled_near_table)\n",
        "\n",
        "arcpy.CreateTable_management(ndis_gdb, \"compiled_near_table_tsun\")\n",
        "for field in [(\"FROM_X\", \"DOUBLE\"), (\"FROM_Y\", \"DOUBLE\"), (\"NEAR_X\", \"DOUBLE\"),\n",
        "              (\"NEAR_Y\", \"DOUBLE\"), (\"NEAR_FID\", \"LONG\"), (\"HazardID\", \"TEXT\")]:\n",
        "    arcpy.AddField_management(compiled_near_table, field[0], field[1])\n",
        "\n",
        "with arcpy.da.InsertCursor(compiled_near_table, [\"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\", \"NEAR_FID\", \"HazardID\"]) as insert_cursor:\n",
        "    for table in near_tables:\n",
        "        with arcpy.da.SearchCursor(table, [\"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\", \"NEAR_FID\", \"HazardID\"]) as cursor:\n",
        "            for row in cursor:\n",
        "                insert_cursor.insertRow(row)\n",
        "\n",
        "# Create line layer\n",
        "line_fc = os.path.join(ndis_gdb, \"compiled_near_lines_tsun\")\n",
        "if arcpy.Exists(line_fc):\n",
        "    arcpy.Delete_management(line_fc)\n",
        "\n",
        "arcpy.XYToLine_management(\n",
        "    compiled_near_table, line_fc,\n",
        "    \"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\"\n",
        ")\n",
        "print(f\"‚úÖ Lines created: {line_fc}\")\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(f\"‚úÖ All tsunami near analysis completed in {elapsed/60:.2f} minutes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rAeg7xvWg4e"
      },
      "outputs": [],
      "source": [
        "tsun_fixed = gpd.read_file(r\"D:\\ArcGISProjects\\GeohazardDB\\NDIS.gdb\", layer=\"tsun_dist\")\n",
        "\n",
        "# Step 1: Drop existing tsunami rows from ghz_pap\n",
        "ghz_no_tsun = ghz_pap[ghz_pap[\"HazardType\"] != \"Tsunami\"].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzahC1HNWg4f"
      },
      "outputs": [],
      "source": [
        "tsun_fixed[\"HazardType\"] = \"Tsunami\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kInZOSCYWg4f"
      },
      "outputs": [],
      "source": [
        "# Step 3: Concatenate both\n",
        "ghz_all = pd.concat([ghz_no_tsun, tsun_fixed], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mgOfanbWg4f"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values in the \"distance\" field\n",
        "nan_rows = ghz_all[ghz_all[\"distance\"].isna()]\n",
        "\n",
        "# Display count by HazardType (if field exists)\n",
        "if \"HazardType\" in nan_rows.columns:\n",
        "    print(nan_rows[\"HazardType\"].value_counts())\n",
        "else:\n",
        "    print(\"HazardType column not found in the data.\")\n",
        "\n",
        "# Optionally: show how many total\n",
        "print(f\"Total rows with NaN in distance: {len(nan_rows)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw6G9yYoWg4g"
      },
      "outputs": [],
      "source": [
        "ghz_all.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SotFshGxWg4g"
      },
      "outputs": [],
      "source": [
        "# Step 4: Clean up temporary columns\n",
        "ghz_all.drop(columns=[\"cpm_total_time\", \"travel_time\"], inplace=True)\n",
        "ghz_all.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl4YMnQuWg4g"
      },
      "outputs": [],
      "source": [
        "ghz_all.to_csv(r\"D:\\NDIS_Database\\ghz_paper2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnrpjvtPWg4h"
      },
      "outputs": [],
      "source": [
        "import arcpy\n",
        "from arcpy.sa import *\n",
        "\n",
        "arcpy.CheckOutExtension(\"Spatial\")\n",
        "arcpy.env.overwriteOutput = True\n",
        "arcpy.env.workspace = r\"D:\\NDIS_Database\\12_Population_synthetic\"\n",
        "\n",
        "# Input raster\n",
        "input_raster = \"nasapopct.tif\"\n",
        "output_raster = \"nasapopct_int.tif\"\n",
        "\n",
        "# Convert float to int (rounded)\n",
        "int_ras = Int(Raster(input_raster) + 0.5)  # round to nearest integer\n",
        "\n",
        "# Save to file\n",
        "int_ras.save(output_raster)\n",
        "\n",
        "print(f\"‚úÖ Saved integer raster to: {output_raster}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF7GmC4jWg4h"
      },
      "outputs": [],
      "source": [
        "# Confirm type\n",
        "r = arcpy.Raster(r\"D:\\NDIS_Database\\12_Population_synthetic\\nasapopct_int.tif\")\n",
        "print(r.pixelType)  # Should now be 'S32' or similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkRw6U8jWg4h"
      },
      "outputs": [],
      "source": [
        "# === SETUP ===\n",
        "arcpy.CheckOutExtension(\"Spatial\")\n",
        "\n",
        "# Paths and constants\n",
        "project_folder = r\"D:\\ArcGISProjects\\GeohazardDB\"\n",
        "gdb_ghz = os.path.join(project_folder, \"GeohazardDB.gdb\")\n",
        "gdb_ndis = os.path.join(project_folder, \"NDIS.gdb\")\n",
        "country_layer = os.path.join(gdb_ghz, \"eez_country\")\n",
        "population_raster = r\"D:\\NDIS_Database\\12_Population_synthetic\\nasapopct_int.tif\"\n",
        "output_field = \"pop_sum\"\n",
        "log_path = os.path.join(project_folder, \"pop_stat_log.txt\")\n",
        "\n",
        "# Load as Raster to ensure integer pixel type is respected\n",
        "value_raster = arcpy.sa.Int(arcpy.Raster(population_raster))\n",
        "print(f\"‚úÖ Forcing Int raster (wrapped): {value_raster.pixelType}\")\n",
        "\n",
        "# Extract ISO codes from country layer\n",
        "iso_codes = set()\n",
        "with arcpy.da.SearchCursor(country_layer, [\"ISO_TER1\"]) as cursor:\n",
        "    for row in cursor:\n",
        "        iso = row[0]\n",
        "        if iso:\n",
        "            iso_codes.add(iso)\n",
        "\n",
        "# Find feature classes matching the ISO suffixes\n",
        "def get_matching_fcs(gdb_path, prefix):\n",
        "    arcpy.env.workspace = gdb_path\n",
        "    return [fc for fc in arcpy.ListFeatureClasses(f\"{prefix}_*\") if fc.split(\"_\")[-1] in iso_codes]\n",
        "\n",
        "ghz_fc = get_matching_fcs(gdb_ghz, \"ghz\")\n",
        "nuc_fc = get_matching_fcs(gdb_ghz, \"nuc\")\n",
        "tsun_fc = get_matching_fcs(gdb_ndis, \"tsun\")\n",
        "all_fc = ghz_fc + nuc_fc + tsun_fc\n",
        "\n",
        "print(f\"Total matching FCs: {len(all_fc)}\")\n",
        "\n",
        "# === ZONAL STATISTICS LOOP ===\n",
        "start_total = time.time()\n",
        "\n",
        "with open(log_path, \"w\", encoding=\"utf-8\") as log:\n",
        "    for idx, fc in enumerate(all_fc, start=1):\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            print(f\"‚è≥ [{idx}/{len(all_fc)}] Processing: {fc}\")\n",
        "            log.write(f\"[START] {fc}\\n\")\n",
        "\n",
        "            # Determine full path\n",
        "            if fc in tsun_fc:\n",
        "                fc_path = os.path.join(gdb_ndis, fc)\n",
        "            else:\n",
        "                fc_path = os.path.join(gdb_ghz, fc)\n",
        "\n",
        "            # Add population field if missing\n",
        "            field_names = [f.name for f in arcpy.ListFields(fc_path)]\n",
        "            if output_field not in field_names:\n",
        "                arcpy.AddField_management(fc_path, output_field, \"DOUBLE\")\n",
        "\n",
        "            # Zonal statistics as table\n",
        "            zone_table = os.path.join(\"in_memory\", f\"{fc}_zonal\")\n",
        "            arcpy.sa.ZonalStatisticsAsTable(\n",
        "                in_zone_data=fc_path,\n",
        "                zone_field=\"OBJECTID\",\n",
        "                in_value_raster=value_raster,\n",
        "                out_table=zone_table,\n",
        "                ignore_nodata=\"DATA\",\n",
        "                statistics_type=\"SUM\"\n",
        "            )\n",
        "\n",
        "            # Join and transfer values\n",
        "            arcpy.JoinField_management(\n",
        "                in_data=fc_path,\n",
        "                in_field=\"OBJECTID\",\n",
        "                join_table=zone_table,\n",
        "                join_field=\"OBJECTID\",\n",
        "                fields=[\"SUM\"]\n",
        "            )\n",
        "\n",
        "            if \"SUM\" in [f.name for f in arcpy.ListFields(fc_path)]:\n",
        "                with arcpy.da.UpdateCursor(fc_path, [\"SUM\", output_field]) as cursor:\n",
        "                    for row in cursor:\n",
        "                        row[1] = row[0]\n",
        "                        cursor.updateRow(row)\n",
        "                arcpy.DeleteField_management(fc_path, [\"SUM\"])\n",
        "\n",
        "            elapsed = round(time.time() - start_time, 2)\n",
        "            log.write(f\"[OK] {fc} completed in {elapsed} sec\\n\")\n",
        "            print(f\"‚úÖ Done in {elapsed} sec\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log.write(f\"[FAIL] {fc} error: {str(e)}\\n\")\n",
        "            print(f\"‚ùå Error in {fc}: {e}\")\n",
        "\n",
        "        # Clear in_memory and force garbage collection\n",
        "        arcpy.Delete_management(\"in_memory\")\n",
        "        gc.collect()\n",
        "\n",
        "    total_elapsed = round(time.time() - start_total, 2)\n",
        "    log.write(f\"\\n‚úÖ All done in {total_elapsed / 60:.2f} min\\n\")\n",
        "    print(f\"\\n‚úÖ All done in {total_elapsed / 60:.2f} min\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnlSuuVvWg4i"
      },
      "source": [
        "----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_wCxFHSWg4i"
      },
      "source": [
        "# Statistics of Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44fQlT9WWg4i"
      },
      "outputs": [],
      "source": [
        "# Set the path to this geodatabase\n",
        "gdb_path = r\"D:\\ArcGISProjects\\GeohazardDB\\GeohazardDB.gdb\"  # This gdb path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpFVKAGOWg4j"
      },
      "outputs": [],
      "source": [
        "# Set the path to this geodatabase\n",
        "gdb_path = r\"D:\\ArcGISProjects\\GeohazardDB\\GeohazardDB.gdb\"  # This gdb path\n",
        "# Specify the feature class name\n",
        "ghz_dist = \"ghz_dist\"  # Geohazard feature class\n",
        "ghz_path = f\"{gdb_path}\\\\{ghz_dist}\"\n",
        "\n",
        "# Use arcpy to create a list of fields\n",
        "ghz_fields = [f.name for f in arcpy.ListFields(f\"{gdb_path}\\\\{ghz_dist}\")]\n",
        "\n",
        "# Use arcpy to create a search cursor and load the data into a list of dictionaries\n",
        "ghz_data = []\n",
        "with arcpy.da.SearchCursor(f\"{gdb_path}\\\\{ghz_dist}\", ghz_fields) as cursor:\n",
        "    for row in cursor:\n",
        "        ghz_data.append(dict(zip(ghz_fields, row)))\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "ghzdf = pd.DataFrame(ghz_data)\n",
        "ghzdf.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzvbqBqzWg4k"
      },
      "outputs": [],
      "source": [
        "nan_in_ghz = ghzdf.isnull().sum().sum()\n",
        "\n",
        "# printing the number of values present in\n",
        "# the whole dataframe\n",
        "print('Number of NaN values present: ' + str(nan_in_ghz))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMYCsK8DWg4l"
      },
      "outputs": [],
      "source": [
        "geohazard_layer = os.path.join(gdb_path, \"ghz_dist\")\n",
        "\n",
        "# Define the output feature class\n",
        "output_fc = os.path.join(gdb_path, \"cleaned_geohazard_data\")\n",
        "\n",
        "# Retrieve data from feature class and store it in DataFrame\n",
        "fields = [\"OBJECTID\", \"Shape@\", \"HazardID\", \"latitude\", \"longitude\", \"HazardType\", \"distance\"]\n",
        "data = []\n",
        "\n",
        "# Use SearchCursor to extract data from feature class\n",
        "with arcpy.da.SearchCursor(geohazard_layer, fields) as cursor:\n",
        "    for row in cursor:\n",
        "        # Separate geometry from the non-geometry fields\n",
        "        geometry = row[1]  # Shape@ is at index 1\n",
        "        data.append(row[:1] + (geometry,) + row[2:])  # Append geometry separately\n",
        "\n",
        "# Create DataFrame without geometry\n",
        "df = pd.DataFrame(data, columns=[\"OBJECTID\", \"Shape@\", \"HazardID\", \"latitude\", \"longitude\", \"HazardType\", \"distance\"])\n",
        "\n",
        "# Convert 'distance' field to numeric, handling errors as NaN\n",
        "df['distance'] = pd.to_numeric(df['distance'], errors='coerce')\n",
        "\n",
        "# Filter out rows with NaN distance values\n",
        "df_no_nulls = df[df['distance'].notna()]\n",
        "\n",
        "# Handle duplicates by HazardID (keep row with smallest distance)\n",
        "df_no_nulls = df_no_nulls.loc[df_no_nulls.groupby('HazardID')['distance'].idxmin()]\n",
        "\n",
        "# Create the output feature class by keeping the geometry\n",
        "arcpy.management.CreateFeatureclass(\n",
        "    out_path=gdb_path,\n",
        "    out_name=\"cleaned_geohazard_data\",\n",
        "    geometry_type=\"POINT\",  # Change to 'POINT' if the geometry is a point\n",
        "    template=geohazard_layer,  # Copy schema from the original feature class\n",
        "    spatial_reference=arcpy.Describe(geohazard_layer).spatialReference\n",
        ")\n",
        "\n",
        "# Insert cleaned data into the new feature class using InsertCursor\n",
        "with arcpy.da.InsertCursor(output_fc, [\"SHAPE@\", \"HazardID\", \"latitude\", \"longitude\", \"HazardType\", \"distance\"]) as cursor:\n",
        "    for idx, row in df_no_nulls.iterrows():\n",
        "        geometry = row['Shape@']  # Retrieve geometry separately\n",
        "        cursor.insertRow([geometry, row['HazardID'], row['latitude'], row['longitude'], row['HazardType'], row['distance']])\n",
        "\n",
        "print(f\"Cleaned data written to: {output_fc}\")\n",
        "# Play sound to notify that it's done\n",
        "# playsound(r\"C:\\Windows\\Media\\Alarm09.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8Vu92hFWg4s"
      },
      "outputs": [],
      "source": [
        "ghz_pop = pd.read_csv(r\"D:\\NDIS_Database\\ghz_pop.csv\")\n",
        "ghz_pop.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEmQPlagWg4t"
      },
      "outputs": [],
      "source": [
        "# Find rows where pop is inf, -inf, or NaN\n",
        "mask = ~np.isfinite(ghz_pop[\"pop\"])\n",
        "problematic_rows = ghz_pop[mask]\n",
        "\n",
        "print(f\"Total problematic rows: {len(problematic_rows)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-3ASw59Wg4t"
      },
      "source": [
        "-----------------------\n",
        "# Population Handle inf\n",
        "--------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJbvSctRWg4u"
      },
      "outputs": [],
      "source": [
        "ghz_pap = pd.read_csv(r\"D:\\NDIS_Database\\ghz_paper2.csv\")\n",
        "ghz_pap.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovTVV5TTWg4u"
      },
      "outputs": [],
      "source": [
        "\n",
        "with rasterio.open(r\"D:/NDIS_Database/12_Population_synthetic/nasapopct.tif\") as src:\n",
        "    print(\"CRS:\", src.crs)\n",
        "    print(\"Bounds:\", src.bounds)\n",
        "    print(\"Res:\", src.res)\n",
        "    print(\"NoData:\", src.nodata)\n",
        "    print(\"Dtype:\", src.dtypes[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMEda_AbWg4u"
      },
      "outputs": [],
      "source": [
        "src_path = r\"D:/NDIS_Database/12_Population_synthetic/nasapopct.tif\"\n",
        "\n",
        "with rasterio.open(src_path) as src:\n",
        "    data = src.read(1)\n",
        "\n",
        "    print(\"CRS:\", src.crs)\n",
        "    print(\"Bounds:\", src.bounds)\n",
        "    print(\"Res:\", src.res)\n",
        "    print(\"NoData:\", src.nodata)\n",
        "    print(\"Dtype:\", src.dtypes[0])\n",
        "\n",
        "    # Check value counts\n",
        "    nodata_val = src.nodata\n",
        "    print(\"\\nValue stats:\")\n",
        "    print(\"  NaNs:\", np.isnan(data).sum())\n",
        "    print(\"  Infs:\", np.isinf(data).sum())\n",
        "    print(\"  NoData:\", np.sum(data == nodata_val))\n",
        "    print(\"  Zeros:\", np.sum(data == 0))\n",
        "    print(\"  Valid (>0):\", np.sum(data > 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdIFOpgLWg4v"
      },
      "outputs": [],
      "source": [
        "dst_path = r\"D:/NDIS_Database/12_Population_synthetic/nasapopct_fixed_int.tif\"\n",
        "\n",
        "with rasterio.open(src_path) as src:\n",
        "    profile = src.profile\n",
        "    data = src.read(1)\n",
        "\n",
        "    # Replace extreme NoData and other invalids\n",
        "    data[data == src.nodata] = 0\n",
        "    data[~np.isfinite(data)] = 0\n",
        "\n",
        "    # Convert to integer\n",
        "    int_data = np.round(data).astype(\"int32\")\n",
        "    profile.update(dtype=\"int32\", nodata=0)\n",
        "\n",
        "    with rasterio.open(dst_path, \"w\", **profile) as dst:\n",
        "        dst.write(int_data, 1)\n",
        "\n",
        "print(\"‚úÖ Saved cleaned raster to:\", dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZTZ1kytWg4v"
      },
      "outputs": [],
      "source": [
        "src_path = r\"D:/NDIS_Database/12_Population_synthetic/nasapopct.tif\"\n",
        "dst_path = r\"D:/NDIS_Database/12_Population_synthetic/nasapopct_int32.tif\"\n",
        "\n",
        "with rasterio.open(src_path) as src:\n",
        "    profile = src.profile\n",
        "    data = src.read(1)\n",
        "\n",
        "    # Clean invalids\n",
        "    data[~np.isfinite(data)] = 0\n",
        "    data[data == src.nodata] = 0\n",
        "\n",
        "    # to integer\n",
        "    int_data = np.round(data).astype(\"int32\")\n",
        "    profile.update(dtype='int32', nodata=0)\n",
        "\n",
        "    with rasterio.open(dst_path, 'w', **profile) as dst:\n",
        "        dst.write(int_data, 1)\n",
        "\n",
        "print(\"‚úÖ Saved raster with int32 precision:\", dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Flo2N7wWg4v"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "\n",
        "# === PARAMETERS ===\n",
        "population_raster = r\"D:\\NDIS_Database\\12_Population_synthetic\\nasapopct_int32.tif\"\n",
        "buffer_dist = 30000  # 30 km\n",
        "chunk_size = 10000\n",
        "output_dir = \"zonal_chunks\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === LOAD AND PREP ===\n",
        "ghz_pap[\"geometry\"] = ghz_pap.apply(\n",
        "    lambda row: Point(row[\"longitude\"], row[\"latitude\"]), axis=1\n",
        ")\n",
        "input_gdf = gpd.GeoDataFrame(ghz_pap, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
        "\n",
        "buffer_deg = buffer_dist / 111320.0\n",
        "total = len(input_gdf)\n",
        "print(f\"üîπ Total features: {total}\")\n",
        "\n",
        "suspicious_chunks = []\n",
        "nan_rows_all = []\n",
        "\n",
        "# === PROCESS IN CHUNKS ===\n",
        "for start in range(0, total, chunk_size):\n",
        "    end = min(start + chunk_size, total)\n",
        "    print(f\"‚è≥ Processing chunk {start+1} to {end}\")\n",
        "\n",
        "    chunk = input_gdf.iloc[start:end].copy()\n",
        "\n",
        "    # Suppress geographic CRS buffer warning\n",
        "    warnings.filterwarnings(\"ignore\", message=\"Geometry is in a geographic CRS.*\")\n",
        "    chunk[\"geometry\"] = chunk.geometry.buffer(buffer_deg)\n",
        "\n",
        "    # Drop bad geometries\n",
        "    chunk = chunk[chunk[\"geometry\"].notnull()]\n",
        "    chunk = chunk[chunk.is_valid]\n",
        "\n",
        "    if chunk.empty:\n",
        "        print(f\"‚ö†Ô∏è Skipping empty/invalid chunk {start+1} to {end}\")\n",
        "        continue\n",
        "\n",
        "    # Zonal stats with correct nodata and pixel inclusion\n",
        "    try:\n",
        "        stats = zonal_stats(\n",
        "            chunk,\n",
        "            population_raster,\n",
        "            stats=[\"sum\"],\n",
        "            geojson_out=False,\n",
        "            nodata=-3.4028235e+38,\n",
        "            all_touched=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in chunk {start+1} to {end}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Clean and assign population values\n",
        "    pop_vals = []\n",
        "    nan_rows = []\n",
        "    for i, row in enumerate(stats):\n",
        "        val = row.get(\"sum\", 0)\n",
        "        if val is None or not np.isfinite(val):\n",
        "            pop_vals.append(np.nan)\n",
        "            nan_rows.append(i)\n",
        "        else:\n",
        "            pop_vals.append(val)\n",
        "\n",
        "    chunk[\"pop\"] = pop_vals\n",
        "\n",
        "    # Flag very high values\n",
        "    chunk[\"pop_flagged\"] = chunk[\"pop\"].apply(\n",
        "        lambda x: \"‚ö†Ô∏è check\" if pd.notna(x) and x > 20_000_000 else \"\"\n",
        "    )\n",
        "\n",
        "    max_pop = chunk[\"pop\"].max()\n",
        "    if pd.notna(max_pop) and max_pop > 20_000_000:\n",
        "        print(f\"‚ö†Ô∏è High population in chunk {start+1}-{end}: {max_pop:,.0f}\")\n",
        "        suspicious_chunks.append(chunk[chunk[\"pop\"] > 20_000_000].copy())\n",
        "\n",
        "    # Log NaN results for QA\n",
        "    if nan_rows:\n",
        "        nan_rows_all.append(chunk.iloc[nan_rows][[\"HazardID\", \"latitude\", \"longitude\", \"pop\"]].copy())\n",
        "\n",
        "    # Save each chunk\n",
        "    chunk_out_path = os.path.join(output_dir, f\"chunk_{start+1}_{end}.csv\")\n",
        "    chunk.drop(columns=\"geometry\").to_csv(chunk_out_path, index=False)\n",
        "    print(f\"üìÅ Saved chunk to: {chunk_out_path}\")\n",
        "\n",
        "    # Clean up memory\n",
        "    del chunk, stats\n",
        "    gc.collect()\n",
        "    print(f\"‚úÖ Chunk {start+1} to {end} processed.\")\n",
        "\n",
        "# === FINAL OUTPUT ===\n",
        "\n",
        "# Save suspicious rows with very high population\n",
        "if suspicious_chunks:\n",
        "    suspicious_df = pd.concat(suspicious_chunks, ignore_index=True)\n",
        "    suspicious_df.to_csv(os.path.join(output_dir, \"suspicious_population_zones.csv\"), index=False)\n",
        "    print(f\"üü° Saved {len(suspicious_df)} suspicious rows for manual review.\")\n",
        "else:\n",
        "    print(\"‚úÖ No suspicious population zones found.\")\n",
        "\n",
        "# Save rows with NaN population values\n",
        "if nan_rows_all:\n",
        "    nan_df = pd.concat(nan_rows_all, ignore_index=True)\n",
        "    nan_df.to_csv(os.path.join(output_dir, \"nan_population_zones.csv\"), index=False)\n",
        "    print(f\"üü† Saved {len(nan_df)} NaN population rows for inspection.\")\n",
        "else:\n",
        "    print(\"‚úÖ No NaN rows in final result.\")\n",
        "\n",
        "print(\"üéâ All done.\")\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(f\"‚úÖ All tsunami near analysis completed in {elapsed/60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_orvNkeWg4w"
      },
      "outputs": [],
      "source": [
        "# Combine all chunk CSVs into a single DataFrame\n",
        "chunk_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.startswith(\"chunk_\") and f.endswith(\".csv\")]\n",
        "final_df = pd.concat([pd.read_csv(f) for f in chunk_files], ignore_index=True)\n",
        "print(f\"üß© Combined all {len(chunk_files)} chunk CSVs into final_df with {len(final_df):,} rows.\")\n",
        "final_df.to_csv(os.path.join(output_dir, \"all_chunks_combined.csv\"), index=False)\n",
        "print(\"üíæ Saved final combined CSV.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJdQDT7-Wg4w"
      },
      "outputs": [],
      "source": [
        "final_df.to_csv(r\"D:\\NDIS_Database\\zonal_chunks\\all_chunks_combined.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVNctQh1Wg4x"
      },
      "outputs": [],
      "source": [
        "final_df.HazardType.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6KarPQ6Wg4x"
      },
      "outputs": [],
      "source": [
        "suspicious_df.to_csv(r\"D:\\NDIS_Database\\zonal_chunks\\suspicious_population.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDXUOLutWg4x"
      },
      "outputs": [],
      "source": [
        "final_df = pd.read_csv(r\"D:\\NDIS_Database\\zonal_chunks\\all_chunks_combined.csv\")\n",
        "final_df.info()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ArcGISPro",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}