{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw6739v6yor5"
      },
      "source": [
        "# NDIS v2.0 - Using Critical Path Method (CPM) and Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIEJR09Xyor8"
      },
      "outputs": [],
      "source": [
        "from arcgis.gis import GIS\n",
        "gis = GIS(\"home\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC3tXx5Ryor-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "#ArcGIS packages\n",
        "import arcpy\n",
        "#from arcgis.mapping import WebScene\n",
        "from arcgis.gis import GIS\n",
        "from arcgis.features import FeatureLayer\n",
        "from IPython.display import display\n",
        "from arcgis.features import GeoAccessor\n",
        "from arcgis import *\n",
        "from arcpy.sa import Raster, Int  # Raster float to integer\n",
        "# Raster processing for dataframe\n",
        "from rasterstats import zonal_stats\n",
        "\n",
        "# basic packages\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import timeit\n",
        "import random\n",
        "import string\n",
        "from playsound import playsound\n",
        "\n",
        "# Data management\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point  # to get points from long lat\n",
        "\n",
        "# Request service\n",
        "#from requests import Request\n",
        "import json\n",
        "import re\n",
        "from functools import reduce\n",
        "#from owslib.wfs import WebFeatureService\n",
        "\n",
        "# Plotting packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning packages\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.tree import export_text\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "import multiprocessing\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "# To save the model and everything else to be API ready\n",
        "import joblib\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gisThm5yor_"
      },
      "outputs": [],
      "source": [
        "# Set the path to this geodatabase\n",
        "gdb_path = r\"D:\\ArcGISProjects\\GeohazardDB\\GeohazardDB.gdb\"  # This gdb path\n",
        "arcpy.env.overwriteOutput = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2LGXh-YyosA"
      },
      "outputs": [],
      "source": [
        "# Import Database/Dataset\n",
        "# Specify the feature class name\n",
        "ghz_clean = \"cleaned_geohazard_data\"  # Geohazard feature class\n",
        "ghz_clean_path = f\"{gdb_path}\\\\{ghz_clean}\"\n",
        "\n",
        "# Use arcpy to create a list of fields\n",
        "ghz_clean_fields = [f.name for f in arcpy.ListFields(f\"{gdb_path}\\\\{ghz_clean}\")]\n",
        "\n",
        "# Use arcpy to create a search cursor and load the data into a list of dictionaries\n",
        "ghz_clean_data = []\n",
        "with arcpy.da.SearchCursor(f\"{gdb_path}\\\\{ghz_clean}\", ghz_clean_fields) as cursor:\n",
        "    for row in cursor:\n",
        "        ghz_clean_data.append(dict(zip(ghz_clean_fields, row)))\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "ghz_celan_df = pd.DataFrame(ghz_clean_data)\n",
        "ghz_celan_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SGUu1RhyosC"
      },
      "source": [
        "### Test Zonal Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqgRyQqlyosD"
      },
      "outputs": [],
      "source": [
        "# Load feature class from the geodatabase using geopandas\n",
        "layer_name = \"buffer_30km\"\n",
        "\n",
        "# This loads the feature class as a GeoDataFrame\n",
        "buffer_gdf = gpd.read_file(gdb_path, layer=layer_name)\n",
        "\n",
        "# Raster path\n",
        "population_raster = r\"D:\\NDIS_Database\\12_Population_synthetic\\nasapop20.tif\"\n",
        "\n",
        "# Run zonal stats (e.g., sum population within each buffer)\n",
        "stats = zonal_stats(buffer_gdf, population_raster, stats=[\"sum\"], geojson_out=True)\n",
        "\n",
        "# Convert the result back to GeoDataFrame\n",
        "result_gdf = gpd.GeoDataFrame.from_features(stats)\n",
        "\n",
        "# Join result to original buffer if needed:\n",
        "buffer_gdf[\"pop_sum\"] = result_gdf[\"sum\"]\n",
        "\n",
        "#buffer_gdf.drop(columns=\"geometry\").to_csv(\"buffer_pop.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waVDjaOUyosE"
      },
      "source": [
        "### Buffering, Zonal Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stF9m-G8yosE"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "# Name of feature class inside the gdb\n",
        "layer_name = \"cleaned_geohazard_data\"\n",
        "# Use geopandas to read it\n",
        "input_gdf = gpd.read_file(gdb_path, layer=layer_name)\n",
        "population_raster = r\"D:\\NDIS_Database\\12_Population_synthetic\\nasapopct.tif\"\n",
        "buffer_dist = 30000  # 30km\n",
        "chunk_size = 10000\n",
        "std_threshold = 10\n",
        "\n",
        "# Load input as GeoDataFrame\n",
        "#gdf = gpd.read_file(input_fc)\n",
        "\n",
        "# Reproject to WGS84 (EPSG:4326) if needed\n",
        "if input_gdf.crs.to_epsg() != 4326:\n",
        "    input_gdf = input_gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Calculate approximate buffer distance in degrees\n",
        "buffer_deg = buffer_dist / 111320.0  # 1 degree â‰ˆ 111.32 km at equator\n",
        "\n",
        "# Final results\n",
        "final_chunks = []\n",
        "\n",
        "total = len(input_gdf)\n",
        "print(f\"ðŸ”¹ Total features: {total}\")\n",
        "\n",
        "for start in range(0, total, chunk_size):\n",
        "    print(f\"ðŸ”¹ Processing chunk {start+1} to {min(start + chunk_size, total)}\")\n",
        "\n",
        "    chunk = input_gdf.iloc[start:start + chunk_size].copy()\n",
        "\n",
        "    # Suppress geographic CRS buffer warning\n",
        "    warnings.filterwarnings(\"ignore\", message=\"Geometry is in a geographic CRS.*\")\n",
        "    chunk[\"geometry\"] = chunk.geometry.buffer(buffer_deg)\n",
        "\n",
        "    # Zonal statistics\n",
        "    stats = zonal_stats(chunk, population_raster,\n",
        "                        stats=[\"mean\", \"majority\", \"count\", \"std\"],\n",
        "                        geojson_out=False)\n",
        "\n",
        "    # Smart pop estimation\n",
        "    smart_pops = []\n",
        "    for row in stats:\n",
        "        count = row.get(\"count\", 0) or 0\n",
        "        mean = row.get(\"mean\", 0) or 0\n",
        "        majority = row.get(\"majority\", 0) or 0\n",
        "        std = row.get(\"std\", 0) or 0\n",
        "\n",
        "        if count == 0:\n",
        "            pop = 0\n",
        "        elif std < std_threshold:\n",
        "            pop = majority * count\n",
        "        else:\n",
        "            pop = mean * count\n",
        "\n",
        "        smart_pops.append(pop)\n",
        "\n",
        "    chunk[\"pop\"] = smart_pops\n",
        "    final_chunks.append(chunk)\n",
        "\n",
        "    print(f\"âœ… Chunk {start+1} processed.\")\n",
        "\n",
        "# Combine all results\n",
        "final_gdf = pd.concat(final_chunks, ignore_index=True)\n",
        "\n",
        "# Optional: export to CSV\n",
        "# final_gdf.drop(columns=\"geometry\").to_csv(\"smart_population_estimate.csv\", index=False)\n",
        "\n",
        "print(\"ðŸŽ‰ All done. Result in dataframe: final_gdf.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIoESmQ4yosG"
      },
      "outputs": [],
      "source": [
        "counts = final_gdf.groupby('HazardType').size()\n",
        "count_volc = counts.get(1, 0)\n",
        "print(\"Occurrences of 'volc':\", count_volc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPG4mSZYyosH"
      },
      "source": [
        "## Preprocess Volcano"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3WAUJqcyosI"
      },
      "outputs": [],
      "source": [
        "# Load\n",
        "# Use known item ID\n",
        "item_id1 = \"0db01e3f54934c0c8060f5acdf326d01\"\n",
        "volc1 = gis.content.get(item_id1)\n",
        "\n",
        "# Inspect the item\n",
        "print(f\"Title: {volc1.title}\")\n",
        "print(f\"Type: {volc1.type}\")\n",
        "\n",
        "# Access the first layer in the feature service\n",
        "v1_layer = volc1.layers[0]\n",
        "\n",
        "# Load it into a Spatially Enabled DataFrame (SeDF)\n",
        "volc1_sdf = v1_layer.query(where=\"1=1\").sdf\n",
        "volc1_sdf.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS9KL0iEyosI"
      },
      "outputs": [],
      "source": [
        "# Load\n",
        "# Use known item ID\n",
        "item_id2 = \"e991eccebfa5425e9180d884af76f16a\"  # Item ID\n",
        "volc2 = gis.content.get(item_id2)\n",
        "\n",
        "# Inspect the item\n",
        "print(f\"Title: {volc2.title}\")\n",
        "print(f\"Type: {volc2.type}\")\n",
        "\n",
        "# Access the first layer in the feature service\n",
        "v2_layer = volc2.layers[0]\n",
        "\n",
        "# Load it into a Spatially Enabled DataFrame (SeDF)\n",
        "volc2_sdf = v2_layer.query(where=\"1=1\").sdf\n",
        "volc2_sdf.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFOM_UMcyosI"
      },
      "outputs": [],
      "source": [
        "volc2_sdf.DAMAGE_TOTAL_DESCRIPTION.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMIik943yosJ"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "main_db = final_gdf      # main geohazard database\n",
        "data1 = volc1_sdf          # External source 1\n",
        "data2 = volc2_sdf          # External source 2\n",
        "\n",
        "# Filter volcanoes\n",
        "volcano_db = main_db[main_db[\"HazardType\"] == 1].copy()\n",
        "\n",
        "# Clean data\n",
        "volcano_db[\"HazardID\"] = volcano_db[\"HazardID\"].astype(\"Int64\")\n",
        "data2[\"ID\"] = data2[\"ID\"].astype(\"Int64\")\n",
        "data1[\"Volcano_Nu\"] = data1[\"Volcano_Nu\"].astype(\"Int64\")\n",
        "# Clean VEI in both datasets to be proper floats\n",
        "data1[\"VEI\"] = pd.to_numeric(data1[\"VEI\"], errors=\"coerce\")\n",
        "data2[\"VEI\"] = pd.to_numeric(data2[\"VEI\"], errors=\"coerce\")\n",
        "data2[\"DEATHS_TOTAL\"] = pd.to_numeric(data2[\"DEATHS_TOTAL\"], errors=\"coerce\")\n",
        "\n",
        "\n",
        "# Match by ID: main_db.HazardID â†” data2.ID\n",
        "merge_d2 = pd.merge(volcano_db, data2[[\"ID\", \"VEI\", \"DEATHS_TOTAL\"]],\n",
        "                    left_on=\"HazardID\", right_on=\"ID\", how=\"left\")\n",
        "\n",
        "# Match by ID: main_db.HazardID â†” data1.Volcano_Nu\n",
        "merge_d1 = pd.merge(volcano_db, data1[[\"Volcano_Nu\", \"VEI\"]],\n",
        "                    left_on=\"HazardID\", right_on=\"Volcano_Nu\", how=\"left\", suffixes=('', '_d1'))\n",
        "\n",
        "# Combine VEIs: prefer data2 over data1\n",
        "vei_combined = merge_d2.copy()\n",
        "vei_combined[\"VEI_d1\"] = merge_d1[\"VEI\"]\n",
        "vei_combined[\"VEI\"] = vei_combined[\"VEI\"].fillna(vei_combined[\"VEI_d1\"])\n",
        "\n",
        "# Estimate VEI from deaths\n",
        "def estimate_vei_from_deaths(deaths):\n",
        "    if pd.isna(deaths):\n",
        "        return np.nan\n",
        "    return min(8, int(deaths / 1000) + 2)\n",
        "\n",
        "missing_vei = vei_combined[\"VEI\"].isna()\n",
        "vei_combined.loc[missing_vei, \"VEI\"] = vei_combined.loc[missing_vei, \"DEATHS_TOTAL\"].apply(estimate_vei_from_deaths)\n",
        "\n",
        "# STEP 3: Weighted random VEI (realistic distribution)\n",
        "missing_vei = vei_combined[\"VEI\"].isna()\n",
        "vei_probs = [0.05, 0.1, 0.2, 0.3, 0.2, 0.1, 0.03, 0.015, 0.005]  # VEI 0â€“8\n",
        "vei_values = list(range(9))\n",
        "vei_combined.loc[missing_vei, \"VEI\"] = np.random.choice(vei_values, size=missing_vei.sum(), p=vei_probs)\n",
        "\n",
        "# Option 2: completely random VEI:\n",
        "# Assign random VEI for any remaining\n",
        "#missing_vei = vei_combined[\"VEI\"].isna()\n",
        "#vei_combined.loc[missing_vei, \"VEI\"] = [random.randint(1, 5) for _ in range(missing_vei.sum())]\n",
        "\n",
        "# Final cleanup\n",
        "vei_combined = vei_combined.rename(columns={\"VEI\": \"vei\"})\n",
        "volcano_df = vei_combined[[\"HazardID\", \"latitude\", \"longitude\", \"HazardType\", \"distance\", \"pop\",\"vei\"]].copy()\n",
        "volcano_df[\"vei\"] = pd.to_numeric(volcano_df[\"vei\"], errors=\"coerce\")\n",
        "volcano_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G0JjyecyosK"
      },
      "source": [
        "### Convert VEI to intensity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9uaCuI7yosL"
      },
      "outputs": [],
      "source": [
        "volcano_sc[\"vei\"] = volcano_sc[\"vei\"].astype(\"int64\")\n",
        "vei = volcano_sc[\"vei\"].values  # pull as NumPy array\n",
        "\n",
        "conditions = [\n",
        "    vei <= 1,\n",
        "    vei == 2,\n",
        "    np.isin(vei, [3, 4]),\n",
        "    np.isin(vei, [5, 6]),\n",
        "    vei >= 7\n",
        "]\n",
        "\n",
        "choices = [1, 2, 3, 4, 5]\n",
        "\n",
        "\n",
        "volcano_sc[\"intensity\"] = np.select(conditions, choices, default=3)  # Default to 3 if VEI is missing\n",
        "volcano_sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdCAQ537yosM"
      },
      "source": [
        "### Get Duration for CPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKpufTuOyosM"
      },
      "outputs": [],
      "source": [
        "# Step 1: Match `data1` rows to volcano_df based on HazardID\n",
        "volc_dur = pd.merge(\n",
        "    volcano_sc[[\"HazardID\", \"vei\"]],\n",
        "    data1,\n",
        "    left_on=\"HazardID\", right_on=\"Volcano_Nu\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Step 2: Parse duration from Start/End fields\n",
        "\n",
        "def safe_date(year, month, day):\n",
        "    try:\n",
        "        if pd.isna(year) or year == \"\":\n",
        "            return None\n",
        "        year = int(float(year))\n",
        "        month = int(float(month)) if month else 1\n",
        "        day = int(float(day)) if day else 1\n",
        "        return datetime(year, month, day)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def calculate_duration_minutes(row):\n",
        "    start = safe_date(row[\"Start_Year\"], row[\"Start_Mont\"], row[\"Start_Day\"])\n",
        "    end = safe_date(row[\"End_Year\"], row[\"End_Month\"], row[\"End_Day\"])\n",
        "    if start and end and end >= start:\n",
        "        return round((end - start).total_seconds() / 60)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "volc_dur[\"duration_minutes\"] = volc_dur.apply(calculate_duration_minutes, axis=1)\n",
        "\n",
        "# Step 3: Generate random duration based on vei_assigned\n",
        "def generate_duration_from_vei(vei):\n",
        "    if pd.isna(vei):\n",
        "        return None\n",
        "    vei = int(vei)\n",
        "    duration_ranges = {\n",
        "        0: (10, 60),\n",
        "        1: (60, 360),\n",
        "        2: (1440, 2880),\n",
        "        3: (2880, 10080),\n",
        "        4: (7200, 43200),\n",
        "        5: (43200, 129600),\n",
        "        6: (129600, 525600),\n",
        "        7: (525600, 2628000),\n",
        "        8: (525600, 2628000),\n",
        "    }\n",
        "    if vei in duration_ranges:\n",
        "        return random.randint(*duration_ranges[vei])\n",
        "    else:\n",
        "        return random.randint(60, 10000)\n",
        "\n",
        "# Fill missing durations\n",
        "volc_dur[\"duration_minutes\"] = volc_dur[\"duration_minutes\"].fillna(\n",
        "    volc_dur[\"vei\"].apply(generate_duration_from_vei)\n",
        ")\n",
        "\n",
        "dur_stats = volc_dur.groupby(\"HazardID\")[\"duration_minutes\"].agg([\"min\", \"max\", \"mean\", \"count\"]).reset_index()\n",
        "dur_stats.columns = [\"HazardID\", \"min_duration\", \"max_duration\", \"avg_duration\", \"eruption_count\"]\n",
        "\n",
        "# Step 4: Merge back into volcano_df\n",
        "volcano_sc = volcano_sc.merge(dur_stats, on=\"HazardID\", how=\"left\")\n",
        "\n",
        "volcano_sc.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jldk9-wXyosN"
      },
      "source": [
        "### Economic loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fiTXfYfyosN"
      },
      "outputs": [],
      "source": [
        "# Map the string info\n",
        "desc_map = {\n",
        "    \"Limited (<$1 million)\": 0.5,\n",
        "    \"Moderate (~$1 to $5 million)\": 3,\n",
        "    \"Severe (~>$5 to $24 million)\": 15,\n",
        "    \"Extreme (~$25 million or more)\": 50\n",
        "}\n",
        "\n",
        "data2[\"damage_desc_est\"] = data2[\"DAMAGE_TOTAL_DESCRIPTION\"].map(desc_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-zj1FjUyosO"
      },
      "outputs": [],
      "source": [
        "# Start with actual values\n",
        "data2[\"econ_loss_million\"] = data2[\"DAMAGE_MILLIONS_DOLLARS_TOTAL\"]\n",
        "\n",
        "# Fill from description-based estimate\n",
        "data2[\"econ_loss_million\"] = data2[\"econ_loss_million\"].fillna(data2[\"damage_desc_est\"])\n",
        "\n",
        "# Fill remaining missing with random or intensity-based simulation\n",
        "def simulate_econ_loss(intensity):\n",
        "    base = {\n",
        "        1: (0.1, 1),\n",
        "        2: (1, 5),\n",
        "        3: (5, 20),\n",
        "        4: (20, 100),\n",
        "        5: (100, 500)\n",
        "    }\n",
        "    return round(random.uniform(*base.get(int(intensity), (1, 10))), 2)\n",
        "\n",
        "# Merge with volcano_df\n",
        "volcano_sc = volcano_sc.merge(\n",
        "    data2[[\"ID\", \"DAMAGE_MILLIONS_DOLLARS_TOTAL\", \"DAMAGE_TOTAL_DESCRIPTION\", \"econ_loss_million\"]],\n",
        "    left_on=\"HazardID\", right_on=\"ID\", how=\"left\"\n",
        ")\n",
        "\n",
        "# Fill any remaining NA values\n",
        "volcano_sc[\"econ_loss_million\"] = volcano_sc.apply(\n",
        "    lambda r: simulate_econ_loss(r[\"intensity\"]) if pd.isna(r[\"econ_loss_million\"]) else r[\"econ_loss_million\"],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO_vn_tDyosO"
      },
      "outputs": [],
      "source": [
        "volcano_sc.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6hXZZdQyosO"
      },
      "outputs": [],
      "source": [
        "# Drop columns 12-14\n",
        "volcano_sc = volcano_sc.drop(['ID', 'DAMAGE_MILLIONS_DOLLARS_TOTAL', 'DAMAGE_TOTAL_DESCRIPTION'], axis=1)\n",
        "volcano_sc.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc4jhkW2yosP"
      },
      "source": [
        "## Preprocess Landslide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4E9LT4ryosP"
      },
      "outputs": [],
      "source": [
        "landslide_df.ls_size.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chCs14-LyosQ"
      },
      "outputs": [],
      "source": [
        "ls_size_map = {\n",
        "    'small': 1,\n",
        "    'medium': 2,\n",
        "    'large': 3,\n",
        "    'very_large': 4,\n",
        "    'catastrophic': 5,\n",
        "    'unknown': np.nan,\n",
        "    'Unknown': np.nan,\n",
        "    None: np.nan\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0dat2UyyosQ"
      },
      "outputs": [],
      "source": [
        "# Map the size/severity\n",
        "landslide_df[\"ls_size_norm\"] = landslide_df[\"ls_size\"].str.lower().map(ls_size_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPx23eXoyosR"
      },
      "outputs": [],
      "source": [
        "# Clean them first\n",
        "landslide_df[\"fatalities\"] = pd.to_numeric(landslide_df[\"fatalities\"], errors=\"coerce\").fillna(0)\n",
        "landslide_df[\"injuries\"] = pd.to_numeric(landslide_df[\"injuries\"], errors=\"coerce\").fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hAn0cTEyosY"
      },
      "outputs": [],
      "source": [
        "landslide_df[\"casualty_score\"] = landslide_df[\"fatalities\"] + 0.5 * landslide_df[\"injuries\"]\n",
        "\n",
        "# Normalize casualty score (so it's 0â€“5 range)\n",
        "casualty_scaled = MinMaxScaler().fit_transform(landslide_df[[\"casualty_score\"]])\n",
        "landslide_df[\"casualty_scaled\"] = casualty_scaled * 5\n",
        "\n",
        "# Final intensity: weighted combo\n",
        "landslide_df[\"intensity\"] = (\n",
        "    landslide_df[\"ls_size_norm\"].fillna(0) * 0.7 +\n",
        "    landslide_df[\"casualty_scaled\"].fillna(0) * 0.3\n",
        ").round()\n",
        "\n",
        "# For simpler version, use landslide size directly without accommodating fatalities\n",
        "#landslide_df[\"intensity\"] = landslide_df[\"ls_size_norm\"]\n",
        "landslide_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9enH1TJIyosY"
      },
      "outputs": [],
      "source": [
        "def estimate_landslide_duration(size):\n",
        "    if pd.isna(size):\n",
        "        return random.randint(30, 180)  # fallback: 30 min â€“ 3 hrs\n",
        "    duration_ranges = {\n",
        "        1: (10, 60),\n",
        "        2: (30, 90),\n",
        "        3: (60, 120),\n",
        "        4: (180, 360),\n",
        "        5: (360, 1440)\n",
        "    }\n",
        "    return random.randint(*duration_ranges.get(int(size), (30, 180)))\n",
        "\n",
        "landslide_df[\"duration_minutes\"] = landslide_df[\"ls_size_norm\"].apply(estimate_landslide_duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOLa-6hxyosZ"
      },
      "outputs": [],
      "source": [
        "len(main_db[main_db[\"HazardType\"] == 2].copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y1USahhyosZ"
      },
      "outputs": [],
      "source": [
        "dupes = landslide_df[\"HazardID\"].value_counts()\n",
        "print(dupes[dupes > 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7ZUzo49yosa"
      },
      "outputs": [],
      "source": [
        "# Keep only one row per HazardID (e.g., first)\n",
        "landslide_df_dedup = landslide_df.drop_duplicates(subset=\"HazardID\", keep=\"first\")\n",
        "\n",
        "# Merge data to main landslide in database\n",
        "# 1. Ensure keys match (rename 'ev_id' to 'HazardID' if needed)\n",
        "landslide_df = landslide_df.rename(columns={\"ev_id\": \"HazardID\"})\n",
        "\n",
        "# 2. Select only the fields to bring over\n",
        "fields_to_merge = [\"HazardID\", \"ls_size\", \"fatalities\", \"injuries\", \"intensity\", \"duration_minutes\"]\n",
        "\n",
        "# # 3. Merge into filtered main_db landslide records\n",
        "landslide_db = main_db[main_db[\"HazardType\"] == 2].copy()\n",
        "\n",
        "# 4. Merge attributes\n",
        "landslide_db = landslide_db.merge(\n",
        "    landslide_df_dedup[fields_to_merge],\n",
        "    on=\"HazardID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "landslide_db.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOqsdqq-yosa"
      },
      "source": [
        "### Economic loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EbZ2o7Myosb"
      },
      "outputs": [],
      "source": [
        "def simulate_landslide_econ_loss(intensity, pop, fatalities=0, injuries=0):\n",
        "    # Base economic ranges per intensity (in millions USD)\n",
        "    base_ranges = {\n",
        "        1: (0.1, 1),\n",
        "        2: (1, 5),\n",
        "        3: (5, 20),\n",
        "        4: (20, 100),\n",
        "        5: (100, 300)\n",
        "    }\n",
        "\n",
        "    # Get the base economic loss range\n",
        "    low, high = base_ranges.get(int(intensity), (1, 10))\n",
        "\n",
        "    # Normalize population (avoid zero division)\n",
        "    pop_factor = (pop / 1e6) if pop and pop > 0 else 1\n",
        "\n",
        "    # Casualty impact (additive boost)\n",
        "    casualty_boost = (fatalities + 0.5 * injuries) * 0.05  # tweak this multiplier as needed\n",
        "\n",
        "    # Final simulated economic loss (in millions USD)\n",
        "    econ_loss = random.uniform(low, high) * pop_factor + casualty_boost\n",
        "    return round(econ_loss, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGa697Asyosb"
      },
      "outputs": [],
      "source": [
        "landslide_db[\"economic_loss_million\"] = landslide_db.apply(\n",
        "    lambda r: simulate_landslide_econ_loss(\n",
        "        r[\"intensity\"],\n",
        "        r[\"pop\"],\n",
        "        r.get(\"fatalities\", 0),\n",
        "        r.get(\"injuries\", 0)\n",
        "    ),\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjE0TlXmyosc"
      },
      "source": [
        "## Tsunami"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FpXvePtyosc"
      },
      "outputs": [],
      "source": [
        "# Load\n",
        "# Use known item ID\n",
        "item_id3 = \"5a44c3d4d465498993120b70ab568876\"  # Item ID\n",
        "tsun = gis.content.get(item_id3)\n",
        "\n",
        "# Inspect the item\n",
        "print(f\"Title: {tsun.title}\")\n",
        "print(f\"Type: {tsun.type}\")\n",
        "\n",
        "# Access the first layer in the feature service\n",
        "tsun_layer = tsun.layers[0]\n",
        "\n",
        "# Load it into a Spatially Enabled DataFrame (SeDF)\n",
        "tsun_sdf = tsun_layer.query(where=\"1=1\").sdf\n",
        "tsun_sdf.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4GXaVuvyosc"
      },
      "outputs": [],
      "source": [
        "tsun_sdf.TS_MT_II.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTgO5GO4yosd"
      },
      "outputs": [],
      "source": [
        "tsun_sdf.TS_INTENSITY.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W3qk2ugyose"
      },
      "outputs": [],
      "source": [
        "# Map the intensity from IIDA intensity\n",
        "def map_tsunami_intensity(ts_mt_ii):\n",
        "    if pd.isna(ts_mt_ii):\n",
        "        return np.nan\n",
        "    elif ts_mt_ii < 1:\n",
        "        return 1\n",
        "    elif ts_mt_ii < 3:\n",
        "        return 2\n",
        "    elif ts_mt_ii < 4.5:\n",
        "        return 3\n",
        "    elif ts_mt_ii < 6:\n",
        "        return 4\n",
        "    else:\n",
        "        return 5\n",
        "\n",
        "tsun_sdf[\"intensity\"] = tsun_sdf[\"TS_MT_II\"].apply(map_tsunami_intensity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fTRlEP9yose"
      },
      "outputs": [],
      "source": [
        "def fallback_from_runup(runup):\n",
        "    if pd.isna(runup):\n",
        "        return np.nan\n",
        "    elif runup < 1:\n",
        "        return 1\n",
        "    elif runup < 3:\n",
        "        return 2\n",
        "    elif runup < 6:\n",
        "        return 3\n",
        "    elif runup < 10:\n",
        "        return 4\n",
        "    else:\n",
        "        return 5\n",
        "\n",
        "tsun_sdf.loc[tsun_sdf[\"intensity\"].isna(), \"intensity\"] = tsun_sdf.loc[\n",
        "    tsun_sdf[\"intensity\"].isna(), \"MAX_EVENT_RUNUP\"].apply(fallback_from_runup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X79ZhTtEyose"
      },
      "outputs": [],
      "source": [
        "tsun_intensity_probs = [0.05, 0.15, 0.4, 0.25, 0.15]\n",
        "tsun_intensity_vals = [1, 2, 3, 4, 5]\n",
        "\n",
        "missing_mask = tsun_sdf[\"intensity\"].isna()\n",
        "tsun_sdf.loc[missing_mask, \"intensity\"] = np.random.choice(\n",
        "    tsun_intensity_vals,\n",
        "    size=missing_mask.sum(),\n",
        "    p=tsun_intensity_probs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur1X1-ZRyosf"
      },
      "outputs": [],
      "source": [
        "def estimate_tsunami_duration(intensity):\n",
        "    if pd.isna(intensity):\n",
        "        return random.randint(60, 240)  # fallback 1â€“4 hours\n",
        "    ranges = {\n",
        "        1: (15, 30),\n",
        "        2: (30, 90),\n",
        "        3: (90, 180),\n",
        "        4: (180, 360),\n",
        "        5: (360, 720),\n",
        "    }\n",
        "    return random.randint(*ranges[intensity])\n",
        "\n",
        "tsun_sdf[\"duration_minutes\"] = tsun_sdf[\"intensity\"].apply(estimate_tsunami_duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3MKYUb4yosf"
      },
      "outputs": [],
      "source": [
        "# Description fallback map\n",
        "desc_map = {\n",
        "    \"Limited (<$1 million)\": 0.5,\n",
        "    \"Moderate (~$1 to $5 million)\": 3,\n",
        "    \"Severe (~>$5 to $24 million)\": 15,\n",
        "    \"Extreme (~$25 million or more)\": 50\n",
        "}\n",
        "\n",
        "tsun_sdf[\"damage_desc_est\"] = tsun_sdf[\"DAMAGE_TOTAL_DESCRIPTION\"].map(desc_map)\n",
        "\n",
        "# Primary economic value\n",
        "tsun_sdf[\"economic_loss_million\"] = tsun_sdf[\"DAMAGE_MILLIONS_DOLLARS_TOTAL\"]\n",
        "tsun_sdf[\"economic_loss_million\"] = tsun_sdf[\"economic_loss_million\"].fillna(tsun_sdf[\"damage_desc_est\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkCAzs5Kyosg"
      },
      "outputs": [],
      "source": [
        "def simulate_tsun_econ_loss(intensity, pop, fatalities=0, injuries=0):\n",
        "    base_ranges = {\n",
        "        1: (1, 5),\n",
        "        2: (5, 20),\n",
        "        3: (20, 100),\n",
        "        4: (100, 500),\n",
        "        5: (500, 2000)\n",
        "    }\n",
        "\n",
        "    # Fallbacks for missing inputs\n",
        "    if pd.isna(intensity):\n",
        "        intensity = 3\n",
        "    if pd.isna(pop) or pop <= 0:\n",
        "        pop = 1e6  # assume 1 million as fallback\n",
        "    if pd.isna(fatalities):\n",
        "        fatalities = 0\n",
        "    if pd.isna(injuries):\n",
        "        injuries = 0\n",
        "\n",
        "    low, high = base_ranges.get(int(intensity), (10, 100))\n",
        "    pop_factor = pop / 1e6\n",
        "    casualty_boost = (fatalities + 0.5 * injuries) * 0.1\n",
        "\n",
        "    return round(random.uniform(low, high) * pop_factor + casualty_boost, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2bPD-U6yosg"
      },
      "outputs": [],
      "source": [
        "## Get into filtered main_db tsunami records\n",
        "tsunami_db = main_db[main_db[\"HazardType\"] == 3].copy()\n",
        "tsunami_db.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiqESWSLyosh"
      },
      "outputs": [],
      "source": [
        "tsun_sdf = tsun_sdf.rename(columns={\"ID\": \"HazardID\"})\n",
        "# Drop existing 'pop' in tsun_sdf if it exists\n",
        "if \"pop\" in tsun_sdf.columns:\n",
        "    tsun_sdf = tsun_sdf.drop(columns=[\"pop\"])\n",
        "\n",
        "# Then do the merge safely\n",
        "tsun_sdf = tsun_sdf.merge(\n",
        "    tsunami_db[[\"HazardID\", \"pop\", 'latitude', 'longitude', 'HazardType', 'distance']],\n",
        "    on=\"HazardID\", how=\"left\"\n",
        ")\n",
        "\n",
        "# Simulate remaining missing values\n",
        "tsun_sdf[\"economic_loss_million\"] = tsun_sdf.apply(\n",
        "    lambda r: simulate_tsun_econ_loss(\n",
        "        r[\"intensity\"],\n",
        "        r[\"pop\"],\n",
        "        r.get(\"DEATHS_TOTAL\", 0),\n",
        "        r.get(\"INJURIES_TOTAL\", 0)\n",
        "    ) if pd.isna(r[\"economic_loss_million\"]) else r[\"economic_loss_million\"],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXkS3PWzyosh"
      },
      "outputs": [],
      "source": [
        "# Merge data to main tsunami in database\n",
        "# 1. Ensure keys match (rename 'ev_id' to 'HazardID' if needed)\n",
        "tsunami_df = tsun_sdf.rename(columns={\"ID\": \"HazardID\"})\n",
        "\n",
        "# 2. Select only the fields to bring over\n",
        "fields_to_merge = [\"HazardID\", \"UNIQUE_ID\",\"economic_loss_million\", \"intensity\", \"duration_minutes\"]\n",
        "\n",
        "# # 3. Merge into filtered main_db tsunami records\n",
        "tsunami_db = main_db[main_db[\"HazardType\"] == 3].copy()\n",
        "\n",
        "# 4. Merge attributes\n",
        "tsunami_db = tsunami_db.merge(\n",
        "    tsunami_df[fields_to_merge],\n",
        "    on=\"HazardID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "tsunami_db.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko6YQdNbyosh"
      },
      "outputs": [],
      "source": [
        "# Drop columns 12-14\n",
        "tsunami_db = tsunami_db.drop(['HazardID','geometry'], axis=1)\n",
        "tsunami_db = tsunami_db.rename(columns={\"UNIQUE_ID\": \"HazardID\"})\n",
        "tsunami_db.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQj34YXZyosi"
      },
      "outputs": [],
      "source": [
        "tsunami_db.intensity.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm_biYAQyosj"
      },
      "source": [
        "## Earthquake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdsiXquJyosk"
      },
      "outputs": [],
      "source": [
        "# Select specific columns\n",
        "eq_df = eq_df[['eventID', 'magnitude','depth']].copy()\n",
        "eq_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq6hxzo7yosl"
      },
      "outputs": [],
      "source": [
        "eq_df[\"intensity\"] = pd.cut(\n",
        "    eq_df[\"magnitude\"],\n",
        "    bins=[-float(\"inf\"), 3.5, 5.0, 6.0, 7.0, float(\"inf\")],\n",
        "    labels=[1, 2, 3, 4, 5],\n",
        "    include_lowest=True).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJwndsyuyosl"
      },
      "outputs": [],
      "source": [
        "# Depth-based adjustment\n",
        "depth = eq_df[\"depth\"].fillna(70)  # default to midrange if missing\n",
        "\n",
        "depth_adj = np.select(\n",
        "    [\n",
        "        depth < 10,\n",
        "        (depth >= 10) & (depth < 70),\n",
        "        (depth >= 70) & (depth < 300),\n",
        "        depth >= 300\n",
        "    ],\n",
        "    [1, 0, -1, -2],\n",
        "    default=0\n",
        ")\n",
        "\n",
        "# Adjust intensity and clip to 1â€“5\n",
        "eq_df[\"intensity\"] = np.clip(eq_df[\"intensity\"] + depth_adj, 1, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xRjzOXQyosm"
      },
      "outputs": [],
      "source": [
        "# Fix for nan RF input\n",
        "# eq_df[\"intensity\"] = pd.cut(eq_df[\"magnitude\"], bins=[0, 3.9, 5.9, 6.9, 10], labels=[1, 2, 3, 4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcC76ksnyosm"
      },
      "outputs": [],
      "source": [
        "eq_df[\"intensity\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWmMb-6Lyosn"
      },
      "outputs": [],
      "source": [
        "# Predefine min/max for each intensity level\n",
        "dur_map = {\n",
        "    1: (0.2, 0.5),\n",
        "    2: (0.5, 1),\n",
        "    3: (1, 2),\n",
        "    4: (2, 4),\n",
        "    5: (4, 10),\n",
        "}\n",
        "\n",
        "# Map min and max duration arrays\n",
        "intensities = eq_df[\"intensity\"].to_numpy()\n",
        "min_dur = np.array([dur_map[i][0] for i in intensities])\n",
        "max_dur = np.array([dur_map[i][1] for i in intensities])\n",
        "\n",
        "# Vectorized random duration sampling\n",
        "rng = np.random.default_rng(seed=42)\n",
        "durations = rng.uniform(min_dur, max_dur)\n",
        "\n",
        "# Store duration in minutes\n",
        "eq_df[\"duration_minutes\"] = np.round(durations, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgNXRy1Kyosn"
      },
      "outputs": [],
      "source": [
        "econ_ranges = {\n",
        "    1: (0.01, 0.1),\n",
        "    2: (0.1, 5),\n",
        "    3: (5, 50),\n",
        "    4: (50, 500),\n",
        "    5: (500, 5000)\n",
        "}\n",
        "\n",
        "# Vectorized econ loss using NumPy\n",
        "min_econ = np.array([econ_ranges[i][0] for i in intensities])\n",
        "max_econ = np.array([econ_ranges[i][1] for i in intensities])\n",
        "#pop_array = eq_df[\"pop\"].fillna(1e6).to_numpy()  # fallback if pop missing\n",
        "\n",
        "econ_loss = rng.uniform(min_econ, max_econ) * (pop / 1e6)\n",
        "eq_df[\"economic_loss_million\"] = np.round(econ_loss, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqb5VsDWyoso"
      },
      "outputs": [],
      "source": [
        "# Merge data to main tsunami in database\n",
        "# 1. Ensure keys match (rename 'ev_id' to 'HazardID' if needed)\n",
        "#eq_df = eq_df.rename(columns={\"eventID\": \"HazardID\"})\n",
        "\n",
        "# 2. Select only the fields to bring over\n",
        "#fields_to_merge = [\"HazardID\", \"intensity\", \"duration_minutes\"]\n",
        "\n",
        "# # 3. Merge into filtered main_db tsunami records\n",
        "eq_db = main_db[main_db[\"HazardType\"] == 5].copy()\n",
        "\n",
        "# 4. Merge attributes\n",
        "eq_db = eq_db.merge(\n",
        "    eq_df,\n",
        "    on=\"HazardID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "eq_db.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1R2qDWyyosp"
      },
      "outputs": [],
      "source": [
        "# Read the shapefile from local disk\n",
        "fault_df = gpd.read_file(r\"D:/NDIS_Database/05_Fault/fault_points_wgs84.shp\")\n",
        "fault_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UndkfF7yosp"
      },
      "outputs": [],
      "source": [
        "# Round to 5â€“6 decimals to avoid micro mismatches\n",
        "fault_db[\"lat_rounded\"] = fault_db[\"latitude\"].round(6)\n",
        "fault_db[\"lon_rounded\"] = fault_db[\"longitude\"].round(6)\n",
        "\n",
        "fault_df[\"lat_rounded\"] = fault_df[\"latitude\"].round(6)\n",
        "fault_df[\"lon_rounded\"] = fault_df[\"longitude\"].round(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vciYCkYyosq"
      },
      "outputs": [],
      "source": [
        "fault_db = fault_db.merge(\n",
        "    fault_df[[\n",
        "        \"lat_rounded\", \"lon_rounded\",\n",
        "        \"average_di\", \"net_slip_r\", \"vert_sep_r\",\n",
        "        \"epistemic_\", \"shortening\", \"downthrown\", \"dip_dir\"\n",
        "    ]],\n",
        "    on=[\"lat_rounded\", \"lon_rounded\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "fault_db.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JGxhfX_yosr"
      },
      "outputs": [],
      "source": [
        "dupesf = fault_db[\"HazardID\"].value_counts()\n",
        "print(dupesf[dupesf > 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81C5-lXUyosr"
      },
      "source": [
        "### Engineer Intensity, Urgency and Economic loss for fault"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT0JnfJhyosr"
      },
      "outputs": [],
      "source": [
        "# Intensity: Fill missing inputs\n",
        "fault_db[\"average_di\"] = pd.to_numeric(fault_db[\"average_di\"], errors=\"coerce\").fillna(0)\n",
        "fault_db[\"net_slip_r\"] = pd.to_numeric(fault_db[\"net_slip_r\"], errors=\"coerce\").fillna(0)\n",
        "fault_db[\"vert_sep_r\"] = pd.to_numeric(fault_db[\"vert_sep_r\"], errors=\"coerce\").fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyS-gAD-yoss"
      },
      "outputs": [],
      "source": [
        "fault_score = (\n",
        "    (fault_db[\"average_di\"] / 90.0) * 0.3 +\n",
        "    (fault_db[\"net_slip_r\"] / 100.0) * 0.4 +\n",
        "    (fault_db[\"vert_sep_r\"] / 50.0) * 0.3\n",
        ")\n",
        "\n",
        "# Scale to 1â€“5 range\n",
        "fault_db[\"intensity\"] = pd.cut(\n",
        "    fault_score,\n",
        "    bins=[-float(\"inf\"), 0.1, 0.3, 0.6, 0.9, float(\"inf\")],\n",
        "    labels=[1, 2, 3, 4, 5]\n",
        ").astype(\"Int64\")\n",
        "# Duration: Fill NaN intensity with a default before mapping\n",
        "intensity_vals = fault_db[\"intensity\"].fillna(3).to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBJ5Hsmayoss"
      },
      "outputs": [],
      "source": [
        "# Fault Duration (in minutes)\n",
        "duration_map = {\n",
        "    1: (60, 180),\n",
        "    2: (180, 360),\n",
        "    3: (360, 720),\n",
        "    4: (720, 1440),\n",
        "    5: (1440, 2880)\n",
        "}\n",
        "\n",
        "intensity_vals = fault_db[\"intensity\"].fillna(3).to_numpy()\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "min_dur = np.array([duration_map[i][0] for i in intensity_vals])\n",
        "max_dur = np.array([duration_map[i][1] for i in intensity_vals])\n",
        "\n",
        "fault_db[\"duration_minutes\"] = np.round(rng.uniform(min_dur, max_dur), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqXSu8V2yost"
      },
      "outputs": [],
      "source": [
        "# Simulated Economic Loss\n",
        "fault_db[\"pop\"] = fault_db[\"pop\"].fillna(1e6)\n",
        "\n",
        "econ_map = {\n",
        "    1: (0.1, 2),\n",
        "    2: (2, 10),\n",
        "    3: (10, 50),\n",
        "    4: (50, 200),\n",
        "    5: (200, 1000)\n",
        "}\n",
        "\n",
        "min_econ = np.array([econ_map[i][0] for i in intensity_vals])\n",
        "max_econ = np.array([econ_map[i][1] for i in intensity_vals])\n",
        "\n",
        "fault_db[\"economic_loss_million\"] = np.round(\n",
        "    rng.uniform(min_econ, max_econ) * (fault_db[\"pop\"] / 1e6), 2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUTjNyiXyost"
      },
      "outputs": [],
      "source": [
        "# Economic Loss: Fallback for population, if pop is na\n",
        "# fault_db[\"pop\"] = fault_db[\"pop\"].fillna(1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjjxMuYmyost"
      },
      "outputs": [],
      "source": [
        "# Keep only one row per HazardID (e.g., first)\n",
        "fault_db_dedup = fault_db.drop_duplicates(subset=\"HazardID\", keep=\"first\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DSB7itMyost"
      },
      "outputs": [],
      "source": [
        "fault_features = fault_db_dedup[[\n",
        "    \"HazardID\", \"latitude\", \"longitude\", \"HazardType\", \"distance\",\n",
        "    \"pop\", \"intensity\", \"duration_minutes\", \"economic_loss_million\"\n",
        "]].copy()\n",
        "fault_features.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwOmNCXnyosu"
      },
      "outputs": [],
      "source": [
        "fault_features.to_csv(r\"D:\\NDIS_Database\\05_Fault\\fault_rf.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS-s8Dosyosu"
      },
      "source": [
        "# Stack datasets back into one DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7lpmAcqyosv"
      },
      "outputs": [],
      "source": [
        "# Make a copy so we don't modify original\n",
        "volcano_trimmed = volcano_sc.rename(columns={\n",
        "    \"avg_duration\": \"duration_minutes\",\n",
        "    \"econ_loss_million\": \"economic_loss_million\"\n",
        "})[[\n",
        "    \"HazardID\", \"latitude\", \"longitude\", \"HazardType\", \"distance\",\n",
        "    \"pop\", \"intensity\", \"duration_minutes\", \"economic_loss_million\"\n",
        "]]\n",
        "volcano_trimmed.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tCr9L3Eyosv"
      },
      "outputs": [],
      "source": [
        "all_hazards_df = pd.concat([\n",
        "    volcano_trimmed[[\n",
        "        \"HazardID\", \"latitude\", \"longitude\", \"HazardType\", \"distance\", \"pop\",\n",
        "        \"intensity\", \"duration_minutes\", \"economic_loss_million\"\n",
        "    ]],\n",
        "    landslide_db[[\n",
        "        \"HazardID\", \"latitude\", \"longitude\", \"HazardType\", \"distance\", \"pop\",\n",
        "        \"intensity\", \"duration_minutes\", \"economic_loss_million\"\n",
        "    ]],\n",
        "    tsunami_db[[\n",
        "        \"HazardID\", \"latitude\", \"longitude\", \"HazardType\", \"distance\", \"pop\",\n",
        "        \"intensity\", \"duration_minutes\", \"economic_loss_million\"\n",
        "    ]],\n",
        "    eq_db[[\n",
        "        \"HazardID\", \"latitude\", \"longitude\", \"HazardType\", \"distance\", \"pop\",\n",
        "        \"intensity\", \"duration_minutes\", \"economic_loss_million\"\n",
        "    ]],\n",
        "    fault_features[[\n",
        "        \"HazardID\", \"latitude\", \"longitude\", \"HazardType\", \"distance\", \"pop\",\n",
        "        \"intensity\", \"duration_minutes\", \"economic_loss_million\"\n",
        "    ]]\n",
        "], ignore_index=True)\n",
        "all_hazards_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBKQHQqdyosw"
      },
      "source": [
        "# CPM Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFwKHfhUyosx"
      },
      "source": [
        "## Step 1: Sensor Selection (based on HazardType)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKgWr1CVyosx"
      },
      "outputs": [],
      "source": [
        "def select_sensor(hazard_type, distance):\n",
        "    # Exclude heavy sensors\n",
        "    excluded = [\"Radarpod\", \"BPR\"]\n",
        "\n",
        "    if hazard_type == 2:  # Landslide\n",
        "        if distance <= 100:\n",
        "            return \"GPR\"\n",
        "        else:\n",
        "            return \"Lidar\"\n",
        "    else:\n",
        "        return \"Seismic\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN2X7Jngyosx"
      },
      "outputs": [],
      "source": [
        "geohazard_df = all_hazards_df.copy()\n",
        "geohazard_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7H2Ayl3yosy"
      },
      "outputs": [],
      "source": [
        "# Step 1: Default all sensors to \"Seismic\"\n",
        "geohazard_df[\"selected_sensor\"] = \"Seismic\"\n",
        "\n",
        "# Step 2: Assign \"GPR\" to Landslides with distance â‰¤ 100m\n",
        "geohazard_df.loc[(geohazard_df[\"HazardType\"] == 2) & (geohazard_df[\"distance\"] <= 100), \"selected_sensor\"] = \"GPR\"\n",
        "\n",
        "# Step 3: Assign \"Lidar\" to Landslides with distance > 100m\n",
        "geohazard_df.loc[(geohazard_df[\"HazardType\"] == 2) & (geohazard_df[\"distance\"] > 100), \"selected_sensor\"] = \"Lidar\"\n",
        "\n",
        "# Step 4: Merge with Sensor Data to get sensor properties\n",
        "geohazard_df = geohazard_df.merge(sensor_df, left_on=\"selected_sensor\", right_on=\"sensor_name\", how=\"left\")\n",
        "\n",
        "geohazard_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRStQ6Dyosy"
      },
      "source": [
        "## Step 2: Drone Matching (based on payload, range, fallback for small distances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6oWhGGmyosz"
      },
      "outputs": [],
      "source": [
        "# Step 1: Precompute sorted drone dataset\n",
        "drone_df_sorted = drone_df.sort_values([\"distance_range\", \"max_payload_weight\"])\n",
        "\n",
        "# Step 2: Optimized drone selection function\n",
        "def fast_select_drone_v2(geohazard_df, drone_df):\n",
        "    geohazard_df = geohazard_df.copy()\n",
        "    geohazard_df[\"selected_drone\"] = None\n",
        "\n",
        "    for index, row in geohazard_df.iterrows():\n",
        "        hazard_distance = row[\"distance\"]\n",
        "        sensor_weight = sensor_df.loc[sensor_df[\"sensor_name\"] == row[\"selected_sensor\"], \"sensor_weight\"].values\n",
        "        sensor_weight = sensor_weight[0] if len(sensor_weight) > 0 else 0\n",
        "\n",
        "        # Step 1: Find the first drone that can handle short distances\n",
        "        if hazard_distance < drone_df_sorted[\"distance_range\"].min():\n",
        "            matching_drones = drone_df_sorted  # Allow all drones for very short distances\n",
        "        else:\n",
        "            # Step 2: Select drones with matching or closest higher distance\n",
        "            matching_drones = drone_df_sorted[drone_df_sorted[\"distance_range\"] >= hazard_distance]\n",
        "\n",
        "        # Step 3: Filter drones that can carry the sensor payload\n",
        "        feasible_drones = matching_drones[matching_drones[\"max_payload_weight\"] >= sensor_weight]\n",
        "\n",
        "        # Step 4: If no direct match, select the **next available drone that can carry the payload**\n",
        "        if feasible_drones.empty:\n",
        "            sorted_drones = drone_df_sorted[drone_df_sorted[\"max_payload_weight\"] >= sensor_weight]\n",
        "            if not sorted_drones.empty:\n",
        "                best_drone = sorted_drones.iloc[0]\n",
        "            else:\n",
        "                best_drone = None\n",
        "        else:\n",
        "            best_drone = feasible_drones.iloc[0]\n",
        "\n",
        "        # Assign the best drone found\n",
        "        geohazard_df.at[index, \"selected_drone\"] = best_drone[\"mfc_model\"] if best_drone is not None else \"No suitable drone found\"\n",
        "\n",
        "    return geohazard_df\n",
        "\n",
        "# Apply the optimized selection\n",
        "geohazard_df = fast_select_drone_v2(geohazard_df, drone_df)\n",
        "geohazard_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udYaFOhcyos0"
      },
      "outputs": [],
      "source": [
        "# Step 1: Merge drone speed and flight_time from drone_df\n",
        "drone_info = drone_df[[\"mfc_model\", \"max_speed\", \"flight_time\"]].copy()\n",
        "drone_info = drone_info.rename(columns={\n",
        "    \"mfc_model\": \"selected_drone\",\n",
        "    \"max_speed\": \"drone_speed\",\n",
        "    \"flight_time\": \"drone_flight_time\"\n",
        "})\n",
        "\n",
        "# Merge into geohazard_df\n",
        "geohazard_df = geohazard_df.merge(drone_info, on=\"selected_drone\", how=\"left\")\n",
        "\n",
        "# Step 2: Handle missing speeds (fallback to median)\n",
        "geohazard_df[\"drone_speed\"] = geohazard_df[\"drone_speed\"].fillna(drone_df[\"max_speed\"].median())\n",
        "\n",
        "# Step 3: Calculate travel time and total CPM time\n",
        "geohazard_df[\"travel_time\"] = 2 * (geohazard_df[\"distance\"] / geohazard_df[\"drone_speed\"])\n",
        "geohazard_df[\"monitor_time\"] = geohazard_df[\"duration_minutes\"]  # Or scale based on sensor later if needed\n",
        "geohazard_df[\"cpm_total_time\"] = geohazard_df[\"travel_time\"] + geohazard_df[\"monitor_time\"]\n",
        "\n",
        "# Optional: Round time fields\n",
        "geohazard_df[\"travel_time\"] = geohazard_df[\"travel_time\"].round(2)\n",
        "geohazard_df[\"cpm_total_time\"] = geohazard_df[\"cpm_total_time\"].round(2)\n",
        "geohazard_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P29VL_Iyos0"
      },
      "outputs": [],
      "source": [
        "# Set seaborn style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Plot 1: Histogram of CPM Total Time\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(geohazard_df[\"cpm_total_time\"], bins=50, kde=True, color=\"skyblue\")\n",
        "plt.title(\"Distribution of CPM Total Time (minutes)\")\n",
        "plt.xlabel(\"CPM Total Time\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Boxplot by Hazard Type\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(x=\"HazardType\", y=\"cpm_total_time\", data=geohazard_df)\n",
        "plt.title(\"CPM Total Time by Hazard Type\")\n",
        "plt.xlabel(\"Hazard Type\")\n",
        "plt.ylabel(\"CPM Total Time (minutes)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: Scatter (optional)\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=\"distance\", y=\"cpm_total_time\", hue=\"HazardType\", data=geohazard_df, alpha=0.5)\n",
        "plt.title(\"CPM Total Time vs. Distance\")\n",
        "plt.xlabel(\"Distance to Site (meters)\")\n",
        "plt.ylabel(\"CPM Total Time (minutes)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NenbpPJ7yos1"
      },
      "source": [
        "# In case operation interrupted, START HERE!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSsthMBxyos1"
      },
      "outputs": [],
      "source": [
        "# Count occurrences of \"No suitable drone found\"\n",
        "no_suitable_drones_count = (geohazard_df[\"selected_drone\"] == \"No suitable drone found\").sum()\n",
        "\n",
        "# Display the count\n",
        "print(f\"Number of geohazards with no suitable drone: {no_suitable_drones_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vasJmnE2yos2"
      },
      "outputs": [],
      "source": [
        "geohazard_df.selected_drone.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-bp3pcqyos2"
      },
      "source": [
        "# Random Forest Process Starts Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWCKAXUWyos3"
      },
      "source": [
        "## RF Classification â€“ Mission Recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5lzVDagyos3"
      },
      "outputs": [],
      "source": [
        "geohazard_df[\"intensity\"].fillna(1, inplace=True)  # or median (~3)\n",
        "geohazard_df[\"duration_minutes\"].fillna(30, inplace=True)\n",
        "geohazard_df[\"economic_loss_million\"].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fufKrBayos3"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Step 1: Feature selection\n",
        "# ----------------------\n",
        "features = [\n",
        "    \"HazardType\", \"distance\", \"pop\",\n",
        "    \"intensity\", \"duration_minutes\",\n",
        "    \"economic_loss_million\"\n",
        "]\n",
        "\n",
        "# ----------------------\n",
        "# Step 2: Generate target label\n",
        "# ----------------------\n",
        "geohazard_df[\"drone_sensor_combo\"] = (\n",
        "    geohazard_df[\"selected_drone\"].astype(str) + \" + \" + geohazard_df[\"selected_sensor\"].astype(str)\n",
        ")\n",
        "\n",
        "# ----------------------\n",
        "# Step 3: Drop NaNs and filter rare classes\n",
        "# ----------------------\n",
        "# Count label frequency\n",
        "label_counts = geohazard_df[\"drone_sensor_combo\"].value_counts()\n",
        "\n",
        "# Keep only labels with at least 2 instances\n",
        "# valid_labels = label_counts[label_counts >= 2].index\n",
        "\n",
        "# Filter dataset\n",
        "#rf_df = geohazard_df[\n",
        " #   geohazard_df[\"drone_sensor_combo\"].isin(valid_labels)\n",
        "#].dropna(subset=features + [\"drone_sensor_combo\"]).copy()\n",
        "min_samples = 30  # or 50\n",
        "valid_labels = geohazard_df[\"drone_sensor_combo\"].value_counts()\n",
        "valid_labels = valid_labels[valid_labels >= min_samples].index\n",
        "\n",
        "rf_df = geohazard_df[geohazard_df[\"drone_sensor_combo\"].isin(valid_labels)].copy()\n",
        "\n",
        "# ----------------------\n",
        "# Step 4: Encode target label\n",
        "# ----------------------\n",
        "label_encoder = LabelEncoder()\n",
        "rf_df[\"label_encoded\"] = label_encoder.fit_transform(rf_df[\"drone_sensor_combo\"])\n",
        "\n",
        "# ----------------------\n",
        "# Step 5: Split into training and testing\n",
        "# ----------------------\n",
        "X = rf_df[features]\n",
        "y = rf_df[\"label_encoded\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rad119uRyos4"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Step 1: Feature selection\n",
        "# ----------------------\n",
        "#features = [\n",
        "#    \"HazardType\", \"distance\", \"pop\",\n",
        "#    \"intensity\", \"duration_minutes\",\n",
        "#    \"economic_loss_million\"\n",
        "#]\n",
        "\n",
        "# Save feature list\n",
        "features = [\n",
        "    \"HazardType\", \"distance\", \"pop\", \"intensity\",\n",
        "    \"duration_minutes\", \"economic_loss_million\",\n",
        "    \"sensor_weight\", \"drone_speed\", \"drone_flight_time\"\n",
        "]\n",
        "# ----------------------\n",
        "# Step 2: Generate target label\n",
        "# ----------------------\n",
        "geohazard_df[\"drone_sensor_combo\"] = (\n",
        "    geohazard_df[\"selected_drone\"].astype(str) + \" + \" + geohazard_df[\"selected_sensor\"].astype(str)\n",
        ")\n",
        "\n",
        "# ----------------------\n",
        "# Step 3: Drop NaNs and filter rare classes\n",
        "# ----------------------\n",
        "# Count label frequency\n",
        "label_counts = geohazard_df[\"drone_sensor_combo\"].value_counts()\n",
        "\n",
        "# Keep only labels with at least 2 instances\n",
        "# valid_labels = label_counts[label_counts >= 2].index\n",
        "\n",
        "# Filter dataset\n",
        "#rf_df = geohazard_df[\n",
        " #   geohazard_df[\"drone_sensor_combo\"].isin(valid_labels)\n",
        "#].dropna(subset=features + [\"drone_sensor_combo\"]).copy()\n",
        "min_samples = 30  # or 50\n",
        "valid_labels = geohazard_df[\"drone_sensor_combo\"].value_counts()\n",
        "valid_labels = valid_labels[valid_labels >= min_samples].index\n",
        "\n",
        "rf_df = geohazard_df[geohazard_df[\"drone_sensor_combo\"].isin(valid_labels)].copy()\n",
        "\n",
        "# ----------------------\n",
        "# Step 4: Encode target label\n",
        "# ----------------------\n",
        "label_encoder = LabelEncoder()\n",
        "rf_df[\"label_encoded\"] = label_encoder.fit_transform(rf_df[\"drone_sensor_combo\"])\n",
        "\n",
        "# ----------------------\n",
        "# Step 5: Split into training and testing\n",
        "# ----------------------\n",
        "X = rf_df[features]\n",
        "y = rf_df[\"label_encoded\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wHHnmxhyos4"
      },
      "outputs": [],
      "source": [
        "# Check how many NaNs per column\n",
        "print(rf_df[features].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TeSdn-Lyos4"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Step 6: Train Random Forest classifier\n",
        "# ----------------------\n",
        "start_time = timeit.default_timer()\n",
        "#rf_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight=\"balanced\")\n",
        "#rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Uses Multiprocessing !BEWARE using all core might crash other running app and eventually TERMINATE this process\n",
        "#n_cores = max(1, multiprocessing.cpu_count() // 2)\n",
        "\n",
        "#rf_model = RandomForestClassifier(n_estimators=100, n_jobs=n_cores, max_depth=12,\n",
        "#                                  class_weight=\"balanced\", random_state=42)\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=12,\n",
        "    max_features=\"sqrt\",\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 RF Training completed! Elapsed time: %s minutes\" % str(elapsed / 60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-kw6IFkyos5"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "joblib.dump(rf_model, r\"D:\\NDIS_Database\\rf_mission_model.joblib\")\n",
        "\n",
        "# Save label encoder\n",
        "joblib.dump(label_encoder, r\"D:\\NDIS_Database\\rf_label_encoder.joblib\")\n",
        "\n",
        "# Save feature list\n",
        "feature_list = [\n",
        "    \"HazardType\", \"distance\", \"pop\", \"intensity\",\n",
        "    \"duration_minutes\", \"economic_loss_million\",\n",
        "    \"sensor_weight\", \"drone_speed\", \"drone_flight_time\"\n",
        "]\n",
        "with open(r\"D:\\NDIS_Database\\rf_features.json\", \"w\") as f:\n",
        "    json.dump(feature_list, f)\n",
        "\n",
        "print(\"\\u2705 All model assets saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9aXbACoyos5"
      },
      "outputs": [],
      "source": [
        "# Load saved model and label encoder\n",
        "rf_model = joblib.load(r'D:\\NDIS_Database\\14_ExB_Dev\\ndis2\\rf_mission_model.joblib')\n",
        "label_encoder = joblib.load(r'D:\\NDIS_Database\\14_ExB_Dev\\ndis2\\rf_label_encoder.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20xdh7M5yos5"
      },
      "outputs": [],
      "source": [
        "# Option B: Two-step pipeline (Sensor â†’ Drone)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Setup\n",
        "# ---------------------------\n",
        "context_features = [\n",
        "    \"HazardType\", \"distance\", \"pop\",\n",
        "    \"intensity\", \"duration_minutes\", \"economic_loss_million\"\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Predict Sensor\n",
        "# ---------------------------\n",
        "sensor_df = geohazard_df.dropna(subset=context_features + [\"selected_sensor\"]).copy()\n",
        "\n",
        "# Encode sensor labels\n",
        "sensor_encoder = LabelEncoder()\n",
        "sensor_df[\"sensor_label\"] = sensor_encoder.fit_transform(sensor_df[\"selected_sensor\"])\n",
        "\n",
        "X_sensor = sensor_df[context_features]\n",
        "y_sensor = sensor_df[\"sensor_label\"]\n",
        "\n",
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
        "    X_sensor, y_sensor, stratify=y_sensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "sensor_model = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=42)\n",
        "sensor_model.fit(X_train_s, y_train_s)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Sensor Prediction Report:\")\n",
        "y_pred_s = sensor_model.predict(X_test_s)\n",
        "print(classification_report(y_test_s, y_pred_s, target_names=sensor_encoder.classes_))\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Predict Drone\n",
        "# ---------------------------\n",
        "drone_df = geohazard_df.dropna(subset=context_features + [\"selected_sensor\", \"selected_drone\"]).copy()\n",
        "\n",
        "# Add encoded sensor as input feature\n",
        "sensor_map = dict(zip(sensor_df[\"HazardID\"], sensor_df[\"sensor_label\"]))\n",
        "drone_df[\"sensor_label\"] = drone_df[\"HazardID\"].map(sensor_map)\n",
        "\n",
        "# Drop if sensor couldn't be mapped\n",
        "drone_df = drone_df.dropna(subset=[\"sensor_label\"]).copy()\n",
        "drone_df[\"sensor_label\"] = drone_df[\"sensor_label\"].astype(int)\n",
        "\n",
        "# Encode drone labels\n",
        "drone_encoder = LabelEncoder()\n",
        "drone_df[\"drone_label\"] = drone_encoder.fit_transform(drone_df[\"selected_drone\"])\n",
        "\n",
        "X_drone = drone_df[context_features + [\"sensor_label\"]]\n",
        "y_drone = drone_df[\"drone_label\"]\n",
        "\n",
        "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
        "    X_drone, y_drone, stratify=y_drone, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "drone_model = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=42)\n",
        "drone_model.fit(X_train_d, y_train_d)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nDrone Prediction Report:\")\n",
        "y_pred_d = drone_model.predict(X_test_d)\n",
        "print(classification_report(y_test_d, y_pred_d, target_names=drone_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPnZPVN5yos6"
      },
      "source": [
        "## RF Prediction and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-m_DiWVyos6"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Step 7: Predict and evaluate\n",
        "# ----------------------\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Only include labels present in y_test\n",
        "present_labels = sorted(set(y_test))\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(\n",
        "    y_test, y_pred,\n",
        "    labels=present_labels,\n",
        "    target_names=label_encoder.inverse_transform(present_labels)\n",
        "))\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Step 8: Add predictions to full dataset\n",
        "# ----------------------\n",
        "rf_df[\"predicted_label\"] = label_encoder.inverse_transform(rf_model.predict(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhGtDqDbyos7"
      },
      "outputs": [],
      "source": [
        "# Try random guessing baseline (predict most frequent class)\n",
        "most_common = rf_df[\"drone_sensor_combo\"].mode()[0]\n",
        "baseline_acc = (rf_df[\"drone_sensor_combo\"] == most_common).mean()\n",
        "print(f\"Baseline (majority class) accuracy: {baseline_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elZ8iD6cyos7"
      },
      "outputs": [],
      "source": [
        "print(rf_df.groupby(\"HazardType\")[\"drone_sensor_combo\"].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU0za5Ekyos8"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "classes = label_encoder.inverse_transform(range(len(label_encoder.classes_)))\n",
        "\n",
        "# Create heatmap (optional: limit to top 10)\n",
        "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "top_classes = pd.Series(y_test).value_counts().nlargest(10).index\n",
        "top_class_labels = label_encoder.inverse_transform(top_classes)\n",
        "\n",
        "cm_df_small = cm_df.loc[top_class_labels, top_class_labels]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_df_small, annot=True, fmt='g', cmap='Blues', cbar=True, square=True)\n",
        "plt.title('Confusion Matrix (Top 10 Drone+Sensor Labels)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU-ZMYspyos8"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI1IgZFXyos9"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Step 9: Export result to CSV\n",
        "# ----------------------\n",
        "rf_df[[\n",
        "    \"HazardID\", \"predicted_label\", \"drone_sensor_combo\", *features, \"cpm_total_time\"\n",
        "]].to_csv(r\"D:\\NDIS_Database\\rf_mission_prediction.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhmslZZYyos9"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "with open(\"rf_mission_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(rf_model, f)\n",
        "\n",
        "# Save label encoder\n",
        "with open(\"rf_label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "# Save feature list (still use JSON here, it's lightweight and human-readable)\n",
        "feature_list = [\n",
        "    \"HazardType\", \"distance\", \"pop\", \"intensity\",\n",
        "    \"duration_minutes\", \"economic_loss_million\",\n",
        "    \"sensor_weight\", \"drone_speed\", \"drone_flight_time\"\n",
        "]\n",
        "with open(\"rf_features.json\", \"w\") as f:\n",
        "    json.dump(feature_list, f)\n",
        "\n",
        "print(\"âœ… All model assets saved using pickle.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYG_q_xlyos-"
      },
      "outputs": [],
      "source": [
        "with open(\"rf_mission_model.pkl\", \"rb\") as f:\n",
        "    rf_model = pickle.load(f)\n",
        "\n",
        "with open(\"rf_label_encoder.pkl\", \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "with open(\"rf_features.json\") as f:\n",
        "    feature_list = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x34KI_n-yos-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv0QZwX7yos-"
      },
      "source": [
        "# Create a predict_mission() Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhSOdju1yos_"
      },
      "outputs": [],
      "source": [
        "def predict_mission(input_dict):\n",
        "    # Load assets\n",
        "    model = joblib.load(\"rf_mission_model.joblib\")\n",
        "    encoder = joblib.load(\"rf_label_encoder.joblib\")\n",
        "    with open(\"rf_features.json\") as f:\n",
        "        features = json.load(f)\n",
        "\n",
        "    # Convert input to DataFrame\n",
        "    X_input = pd.DataFrame([input_dict])[features]\n",
        "\n",
        "    # Predict class\n",
        "    pred_class = model.predict(X_input)[0]\n",
        "    pred_label = encoder.inverse_transform([pred_class])[0]\n",
        "\n",
        "    return {\n",
        "        \"recommended_combo\": pred_label\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4iXktpmyos_"
      },
      "outputs": [],
      "source": [
        "# Load saved model and label encoder\n",
        "rf_model = joblib.load(r'D:\\NDIS_Database\\14_ExB_Dev\\ndis2\\rf_mission_model.joblib')\n",
        "label_encoder = joblib.load(r'D:\\NDIS_Database\\14_ExB_Dev\\ndis2\\rf_label_encoder.joblib')\n",
        "\n",
        "# Full dataset\n",
        "df = geohazard_df\n",
        "\n",
        "X = df.drop('drone_sensor_combo', axis=1)\n",
        "y = label_encoder.transform(df['drone_sensor_combo'])  # encode using same encoder\n",
        "\n",
        "# Optionally split to simulate testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# ----------------------\n",
        "# Step 4: Encode target label\n",
        "# ----------------------\n",
        "label_encoder = LabelEncoder()\n",
        "rf_df[\"label_encoded\"] = label_encoder.fit_transform(df[\"drone_sensor_combo\"])\n",
        "\n",
        "# ----------------------\n",
        "# Step 5: Split into training and testing\n",
        "# ----------------------\n",
        "X = rf_df[features]\n",
        "y = rf_df[\"label_encoded\"]\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "classes = label_encoder.inverse_transform(range(len(label_encoder.classes_)))\n",
        "\n",
        "# Create heatmap (optional: limit to top 10)\n",
        "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "top_classes = pd.Series(y_test).value_counts().nlargest(10).index\n",
        "top_class_labels = label_encoder.inverse_transform(top_classes)\n",
        "\n",
        "cm_df_small = cm_df.loc[top_class_labels, top_class_labels]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm_df_small, annot=True, fmt='g', cmap='Blues', cbar=True, square=True)\n",
        "plt.title('Confusion Matrix (Top 10 Drone+Sensor Labels)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVbpSFqHyotA"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76aCNRgvyotA"
      },
      "source": [
        "# For Display Purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PPXx_NlyotA"
      },
      "outputs": [],
      "source": [
        "ghz_gpkg = r\"D:\\NDIS_Database\\ghz84.gpkg\"\n",
        "# Load the GeoPackage\n",
        "ghz = gpd.read_file(ghz_gpkg, layer=\"ghz84\")\n",
        "ghz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5glqxdkjyotC"
      },
      "outputs": [],
      "source": [
        "# Step 1: Drop geometry from ghz if it's a GeoDataFrame\n",
        "ghz_df = pd.DataFrame(ghz.drop(columns='geometry'))\n",
        "\n",
        "# Round coordinates to avoid float mismatch\n",
        "geohazard_df['latitude'] = geohazard_df['latitude'].round(6)\n",
        "geohazard_df['longitude'] = geohazard_df['longitude'].round(6)\n",
        "ghz_df['latitude'] = ghz_df['latitude'].round(6)\n",
        "ghz_df['longitude'] = ghz_df['longitude'].round(6)\n",
        "\n",
        "# Step 2: Create a coordinate tuple in both DataFrames\n",
        "geohazard_df['coord'] = list(zip(geohazard_df['latitude'], geohazard_df['longitude'], geohazard_df['HazardType']))\n",
        "ghz_df['coord'] = list(zip(ghz_df['latitude'], ghz_df['longitude'], ghz_df['HazardType']))\n",
        "\n",
        "# Step 3: Filter only ghz rows with new coordinates\n",
        "existing_coords = set(geohazard_df['coord'])\n",
        "new_rows = ghz_df[~ghz_df['coord'].isin(existing_coords)]\n",
        "\n",
        "# Step 4: Reindex to match columns in geohazard_df\n",
        "new_rows_reindexed = new_rows.reindex(columns=geohazard_df.columns)\n",
        "\n",
        "print(f\"Total input (ghz): {len(ghz_df)}\")\n",
        "print(f\"Subset (geohazard_df): {len(geohazard_df)}\")\n",
        "print(f\"Filtered new rows: {len(new_rows)}\")\n",
        "print(f\"Expected final count: {len(ghz_df)}\")\n",
        "\n",
        "\n",
        "# Step 5: Concatenate\n",
        "combined_df = pd.concat([geohazard_df, new_rows_reindexed], ignore_index=True)\n",
        "\n",
        "print(f\"Actual final count: {len(combined_df)}\")\n",
        "\n",
        "# Map numeric HazardType to string labels\n",
        "hazard_type_map = {\n",
        "    1: 'Volcano',\n",
        "    2: 'Landslide',\n",
        "    3: 'Tsunami',\n",
        "    4: 'Fault',\n",
        "    5: 'Earthquake'\n",
        "}\n",
        "\n",
        "combined_df['HazardType'] = combined_df['HazardType'].map(hazard_type_map)\n",
        "\n",
        "combined_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWcfkLP_yotC"
      },
      "outputs": [],
      "source": [
        "# How many duplicate coordinate rows in ghz_df?\n",
        "ghz_df['coord'] = list(zip(ghz_df['latitude'], ghz_df['longitude']))\n",
        "dupe_coords = ghz_df.duplicated(subset='coord', keep=False)\n",
        "print(f\"Duplicate rows in ghz_df based on coordinates: {dupe_coords.sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iDaNuUHyotD"
      },
      "outputs": [],
      "source": [
        "# Which geohazard_df coords are NOT in ghz_df?\n",
        "missing_coords = set(geohazard_df['coord']) - set(ghz_df['coord'])\n",
        "print(f\"Coords in geohazard_df that are NOT in ghz_df: {len(missing_coords)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgyQ8WgmyotD"
      },
      "outputs": [],
      "source": [
        "# Check for duplicated coordinates in ghz_df\n",
        "ghz_df['coord'] = list(zip(ghz_df['latitude'], ghz_df['longitude']))\n",
        "dupes = ghz_df.duplicated(subset='coord', keep=False)\n",
        "\n",
        "print(f\"Total duplicate coord rows in ghz_df: {dupes.sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFNRa6k5yotE"
      },
      "outputs": [],
      "source": [
        "# Drop duplicate coordinate rows before filtering\n",
        "ghz_df_unique = ghz_df.drop_duplicates(subset='coord')\n",
        "len(ghz_df_unique)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI0TeVOoyotF"
      },
      "outputs": [],
      "source": [
        "# Step 6: Recreate geometry\n",
        "from shapely.geometry import Point\n",
        "geometry = [Point(xy) for xy in zip(combined_df['longitude'], combined_df['latitude'])]\n",
        "combined_gdf = gpd.GeoDataFrame(combined_df.drop(columns='coord'), geometry=geometry, crs=\"EPSG:4326\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ArcGISPro",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}