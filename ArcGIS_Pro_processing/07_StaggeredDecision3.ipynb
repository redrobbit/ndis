{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCalBk1N_UWu"
      },
      "outputs": [],
      "source": [
        "from arcgis.gis import GIS\n",
        "gis = GIS(\"home\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Zc5eqhk_UW0"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "#ArcGIS packages\n",
        "import arcpy\n",
        "#from arcgis.mapping import WebScene\n",
        "from arcgis.gis import GIS\n",
        "from arcgis.features import FeatureLayer\n",
        "from IPython.display import display\n",
        "from arcgis.features import GeoAccessor\n",
        "from arcgis import *\n",
        "from arcpy.sa import Int\n",
        "# Raster processing for dataframe\n",
        "from rasterstats import zonal_stats\n",
        "import rasterio\n",
        "\n",
        "# basic packages\n",
        "import csv\n",
        "import numpy as np\n",
        "import os\n",
        "import timeit\n",
        "import random\n",
        "import string\n",
        "from playsound import playsound\n",
        "import gc # Force Garbage Collection. This helps reduce memory leaks in long loops.\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import time\n",
        "import threading\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm #Bar status\n",
        "\n",
        "# Data management\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point  # to get points from long lat\n",
        "\n",
        "# Request service\n",
        "#from requests import Request\n",
        "import json\n",
        "import re\n",
        "from functools import reduce\n",
        "#from owslib.wfs import WebFeatureService\n",
        "import sqlite3\n",
        "\n",
        "# Plotting packages\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMUWbWJ9_UW3"
      },
      "outputs": [],
      "source": [
        "# Select only the desired columns from merged_df_final\n",
        "ghz_df  = final_df[[\n",
        "    \"HazardID\",\n",
        "    \"latitude\",\n",
        "    \"longitude\",\n",
        "    \"HazardType\",\n",
        "    \"distance\",\n",
        "    \"intensity\",\n",
        "    \"economic_loss_million\",\n",
        "    \"duration_minutes\",\n",
        "    \"pop\"\n",
        "]].copy()\n",
        "ghz_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF55XJk8_UW3"
      },
      "outputs": [],
      "source": [
        "# Mask for all invalid distance values (NaN or <= 0)\n",
        "invalid_mask = ghz_df['distance'].isna() | (ghz_df['distance'] <= 0)\n",
        "\n",
        "# Exclude Earthquakes and distance == 0\n",
        "excluded_mask = (ghz_df['distance'] == 0)\n",
        "\n",
        "# Final filtered DataFrame\n",
        "invalid_distance_df = ghz_df[invalid_mask & ~excluded_mask].copy()\n",
        "\n",
        "print(\"Filtered invalid distances:\", len(invalid_distance_df))\n",
        "print(invalid_distance_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaCgyjZ1_UW5"
      },
      "outputs": [],
      "source": [
        "invalid_distance_df.to_csv(r\"D:\\NDIS_Database\\06_Infrastructureinvalid_distance_df.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMWQXC3g_UW7"
      },
      "outputs": [],
      "source": [
        "# Count invalid distances per HazardType (excluding Earthquake and distance == 0)\n",
        "invalid_counts = invalid_distance_df['HazardType'].value_counts()\n",
        "\n",
        "print(invalid_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3VLrZz1w_UW8"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "\n",
        "# Paths\n",
        "project_folder = r\"D:\\ArcGISProjects\\GeohazardDB\"\n",
        "\n",
        "ndis_gdb = os.path.join(project_folder, \"ghzgdb_Legacy.gdb\")\n",
        "road_gdb = os.path.join(project_folder, \"GeohazardDB.gdb\")\n",
        "road_layer_template = os.path.join(road_gdb, \"roads\")\n",
        "country_layer = os.path.join(road_gdb, \"eez_country\")\n",
        "\n",
        "invalid_layer = os.path.join(road_gdb, \"invalid_distance\")  # input geohazard points\n",
        "\n",
        "# Near analysis preparation\n",
        "ghz_list = []\n",
        "near_tables = []\n",
        "\n",
        "total_countries = int(arcpy.GetCount_management(country_layer)[0])\n",
        "\n",
        "with arcpy.da.SearchCursor(country_layer, [\"ISO_TER1\", \"SHAPE@\"]) as country_cursor:\n",
        "    for index, row in enumerate(country_cursor, start=1):\n",
        "        iso = row[0]\n",
        "        shape = row[1]\n",
        "        print(f\"⏳ Processing {iso} ({index}/{total_countries})...\")\n",
        "\n",
        "        inv_clip = os.path.join(ndis_gdb, f\"invdist_{iso}\")\n",
        "        road_clip = os.path.join(road_gdb, f\"road_{iso}\")\n",
        "        near_table = os.path.join(ndis_gdb, f\"near_invdist_{iso}\")\n",
        "\n",
        "        # Clip invalid geohazard points\n",
        "        if arcpy.Exists(inv_clip):\n",
        "            arcpy.Delete_management(inv_clip)\n",
        "        arcpy.analysis.Clip(invalid_layer, shape, inv_clip)\n",
        "\n",
        "        # Run Near analysis\n",
        "        if arcpy.Exists(near_table):\n",
        "            arcpy.Delete_management(near_table)\n",
        "        arcpy.analysis.GenerateNearTable(\n",
        "            in_features=inv_clip,\n",
        "            near_features=road_clip,\n",
        "            out_table=near_table,\n",
        "            location=\"LOCATION\",\n",
        "            angle=\"ANGLE\",\n",
        "            closest=\"CLOSEST\",\n",
        "            method=\"GEODESIC\"\n",
        "        )\n",
        "        print(f\"  ✅ Near table created: {near_table}\")\n",
        "        near_tables.append(near_table)\n",
        "\n",
        "        # Add 'distance' to inv_clip\n",
        "        if \"distance\" not in [f.name for f in arcpy.ListFields(inv_clip)]:\n",
        "            arcpy.AddField_management(inv_clip, \"distance\", \"DOUBLE\")\n",
        "\n",
        "        with arcpy.da.UpdateCursor(inv_clip, [\"OBJECTID\", \"distance\"]) as up_cursor:\n",
        "            for up_row in up_cursor:\n",
        "                oid = up_row[0]\n",
        "                with arcpy.da.SearchCursor(near_table, [\"IN_FID\", \"NEAR_DIST\"]) as near_cursor:\n",
        "                    for near_row in near_cursor:\n",
        "                        if near_row[0] == oid:\n",
        "                            up_row[1] = near_row[1]\n",
        "                            up_cursor.updateRow(up_row)\n",
        "                            break\n",
        "\n",
        "        # Add 'HazardID' to near table\n",
        "        if \"HazardID\" not in [f.name for f in arcpy.ListFields(near_table)]:\n",
        "            arcpy.AddField_management(near_table, \"HazardID\", \"TEXT\")\n",
        "\n",
        "        with arcpy.da.UpdateCursor(near_table, [\"IN_FID\", \"HazardID\"]) as cursor:\n",
        "            for row in cursor:\n",
        "                with arcpy.da.SearchCursor(inv_clip, [\"OBJECTID\", \"HazardID\"]) as src:\n",
        "                    for src_row in src:\n",
        "                        if row[0] == src_row[0]:\n",
        "                            row[1] = src_row[1]\n",
        "                            cursor.updateRow(row)\n",
        "                            break\n",
        "\n",
        "        ghz_list.append(inv_clip)\n",
        "\n",
        "# Merge clipped geohazard points\n",
        "merged_output = os.path.join(ndis_gdb, \"invalid_dist_merged\")\n",
        "if arcpy.Exists(merged_output):\n",
        "    arcpy.Delete_management(merged_output)\n",
        "arcpy.Merge_management(ghz_list, merged_output)\n",
        "print(f\"✅ All invalid geohazard points merged into {merged_output}\")\n",
        "\n",
        "# Merge near tables\n",
        "compiled_near_table = os.path.join(ndis_gdb, \"compiled_near_table_invalid\")\n",
        "if arcpy.Exists(compiled_near_table):\n",
        "    arcpy.Delete_management(compiled_near_table)\n",
        "\n",
        "arcpy.CreateTable_management(ndis_gdb, \"compiled_near_table_invalid\")\n",
        "for field in [(\"FROM_X\", \"DOUBLE\"), (\"FROM_Y\", \"DOUBLE\"), (\"NEAR_X\", \"DOUBLE\"),\n",
        "              (\"NEAR_Y\", \"DOUBLE\"), (\"NEAR_FID\", \"LONG\"), (\"HazardID\", \"TEXT\")]:\n",
        "    arcpy.AddField_management(compiled_near_table, field[0], field[1])\n",
        "\n",
        "with arcpy.da.InsertCursor(compiled_near_table, [\"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\", \"NEAR_FID\", \"HazardID\"]) as insert_cursor:\n",
        "    for table in near_tables:\n",
        "        with arcpy.da.SearchCursor(table, [\"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\", \"NEAR_FID\", \"HazardID\"]) as cursor:\n",
        "            for row in cursor:\n",
        "                insert_cursor.insertRow(row)\n",
        "\n",
        "# Create line layer\n",
        "line_fc = os.path.join(ndis_gdb, \"compiled_near_lines_invalid\")\n",
        "if arcpy.Exists(line_fc):\n",
        "    arcpy.Delete_management(line_fc)\n",
        "\n",
        "arcpy.XYToLine_management(\n",
        "    compiled_near_table, line_fc,\n",
        "    \"FROM_X\", \"FROM_Y\", \"NEAR_X\", \"NEAR_Y\"\n",
        ")\n",
        "print(f\"✅ Lines created: {line_fc}\")\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(f\"✅ All invalid distance near analysis completed in {elapsed/60:.2f} minutes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfMLEpRf_UW-"
      },
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfN86dak_UXA"
      },
      "source": [
        "# PreProcessing for Staggered Decision v3.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4Wg6SL8_UXB"
      },
      "outputs": [],
      "source": [
        "# Get the recalculated invalid distance\n",
        "# Path to the merged feature class\n",
        "merged_fc = r\"D:\\ArcGISProjects\\GeohazardDB\\ghzgdb_Legacy.gdb\\invalid_dist_merged\"\n",
        "\n",
        "# List fields to extract\n",
        "fields = [f.name for f in arcpy.ListFields(merged_fc) if f.type not in (\"Geometry\", \"OID\")]\n",
        "\n",
        "# Add geometry fields if needed\n",
        "fields += [\"SHAPE@XY\"]\n",
        "\n",
        "# Read into a DataFrame\n",
        "data = []\n",
        "with arcpy.da.SearchCursor(merged_fc, fields) as cursor:\n",
        "    for row in cursor:\n",
        "        data.append(row)\n",
        "\n",
        "# Create DataFrame\n",
        "invalid_dist_df = pd.DataFrame(data, columns=fields)\n",
        "\n",
        "invalid_dist_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhzN8QG-_UXC"
      },
      "outputs": [],
      "source": [
        "# Filter only rows with non-null distance\n",
        "valid_distance_df = invalid_dist_df[invalid_dist_df[\"distance\"].notna()].copy()\n",
        "\n",
        "# Show count and preview\n",
        "print(\"Valid distances extracted:\", len(valid_distance_df))\n",
        "print(valid_distance_df[[\"HazardID\", \"distance\"]].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgMgO_au_UXE"
      },
      "outputs": [],
      "source": [
        "# Select only the desired columns from merged_df_final\n",
        "valid_distance_df  = valid_distance_df[[\n",
        "    \"HazardID\",\n",
        "    \"latitude\",\n",
        "    \"longitude\",\n",
        "    \"HazardType\",\n",
        "    \"distance\",\n",
        "    \"intensity\",\n",
        "    \"economic_loss_million\",\n",
        "    \"duration_minutes\",\n",
        "    \"pop\"\n",
        "]].copy()\n",
        "valid_distance_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHowf3dV_UXE"
      },
      "outputs": [],
      "source": [
        "nuclear = ghz_df[(ghz_df[\"HazardType\"] == \"Nuclear\")]\n",
        "nuclear.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1vbxaZi_UXG"
      },
      "outputs": [],
      "source": [
        "# Find duplicated HazardIDs\n",
        "duplicates = nuclear[nuclear.duplicated(\"HazardID\", keep=False)]\n",
        "\n",
        "print(\"Total duplicated HazardIDs:\", duplicates[\"HazardID\"].nunique())\n",
        "print(\"Duplicated entries:\")\n",
        "print(duplicates.sort_values(\"HazardID\").head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax2p_D9X_UXG"
      },
      "outputs": [],
      "source": [
        "cleaned_nuclear_df = pd.read_csv(r\"D:\\NDIS_Database\\cleaned_nuclear_df.csv\")\n",
        "cleaned_nuclear_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twOU7SU7_UXH"
      },
      "outputs": [],
      "source": [
        "# Define fields to check\n",
        "fields_to_check = [\"distance\", \"pop\", \"economic_loss_million\", \"intensity\", \"duration_minutes\"]\n",
        "\n",
        "# Tier 1: Fully valid rows\n",
        "valid_mask = nuclear[fields_to_check].notna().all(axis=1) & (nuclear[fields_to_check] > 0).all(axis=1)\n",
        "valid_df = nuclear[valid_mask].copy()\n",
        "valid_df = valid_df.sort_values(\"economic_loss_million\", ascending=False)\n",
        "valid_dedup = valid_df.drop_duplicates(subset=\"HazardID\", keep=\"first\")\n",
        "\n",
        "# Tier 2: All remaining HazardIDs not in Tier 1\n",
        "remaining_df = nuclear[~nuclear[\"HazardID\"].isin(valid_dedup[\"HazardID\"])]\n",
        "fallback_dedup = remaining_df.drop_duplicates(subset=\"HazardID\", keep=\"first\")\n",
        "\n",
        "# Combine both\n",
        "final_dedup = pd.concat([valid_dedup, fallback_dedup], ignore_index=True)\n",
        "\n",
        "print(\"✅ Final deduplicated nuclear count:\", len(final_dedup))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laNro97o_UXJ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Remove all existing Nuclear rows from ghz_df\n",
        "ghz_df_no_nuclear = ghz_df[ghz_df[\"HazardType\"] != \"Nuclear\"].copy()\n",
        "\n",
        "# Step 2: Append final deduplicated nuclear DataFrame\n",
        "ghz_updated = pd.concat([ghz_df_no_nuclear, final_dedup], ignore_index=True)\n",
        "\n",
        "# Step 3: Update rows using valid_distance_df\n",
        "# We'll use HazardID as the join key and update all matching rows\n",
        "\n",
        "# Ensure HazardID is same dtype\n",
        "valid_distance_df[\"HazardID\"] = valid_distance_df[\"HazardID\"].astype(ghz_updated[\"HazardID\"].dtype)\n",
        "\n",
        "# Set HazardID as index for fast lookup\n",
        "ghz_updated.set_index(\"HazardID\", inplace=True)\n",
        "valid_distance_df.set_index(\"HazardID\", inplace=True)\n",
        "\n",
        "# Update all overlapping columns with corrected values\n",
        "ghz_updated.update(valid_distance_df)\n",
        "\n",
        "# Reset index\n",
        "ghz_updated.reset_index(inplace=True)\n",
        "\n",
        "# Done\n",
        "print(\"Final GHZ DataFrame shape:\", ghz_updated.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI_pTDXn_UXJ"
      },
      "outputs": [],
      "source": [
        "volcano = ghz_updated[(ghz_updated[\"HazardType\"] == \"Volcano\")]\n",
        "volcano.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlcFv5S0_UXK"
      },
      "outputs": [],
      "source": [
        "volcano['distance'].notna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmA0XDzN_UXL"
      },
      "outputs": [],
      "source": [
        "ghz_updated['distance'].isna().unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8797axVF_UXL"
      },
      "outputs": [],
      "source": [
        "ghz_updated['distance'].notna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEM8IA6c_UXM"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "project_folder = r\"D:\\ArcGISProjects\\GeohazardDB\"\n",
        "output_gdb = arcpy.CreateFileGDB_management(project_folder, \"ndis3.gdb\")\n",
        "output_fc = os.path.join(project_folder, \"ndis3.gdb\", \"ghz_updated_fc\")\n",
        "\n",
        "# Convert to spatially-enabled DataFrame (requires lat/lon in WGS 1984)\n",
        "sdf = pd.DataFrame(ghz_updated.copy())\n",
        "sdf = sdf[sdf[\"latitude\"].notna() & sdf[\"longitude\"].notna()]  # Ensure valid coordinates\n",
        "\n",
        "# Create point geometry\n",
        "geometry = [arcpy.Point(row[\"longitude\"], row[\"latitude\"]) for idx, row in sdf.iterrows()]\n",
        "spatial_ref = arcpy.SpatialReference(4326)  # WGS 1984\n",
        "features = [arcpy.Polygon(arcpy.Array([pt]), spatial_ref) if isinstance(pt, arcpy.Point) else None for pt in geometry]\n",
        "\n",
        "# Create Feature Class from scratch\n",
        "if arcpy.Exists(output_fc):\n",
        "    arcpy.Delete_management(output_fc)\n",
        "arcpy.CreateFeatureclass_management(out_path=os.path.dirname(output_fc), out_name=os.path.basename(output_fc),\n",
        "                                    geometry_type=\"POINT\", spatial_reference=spatial_ref)\n",
        "\n",
        "# Add fields from DataFrame (skip lat/lon and geometry)\n",
        "fields_to_add = [col for col in sdf.columns if col not in [\"latitude\", \"longitude\"]]\n",
        "for field in fields_to_add:\n",
        "    sample_value = sdf[field].dropna().iloc[0] if not sdf[field].dropna().empty else \"\"\n",
        "    field_type = \"TEXT\"\n",
        "    if pd.api.types.is_integer_dtype(sdf[field]):\n",
        "        field_type = \"LONG\"\n",
        "    elif pd.api.types.is_float_dtype(sdf[field]):\n",
        "        field_type = \"DOUBLE\"\n",
        "    arcpy.AddField_management(output_fc, field, field_type)\n",
        "\n",
        "# Write to feature class\n",
        "insert_fields = [\"SHAPE@\"] + fields_to_add\n",
        "with arcpy.da.InsertCursor(output_fc, insert_fields) as cursor:\n",
        "    for idx, row in sdf.iterrows():\n",
        "        pt = arcpy.Point(row[\"longitude\"], row[\"latitude\"])\n",
        "        values = [pt] + [row[f] for f in fields_to_add]\n",
        "        cursor.insertRow(values)\n",
        "\n",
        "print(f\"✅ Feature class created: {output_fc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRWeNHDC_UXN"
      },
      "outputs": [],
      "source": [
        "# Check for null or zero in 'pop' and 'distance'\n",
        "invalid_pop_dist = ghz_updated[\n",
        "    (ghz_updated[\"pop\"].isna() | (ghz_updated[\"pop\"] == 0)) |\n",
        "    (ghz_updated[\"distance\"].isna() | (ghz_updated[\"distance\"] == 0))\n",
        "]\n",
        "\n",
        "# Show summary\n",
        "print(\"❗ Rows with null or zero in pop or distance:\", len(invalid_pop_dist))\n",
        "print(invalid_pop_dist[[\"HazardID\", \"HazardType\", \"pop\", \"distance\"]].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MpvUyya_UXO"
      },
      "outputs": [],
      "source": [
        "invalid_pop_dist.to_csv(r\"D:\\NDIS_Database\\19_PostProcessing\\invalid_pop_dist.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DrY5jLl_UXO"
      },
      "source": [
        "------\n",
        "# STAGGERED DECISION 3.8.0\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvKU7wir_UXP"
      },
      "outputs": [],
      "source": [
        "def classify_road_distance(d):\n",
        "    if d < 1000:\n",
        "        return \"<1 km\"\n",
        "    elif 1000 <= d < 5000:\n",
        "        return \"1–5 km\"\n",
        "    elif 5000 <= d < 20000:\n",
        "        return \"5–20 km\"\n",
        "    elif 20000 <= d < 50000:\n",
        "        return \"20–50 km\"\n",
        "    else:\n",
        "        return \">50 km\"\n",
        "\n",
        "ghz_updated[\"road_bin\"] = ghz_updated[\"distance\"].apply(classify_road_distance)\n",
        "ghz_updated.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HstKB9g_UXQ"
      },
      "outputs": [],
      "source": [
        "def classify_population_bins(p):\n",
        "    if p == 0:\n",
        "        return \"0\"\n",
        "    elif p < 1000:\n",
        "        return \"<1k\"\n",
        "    elif p < 10000:\n",
        "        return \"1k–10k\"\n",
        "    elif p < 100000:\n",
        "        return \"10k–100k\"\n",
        "    elif p < 1000000:\n",
        "        return \"100k–1M\"\n",
        "    elif p < 10000000:\n",
        "        return \"1M–10M\"\n",
        "    else:\n",
        "        return \">10M\"\n",
        "\n",
        "ghz_updated[\"pop_bin\"] = ghz_updated[\"pop\"].apply(classify_population_bins)\n",
        "ghz_updated.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf-4dIks_UXY"
      },
      "outputs": [],
      "source": [
        "nuclear = ghz_updated[(ghz_updated[\"HazardType\"] == \"Nuclear\")]\n",
        "nuclear.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bmh1Jc-G_UXZ"
      },
      "outputs": [],
      "source": [
        "# List of columns to extract from the 'nuclear_df' dataframe, including Latitude and Longitude\n",
        "selected_columns = [\n",
        "    \"Country/Area\",\n",
        "    \"Project Name\",\n",
        "    \"Capacity (MW)\",\n",
        "    \"Status\",\n",
        "    \"Reactor Type\",\n",
        "    \"Owner\",\n",
        "    \"Wiki URL\",\n",
        "    \"Latitude\",  # Include Latitude for matching\n",
        "    \"Longitude\"  # Include Longitude for matching\n",
        "]\n",
        "\n",
        "# Select only the relevant columns from the 'nuclear_df' dataframe\n",
        "nuclear_df_selected = nuclear_df[selected_columns]\n",
        "\n",
        "# Ensure that latitudes and longitudes are of float type in both dataframes\n",
        "nuclear_df_selected.loc[:, 'Latitude'] = nuclear_df_selected['Latitude'].astype(float)\n",
        "nuclear_df_selected.loc[:, 'Longitude'] = nuclear_df_selected['Longitude'].astype(float)\n",
        "nuclear.loc[:, 'latitude'] = nuclear['latitude'].astype(float)\n",
        "nuclear.loc[:, 'longitude'] = nuclear['longitude'].astype(float)\n",
        "\n",
        "# Define a function to match latitude and longitude with a small tolerance\n",
        "def match_lat_lon(row, df, tolerance=0.001):\n",
        "    # Match latitudes and longitudes within a tolerance range\n",
        "    matched_row = df[\n",
        "        (df['Latitude'].between(row['latitude'] - tolerance, row['latitude'] + tolerance)) &\n",
        "        (df['Longitude'].between(row['longitude'] - tolerance, row['longitude'] + tolerance))\n",
        "    ]\n",
        "    if not matched_row.empty:\n",
        "        return matched_row.iloc[0]  # Return the first matched row\n",
        "    return None  # Return None if no match is found\n",
        "\n",
        "# Merge based on matching latitude and longitude\n",
        "merged_rows = []\n",
        "for idx, row in nuclear.iterrows():\n",
        "    matched_row = match_lat_lon(row, nuclear_df_selected)\n",
        "    if matched_row is not None:\n",
        "        # Combine the original row from 'nuclear' with matched fields from 'nuclear_df'\n",
        "        merged_row = pd.concat([row, matched_row], axis=0)\n",
        "        merged_rows.append(merged_row)\n",
        "\n",
        "# Convert merged rows into a new DataFrame\n",
        "merged_nuclear = pd.DataFrame(merged_rows)\n",
        "merged_nuclear.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMurE6S1_UXZ"
      },
      "outputs": [],
      "source": [
        "# Output folder path\n",
        "output_folder = r\"D:\\NDIS_Database\\20_PaperSimulation\\csv_exports\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Loop through each hazard type and export\n",
        "for hazard in ghz_updated[\"HazardType\"].unique():\n",
        "    df_hazard = ghz_updated[ghz_updated[\"HazardType\"] == hazard].copy()\n",
        "    file_name = hazard.replace(\" \", \"_\").replace(\"-\", \"_\") + \".csv\"\n",
        "    output_path = os.path.join(output_folder, file_name)\n",
        "\n",
        "    df_hazard.to_csv(output_path, index=False)\n",
        "    print(f\"✅ Exported: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q2yKhDD_UXa"
      },
      "outputs": [],
      "source": [
        "# Drop NaNs first (optional)\n",
        "grouped_exact = drone_df.dropna(subset=[\"comm_range\"]).groupby(\"comm_range\")\n",
        "\n",
        "# Count per group\n",
        "print(grouped_exact.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR3DRZkH_UXb"
      },
      "outputs": [],
      "source": [
        "def classify_comm(m):\n",
        "    if pd.isna(m):\n",
        "        return \"Unknown\"\n",
        "    elif m < 1000:\n",
        "        return \"<1km\"\n",
        "    elif m < 5000:\n",
        "        return \"1–5km\"\n",
        "    elif m < 20000:\n",
        "        return \"5–20km\"\n",
        "    elif m < 50000:\n",
        "        return \"20–50km\"\n",
        "    else:\n",
        "        return \">50km\"\n",
        "\n",
        "drone_df[\"comm_category\"] = drone_df[\"comm_range\"].apply(classify_comm)\n",
        "comm_count = drone_df[\"comm_category\"].value_counts().reindex([\"<1km\", \"1–5km\", \"5–20km\", \"20–50km\", \">50km\", \"Unknown\"], fill_value=0)\n",
        "\n",
        "print(comm_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyr0JT_J_UXc"
      },
      "outputs": [],
      "source": [
        "comm_countsr = pd.Series({\n",
        "    \"<1km\": 3,\n",
        "    \"1–5km\": 37,\n",
        "    \"5–20km\": 47,\n",
        "    \"20–50km\": 16,\n",
        "    \">50km\": 15,\n",
        "    \"Unknown\": 61\n",
        "})\n",
        "\n",
        "# Define colorblind-safe palette including one for \"Unknown\"\n",
        "colors = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#999999\"]\n",
        "\n",
        "# Normalize for brightness check\n",
        "def get_text_color(hex_color):\n",
        "    rgb = mcolors.hex2color(hex_color)\n",
        "    brightness = np.dot(rgb, [0.299, 0.587, 0.114])  # luminance\n",
        "    return 'black' if brightness > 0.6 else 'white'\n",
        "\n",
        "# Plot donut chart\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "fig.subplots_adjust(top=0.9, bottom=0.2)\n",
        "wedges, texts, autotexts = ax.pie(\n",
        "    comm_countsr,\n",
        "    labels=comm_countsr.index,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=10,\n",
        "    pctdistance=0.82,\n",
        "    wedgeprops=dict(width=0.4),\n",
        "    colors=colors\n",
        ")\n",
        "\n",
        "# Adjust text styles\n",
        "for i, autotext in enumerate(autotexts):\n",
        "    autotext.set_color(get_text_color(colors[i]))\n",
        "    autotext.set_fontsize(19)\n",
        "for t in texts:\n",
        "    t.set_fontsize(19)\n",
        "\n",
        "# Donut hole\n",
        "centre_circle = plt.Circle((0, 0), 0.65, fc='white')\n",
        "ax.add_artist(centre_circle)\n",
        "\n",
        "# Add center title\n",
        "ax.text(0, 0, \"Drone\\nCommunication\\nRange\", ha='center', va='center', fontsize=24, weight='bold', color='black')\n",
        "\n",
        "# Caption\n",
        "#plt.figtext(0.5, 0.025,\n",
        "#    \"Distribution of drone communication range (including unknown values).\",\n",
        "#    wrap=True, horizontalalignment='center', fontsize=18)\n",
        "\n",
        "ax.axis('equal')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save if needed\n",
        "plt.savefig(r'D:\\NDIS_Database\\13_NDIS_Display\\Page3\\pictures\\drone_comm2.png', dpi=300, transparent=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wr7CCvz_UXc"
      },
      "source": [
        "# STAGE 2: Assign Disaster Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3AzMFRW_UXc"
      },
      "outputs": [],
      "source": [
        "ghz_updated.HazardType.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1YhP_4l_UXd"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define disaster phase + sensor map\n",
        "disaster_map = [\n",
        "    (\"Volcano\", \"Pre-Event\", [\"Magnetometers\", \"Seismic\", \"Camera\"]),\n",
        "    (\"Volcano\", \"During\", [\"Thermal Camera\", \"Camera\", \"LiDAR\"]),\n",
        "    (\"Volcano\", \"Post-Event\", [\"LiDAR\", \"Camera\", \"Seismic\"]),\n",
        "    (\"Volcano\", \"Clean-Up\", [\"LiDAR\", \"Camera\", \"Seismic\"]),\n",
        "    (\"Earthquake\", \"Pre-Event\", [\"Seismic\", \"Magnetometers\", \"Camera\"]),\n",
        "    (\"Earthquake\", \"During\", [\"Seismic\", \"Camera\", \"LiDAR\"]),\n",
        "    (\"Earthquake\", \"Post-Event\", [\"LiDAR\", \"Camera\", \"Seismic\"]),\n",
        "    (\"Fault\", \"Pre-Event\", [\"Seismic\", \"Magnetometers\", \"Camera\"]),\n",
        "    (\"Fault\", \"Post-Event\", [\"Seismic\", \"Camera\", \"LiDAR\"]),\n",
        "    (\"Landslide\", \"Pre-Event\", [\"LiDAR\", \"GPR\", \"Camera\"]),\n",
        "    (\"Landslide\", \"During\", [\"Camera\", \"Thermal Camera\", \"LiDAR\"]),\n",
        "    (\"Landslide\", \"Post-Event\", [\"LiDAR\", \"Seismic\", \"Camera\"]),\n",
        "    (\"Landslide\", \"Clean-Up\", [\"Camera\", \"LiDAR\", \"Seismic\"]),\n",
        "    (\"Tsunami\", \"During\", [\"BPR\", \"Camera\", \"Seismic\"]),\n",
        "    (\"Tsunami\", \"Post-Event\", [\"BPR\", \"Camera\", \"LiDAR\"]),\n",
        "    (\"Tsunami\", \"Clean-Up\", [\"Camera\", \"LiDAR\", \"Thermal Camera\"]),\n",
        "    (\"Nuclear\", \"Pre-Event\", [\"Thermal Camera\", \"Camera\", \"LiDAR\"]),\n",
        "    (\"Nuclear\", \"During\", [\"Thermal Camera\", \"Camera\", \"LiDAR\"]),\n",
        "    (\"Nuclear\", \"Post-Event\", [\"Camera\", \"LiDAR\", \"Gamma Spectrometer\"]),\n",
        "    (\"Nuclear\", \"Clean-Up\", [\"Camera\", \"LiDAR\", \"Gamma Spectrometer\"]),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcnBg-e6_UXd"
      },
      "outputs": [],
      "source": [
        "disaster_df = pd.DataFrame(disaster_map, columns=[\"HazardType\", \"DisasterPhase\", \"RecommendedSensors\"])\n",
        "\n",
        "# Step 2: Merge every HazardID with its corresponding disaster phases\n",
        "ghz_core = ghz_updated[[\"HazardID\", \"HazardType\", \"latitude\", \"longitude\"]]  # minimal fields to avoid memory issue\n",
        "ghz_expanded = ghz_core.merge(disaster_df, on=\"HazardType\", how=\"left\")\n",
        "\n",
        "# Step 3: Explode to get 1 row per recommended sensor\n",
        "ghz_expanded = ghz_expanded.explode(\"RecommendedSensors\").rename(columns={\"RecommendedSensors\": \"RecommendedSensor\"})\n",
        "ghz_expanded.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKEY2KhI_UXe"
      },
      "source": [
        "# STAGE 3: Assign Sensor Weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wuFrzuS_UXe"
      },
      "outputs": [],
      "source": [
        "# Clean sensor name just in case (strip whitespace)\n",
        "sensor_df[\"sensor_name_clean\"] = sensor_df[\"sensor_name\"].str.strip()\n",
        "ghz_expanded[\"RecommendedSensor_clean\"] = ghz_expanded[\"RecommendedSensor\"].str.strip()\n",
        "\n",
        "# Merge and keep only relevant columns\n",
        "merged_df = ghz_expanded.merge(\n",
        "    sensor_df[[\"sensor_name_clean\", \"sensor_weight\", \"model\"]],\n",
        "    left_on=\"RecommendedSensor_clean\",\n",
        "    right_on=\"sensor_name_clean\",\n",
        "    how=\"left\"\n",
        ").drop(columns=[\"sensor_name_clean\", \"RecommendedSensor_clean\"])\n",
        "\n",
        "# Rename model to sensor_model\n",
        "merged_df = merged_df.rename(columns={\"model\": \"sensor_model\"})\n",
        "merged_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2wzjFn-_UXf"
      },
      "outputs": [],
      "source": [
        "merged_df.RecommendedSensor.isna().unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBL08klH_UXf"
      },
      "outputs": [],
      "source": [
        "merged_df = merged_df.merge(\n",
        "    ghz_updated[[\"HazardID\", \"distance\"]],\n",
        "    on=\"HazardID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "merged_df[\"distance\"] = pd.to_numeric(merged_df[\"distance\"], errors=\"coerce\")\n",
        "merged_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW-CSkZy_UXf"
      },
      "outputs": [],
      "source": [
        "sample = merged_df.sample(100000, random_state=42)\n",
        "print(sample[[\"RecommendedSensor\", \"sensor_weight\", \"distance\", \"mission_distance\"]])\n",
        "sample.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSEiH482_UXg"
      },
      "source": [
        "# Calculate Mission Distance (Mapping or Delivery)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNOPOzJQ_UXg"
      },
      "outputs": [],
      "source": [
        "# Area definitions (in meters)\n",
        "sensor_default_area = {\n",
        "    \"Seismic\": (None, None),\n",
        "    \"Magnetometers\": (500, 200),\n",
        "    \"Lidar\": (400, 400),\n",
        "    \"GPR\": (None, None),\n",
        "    \"Camera\": (300, 300),\n",
        "    \"Thermal_Camera\": (300, 300),\n",
        "    \"Hyperspectral\": (1000, 200),\n",
        "    \"Multispectral\": (1000, 200),\n",
        "    \"EM\": (400, 400),\n",
        "    \"Gravimeter\": (500, 500),\n",
        "    \"BPR\": (None, None),\n",
        "    \"Gamma Spectrometer\": (500, 500)  # treated as mapping\n",
        "}\n",
        "\n",
        "sensor_spacing = {\n",
        "    \"Magnetometers\": 5,\n",
        "    \"Lidar\": 10,\n",
        "    \"EM\": 10,\n",
        "    \"Gravimeter\": 10,\n",
        "    \"Hyperspectral\": 20,\n",
        "    \"Multispectral\": 20,\n",
        "    \"Camera\": 20,\n",
        "    \"Thermal_Camera\": 20,\n",
        "    \"Gamma Spectrometer\": 20\n",
        "}\n",
        "\n",
        "direct_delivery_sensors = {\"Seismic\", \"GPR\", \"BPR\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffIF3FuU_UXh"
      },
      "outputs": [],
      "source": [
        "def calculate_mission_distance_vectorized(row):\n",
        "    sensor = row[\"RecommendedSensor\"]\n",
        "    try:\n",
        "        hazard_dist = float(row[\"distance\"])\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "    # Direct delivery → use hazard distance directly\n",
        "    if sensor in direct_delivery_sensors:\n",
        "        return hazard_dist\n",
        "\n",
        "    # Area-based mapping\n",
        "    area = sensor_default_area.get(sensor, (None, None))\n",
        "    spacing = sensor_spacing.get(sensor, 10)\n",
        "\n",
        "    if area[0] is not None and area[1] is not None:\n",
        "        area_length, area_width = area\n",
        "        num_lines = math.ceil(area_width / spacing)\n",
        "        return num_lines * area_length\n",
        "    else:\n",
        "        # If no area defined, fallback to estimate\n",
        "        if sensor == \"Magnetometers\":\n",
        "            return 20000\n",
        "        elif sensor == \"Lidar\":\n",
        "            return math.pi * hazard_dist\n",
        "        else:\n",
        "            return hazard_dist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPpjK5io_UXh"
      },
      "outputs": [],
      "source": [
        "def batch_calculate_mission_distance(ghz_df, chunk_size=50000):\n",
        "    # Defaults\n",
        "    sensor_default_area = {\n",
        "        \"Seismic\": (None, None),\n",
        "        \"Magnetometers\": (500, 200),\n",
        "        \"Lidar\": (400, 400),\n",
        "        \"GPR\": (None, None),\n",
        "        \"Camera\": (300, 300),\n",
        "        \"Thermal_Camera\": (300, 300),\n",
        "        \"Hyperspectral\": (1000, 200),\n",
        "        \"Multispectral\": (1000, 200),\n",
        "        \"EM\": (400, 400),\n",
        "        \"Gravimeter\": (500, 500),\n",
        "        \"Gamma Spectrometer\": (500, 500),\n",
        "        \"BPR\": (None, None)\n",
        "    }\n",
        "\n",
        "    sensor_spacing = {\n",
        "        \"Magnetometers\": 5,\n",
        "        \"Lidar\": 10,\n",
        "        \"EM\": 10,\n",
        "        \"Gravimeter\": 10,\n",
        "        \"Hyperspectral\": 20,\n",
        "        \"Multispectral\": 20,\n",
        "        \"Camera\": 20,\n",
        "        \"Thermal_Camera\": 20,\n",
        "        \"Gamma Spectrometer\": 20\n",
        "    }\n",
        "\n",
        "    direct_delivery_sensors = {\"Seismic\", \"GPR\", \"BPR\"}\n",
        "\n",
        "    def get_mission_distance(sensor, hazard_dist):\n",
        "        try:\n",
        "            hazard_dist = float(hazard_dist)\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "        if sensor in direct_delivery_sensors:\n",
        "            return hazard_dist\n",
        "\n",
        "        area = sensor_default_area.get(sensor, (None, None))\n",
        "        spacing = sensor_spacing.get(sensor, 10)\n",
        "\n",
        "        if area[0] and area[1]:\n",
        "            area_length, area_width = area\n",
        "            num_lines = math.ceil(area_width / spacing)\n",
        "            return num_lines * area_length\n",
        "        elif sensor == \"Magnetometers\":\n",
        "            return 20000\n",
        "        elif sensor == \"Lidar\":\n",
        "            return math.pi * hazard_dist\n",
        "        else:\n",
        "            return hazard_dist\n",
        "\n",
        "    # Chunked application\n",
        "    results = []\n",
        "    for start in range(0, len(ghz_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(ghz_df))\n",
        "        chunk = ghz_df.iloc[start:end].copy()\n",
        "        print(f\"\\U0001f373 Calculating mission_distance for chunk {start} to {end}...\")\n",
        "\n",
        "        chunk[\"mission_distance\"] = chunk.apply(\n",
        "            lambda row: get_mission_distance(row[\"RecommendedSensor\"], row[\"distance\"]), axis=1\n",
        "        )\n",
        "        results.append(chunk)\n",
        "\n",
        "    return pd.concat(results, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4QLE6Vn_UXi"
      },
      "source": [
        "## Mission Distance for CPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92qqCuTN_UXi"
      },
      "outputs": [],
      "source": [
        "def batch_calculate_mission_distance(ghz_df, chunk_size=50000):\n",
        "    # Defaults\n",
        "    sensor_default_area = {\n",
        "        \"Seismic\": (None, None),\n",
        "        \"Magnetometers\": (500, 200),\n",
        "        \"Lidar\": (400, 400),\n",
        "        \"GPR\": (None, None),\n",
        "        \"Camera\": (300, 300),\n",
        "        \"Thermal_Camera\": (300, 300),\n",
        "        \"Hyperspectral\": (1000, 200),\n",
        "        \"Multispectral\": (1000, 200),\n",
        "        \"EM\": (400, 400),\n",
        "        \"Gravimeter\": (500, 500),\n",
        "        \"Gamma Spectrometer\": (500, 500),\n",
        "        \"BPR\": (None, None)\n",
        "    }\n",
        "\n",
        "    sensor_spacing = {\n",
        "        \"Magnetometers\": 5,\n",
        "        \"Lidar\": 10,\n",
        "        \"EM\": 10,\n",
        "        \"Gravimeter\": 10,\n",
        "        \"Hyperspectral\": 20,\n",
        "        \"Multispectral\": 20,\n",
        "        \"Camera\": 20,\n",
        "        \"Thermal_Camera\": 20,\n",
        "        \"Gamma Spectrometer\": 20\n",
        "    }\n",
        "\n",
        "    direct_delivery_sensors = {\"Seismic\", \"GPR\", \"BPR\"}\n",
        "\n",
        "    def get_mission_distance(sensor, hazard_dist):\n",
        "        # Replace NaN with 1000 for safe fallback\n",
        "        try:\n",
        "            hazard_dist = float(hazard_dist)\n",
        "            if np.isnan(hazard_dist):\n",
        "                hazard_dist = 1000 # Fallback when it's MOR environment, assume use boat and 1000 is safe distance\n",
        "        except:\n",
        "            hazard_dist = 1000\n",
        "\n",
        "        if sensor in direct_delivery_sensors:\n",
        "            return hazard_dist\n",
        "\n",
        "        area = sensor_default_area.get(sensor, (None, None))\n",
        "        spacing = sensor_spacing.get(sensor, 10)\n",
        "\n",
        "        if area[0] and area[1]:\n",
        "            area_length, area_width = area\n",
        "            num_lines = math.ceil(area_width / spacing)\n",
        "            return num_lines * area_length\n",
        "        elif sensor == \"Magnetometers\":\n",
        "            return 20000\n",
        "        elif sensor == \"Lidar\":\n",
        "            return math.pi * hazard_dist\n",
        "        else:\n",
        "            return hazard_dist\n",
        "\n",
        "    # Chunked application\n",
        "    results = []\n",
        "    for start in range(0, len(ghz_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(ghz_df))\n",
        "        chunk = ghz_df.iloc[start:end].copy()\n",
        "        print(f\"\\U0001f373 Calculating mission_distance for chunk {start} to {end}...\")\n",
        "\n",
        "        chunk[\"mission_distance\"] = chunk.apply(\n",
        "            lambda row: get_mission_distance(row[\"RecommendedSensor\"], row[\"distance\"]), axis=1\n",
        "        )\n",
        "        results.append(chunk)\n",
        "\n",
        "    return pd.concat(results, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "pR-9h1rX_UXj"
      },
      "outputs": [],
      "source": [
        "merged_df = batch_calculate_mission_distance(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KGSMUgc_UXj"
      },
      "outputs": [],
      "source": [
        "print(max(drone_df['comm_range']))\n",
        "print(max(merged_df['distance']))\n",
        "print(max(merged_df.mission_distance.isna().unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAMZS6Wp_UXk"
      },
      "source": [
        "----\n",
        "# CPM\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUVjsT37_UXk"
      },
      "outputs": [],
      "source": [
        "def compute_cpm(row):\n",
        "    distance = row[\"distance\"]  # meters\n",
        "    mission_distance = row[\"mission_distance\"]  # meters\n",
        "    sensor = row[\"RecommendedSensor\"]\n",
        "\n",
        "    # Step 1: Travel Time\n",
        "    cruise_speed = 16  # m/s, average multicopter cruise speed\n",
        "    travel_time = distance / cruise_speed / 60  # in minutes\n",
        "\n",
        "    # Step 2: Monitor Time\n",
        "    if sensor in [\"Camera\", \"LiDAR\"]:\n",
        "        mapping_speed = 5  # m/s\n",
        "        monitor_time = mission_distance / mapping_speed / 60\n",
        "    elif sensor in [\"GPR\", \"Thermal Camera\"]:\n",
        "        mapping_speed = 3  # m/s\n",
        "        monitor_time = mission_distance / mapping_speed / 60\n",
        "    elif sensor == \"Magnetometer\":\n",
        "        mapping_speed = 10  # m/s\n",
        "        monitor_time = mission_distance / mapping_speed / 60\n",
        "    elif sensor in [\"Seismic\", \"BPR\"]:\n",
        "        monitor_time = 3  # fixed minutes\n",
        "    else:\n",
        "        monitor_time = 5  # fallback default\n",
        "\n",
        "    # Step 3: Setup Buffer\n",
        "    if sensor in [\"Camera\", \"LiDAR\", \"GPR\", \"Thermal Camera\", \"Magnetometer\"]:\n",
        "        setup_buffer = 40  # mapping mission: GCPs, calibration\n",
        "    elif mission_distance == distance:\n",
        "        setup_buffer = 20  # delivery-style: point drop/pick\n",
        "    else:\n",
        "        setup_buffer = 30  # fallback intermediate buffer\n",
        "\n",
        "    # Final CPM Total Time\n",
        "    cpm_total_time = travel_time + monitor_time + setup_buffer\n",
        "\n",
        "    return pd.Series({\n",
        "        \"travel_time\": travel_time,\n",
        "        \"monitor_time\": monitor_time,\n",
        "        \"setup_buffer\": setup_buffer,\n",
        "        \"cpm_total_time\": cpm_total_time\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJTHhiF7_UXk"
      },
      "outputs": [],
      "source": [
        "def batch_compute_cpm(ghz_df, chunk_size=50000):\n",
        "    results = []\n",
        "\n",
        "    for start in range(0, len(ghz_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(ghz_df))\n",
        "        chunk = ghz_df.iloc[start:end].copy()\n",
        "        print(f\"\\u23F3 Computing CPM for chunk {start} to {end}...\")\n",
        "\n",
        "        cpm_chunk = chunk.apply(compute_cpm, axis=1)\n",
        "        combined = pd.concat([chunk, cpm_chunk], axis=1)\n",
        "        results.append(combined)\n",
        "\n",
        "    return pd.concat(results, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-BRaWvmK_UXn"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "\n",
        "ghz_with_cpm = batch_compute_cpm(merged_df)\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpOkw3PW_UXo"
      },
      "source": [
        "# Assign Drone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sllAjeuh_UXo"
      },
      "outputs": [],
      "source": [
        "def batch_assign_best_drones(merged_df, drone_df, chunk_size=50000):\n",
        "    import numpy as np\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Only keep necessary drone fields and drop NaNs\n",
        "    drone_df = drone_df[[\"mfc_model\", \"max_payload_weight\", \"distance_range\", \"comm_range\"]].dropna()\n",
        "\n",
        "    for start in range(0, len(merged_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(merged_df))\n",
        "        chunk = merged_df.iloc[start:end].copy()\n",
        "        print(f\"\\U0001f373 Assigning drones for chunk {start} to {end}...\")\n",
        "\n",
        "        # Make sure fields are numeric\n",
        "        chunk[\"distance\"] = pd.to_numeric(chunk[\"distance\"], errors=\"coerce\")\n",
        "        chunk[\"mission_distance\"] = pd.to_numeric(chunk[\"mission_distance\"], errors=\"coerce\")\n",
        "        chunk[\"sensor_weight\"] = pd.to_numeric(chunk[\"sensor_weight\"], errors=\"coerce\")\n",
        "\n",
        "        # Prepare result columns\n",
        "        drone_cols = [\"drone1\", \"drone2\", \"drone3\"]\n",
        "        note_cols = [\"note1\", \"note2\", \"note3\"]\n",
        "        for col in drone_cols + note_cols:\n",
        "            chunk[col] = None\n",
        "\n",
        "        for i, row in chunk.iterrows():\n",
        "            s_weight = row[\"sensor_weight\"]\n",
        "            h_dist = row[\"distance\"]\n",
        "            m_dist = row[\"mission_distance\"]\n",
        "            sensor = row[\"RecommendedSensor\"]\n",
        "\n",
        "            # Handle NaN geohazard distance → treat as offshore (>500 km)\n",
        "            if np.isnan(h_dist):\n",
        "                h_dist = 999999\n",
        "\n",
        "            if np.isnan(m_dist):\n",
        "                continue  # mission distance must be defined\n",
        "\n",
        "            # Determine candidates\n",
        "            if sensor == \"Camera\":\n",
        "                candidates = drone_df.copy()  # all drones allowed\n",
        "            elif np.isnan(s_weight):\n",
        "                # fallback: assume heavy payload, use top 25% heavy drones\n",
        "                min_required = drone_df[\"max_payload_weight\"].quantile(0.75)\n",
        "                candidates = drone_df[drone_df[\"max_payload_weight\"] >= min_required].copy()\n",
        "            else:\n",
        "                candidates = drone_df[drone_df[\"max_payload_weight\"] >= s_weight].copy()\n",
        "\n",
        "            if candidates.empty:\n",
        "                continue\n",
        "\n",
        "            # Evaluate suitability\n",
        "            def eval_row(dr):\n",
        "                note = \"\"\n",
        "                tier = 1e6\n",
        "\n",
        "                # Always check comm range (for all missions)\n",
        "                if dr[\"comm_range\"] < h_dist:\n",
        "                    note = \"Comm range insufficient\"\n",
        "\n",
        "                if dr[\"distance_range\"] >= m_dist:\n",
        "                    tier = 1\n",
        "                    note = \"Full coverage\"\n",
        "                elif dr[\"distance_range\"] >= 0.5 * m_dist:\n",
        "                    tier = 2\n",
        "                    note = \"Needs 2–3 flights\"\n",
        "                elif dr[\"distance_range\"] >= 0.25 * m_dist:\n",
        "                    tier = 3\n",
        "                    note = \"Multiple sweeps\"\n",
        "                else:\n",
        "                    tier = 4\n",
        "                    note = \"Limited coverage\"\n",
        "\n",
        "                # Fallback for long-range/offshore\n",
        "                if h_dist > 500000 and tier == 1e6:\n",
        "                    tier = 5\n",
        "                    note = \"Long-range mission — consider boat or relay\"\n",
        "\n",
        "                return pd.Series([tier, note])\n",
        "\n",
        "            candidates[[\"tier\", \"note\"]] = candidates.apply(eval_row, axis=1)\n",
        "            candidates = candidates[candidates[\"tier\"] < 6].sort_values(\"tier\").head(3)\n",
        "\n",
        "            for j, (_, drone_row) in enumerate(candidates.iterrows()):\n",
        "                chunk.at[i, drone_cols[j]] = drone_row[\"mfc_model\"]\n",
        "                chunk.at[i, note_cols[j]] = drone_row[\"note\"]\n",
        "\n",
        "        results.append(chunk)\n",
        "\n",
        "    return pd.concat(results, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZ4DxCVQ_UXp"
      },
      "outputs": [],
      "source": [
        "def heartbeat():\n",
        "    while True:\n",
        "        print(\"...still running...\")\n",
        "        time.sleep(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9oksFNd_UXp"
      },
      "outputs": [],
      "source": [
        "threading.Thread(target=heartbeat, daemon=True).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "B1cov7JW_UXq"
      },
      "outputs": [],
      "source": [
        "start_time = timeit.default_timer()\n",
        "\n",
        "final_df = batch_assign_best_drones(merged_df, drone_df, chunk_size=50000)\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adnCp_a0_UXq"
      },
      "outputs": [],
      "source": [
        "# Define the heartbeat thread function\n",
        "def heartbeat(interval=3600):\n",
        "    while True:\n",
        "        print(f\"⏳ Heartbeat: still running at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        time.sleep(interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEVrMyUW_UXq"
      },
      "outputs": [],
      "source": [
        "# Start the background heartbeat thread\n",
        "threading.Thread(target=heartbeat, args=(3600,), daemon=True).start()\n",
        "\n",
        "# Run the main process with timer\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "assign_top3_drones_grouped_fixed(merged_df, drone_df, save_dir=\"D:/NDIS_Database/19_PostProcessing\")\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s hour(s)\"%str(elapsed/60/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coG0xpUo_UXr"
      },
      "outputs": [],
      "source": [
        "def batch_assign_best_drones(merged_df, drone_df, chunk_size=50000, save_dir=\"D:/NDIS_Database/19_PostProcessing\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    drone_df = drone_df[[\"mfc_model\", \"max_payload_weight\", \"distance_range\", \"comm_range\"]].dropna()\n",
        "    all_results = []\n",
        "\n",
        "    for start in range(0, len(merged_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(merged_df))\n",
        "        chunk = merged_df.iloc[start:end].copy()\n",
        "        print(f\"\\U0001f373 Assigning drones for chunk {start} to {end}...\")\n",
        "\n",
        "        chunk[\"distance\"] = pd.to_numeric(chunk[\"distance\"], errors=\"coerce\")\n",
        "        chunk[\"mission_distance\"] = pd.to_numeric(chunk[\"mission_distance\"], errors=\"coerce\")\n",
        "        chunk[\"sensor_weight\"] = pd.to_numeric(chunk[\"sensor_weight\"], errors=\"coerce\")\n",
        "\n",
        "        drone_cols = [\"drone1\", \"drone2\", \"drone3\"]\n",
        "        note_cols = [\"note1\", \"note2\", \"note3\"]\n",
        "        for col in drone_cols + note_cols:\n",
        "            chunk[col] = None\n",
        "\n",
        "        for i, row in chunk.iterrows():\n",
        "            s_weight = row[\"sensor_weight\"]\n",
        "            h_dist = row[\"distance\"]\n",
        "            m_dist = row[\"mission_distance\"]\n",
        "            sensor = row[\"RecommendedSensor\"]\n",
        "\n",
        "            if np.isnan(h_dist):\n",
        "                h_dist = 999999\n",
        "            if np.isnan(m_dist):\n",
        "                continue\n",
        "\n",
        "            if sensor == \"Camera\":\n",
        "                candidates = drone_df.copy()\n",
        "            elif np.isnan(s_weight):\n",
        "                min_required = drone_df[\"max_payload_weight\"].quantile(0.75)\n",
        "                candidates = drone_df[drone_df[\"max_payload_weight\"] >= min_required].copy()\n",
        "            else:\n",
        "                candidates = drone_df[drone_df[\"max_payload_weight\"] >= s_weight].copy()\n",
        "\n",
        "            if candidates.empty:\n",
        "                continue\n",
        "\n",
        "            def eval_row(dr):\n",
        "                note = \"\"\n",
        "                tier = 1e6\n",
        "                if dr[\"comm_range\"] < h_dist:\n",
        "                    note = \"Comm range insufficient\"\n",
        "                if dr[\"distance_range\"] >= m_dist:\n",
        "                    tier = 1\n",
        "                    note = \"Full coverage\"\n",
        "                elif dr[\"distance_range\"] >= 0.5 * m_dist:\n",
        "                    tier = 2\n",
        "                    note = \"Needs 2–3 flights\"\n",
        "                elif dr[\"distance_range\"] >= 0.25 * m_dist:\n",
        "                    tier = 3\n",
        "                    note = \"Multiple sweeps\"\n",
        "                else:\n",
        "                    tier = 4\n",
        "                    note = \"Limited coverage\"\n",
        "                if h_dist > 500000 and tier == 1e6:\n",
        "                    tier = 5\n",
        "                    note = \"Long-range mission — consider boat or relay\"\n",
        "                return pd.Series([tier, note])\n",
        "\n",
        "            candidates[[\"tier\", \"note\"]] = candidates.apply(eval_row, axis=1)\n",
        "            candidates = candidates[candidates[\"tier\"] < 6].sort_values(\"tier\").head(3)\n",
        "\n",
        "            for j, (_, drone_row) in enumerate(candidates.iterrows()):\n",
        "                chunk.at[i, drone_cols[j]] = drone_row[\"mfc_model\"]\n",
        "                chunk.at[i, note_cols[j]] = drone_row[\"note\"]\n",
        "\n",
        "        # Save intermediate result\n",
        "        out_path = os.path.join(save_dir, f\"drones_chunk_{start}_{end}.feather\")\n",
        "        chunk.reset_index(drop=True).to_feather(out_path)\n",
        "        print(f\"\\u2705 Saved chunk {start} to {end} → {out_path}\")\n",
        "\n",
        "        all_results.append(chunk)\n",
        "        gc.collect()\n",
        "\n",
        "    return pd.concat(all_results, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SKZN85k1_UXr"
      },
      "outputs": [],
      "source": [
        "# Start the background heartbeat thread\n",
        "threading.Thread(target=heartbeat, args=(3600,), daemon=True).start()\n",
        "\n",
        "# Run the main process with timer\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "final_df = batch_assign_best_drones(merged_df, drone_df, chunk_size=50000)\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bjLwh0s_UXs"
      },
      "outputs": [],
      "source": [
        "# Define the directory where feather files are saved\n",
        "feather_dir = \"D:/NDIS_Database/19_PostProcessing\"\n",
        "merged_output_path = os.path.join(feather_dir, \"drones_full_result.feather\")\n",
        "\n",
        "# List all feather files in the directory (make sure they're in correct order)\n",
        "feather_files = sorted([f for f in os.listdir(feather_dir) if f.endswith(\".feather\") and f.startswith(\"drones_chunk_\")])\n",
        "\n",
        "# Load and concatenate all feather chunks\n",
        "chunks = [pd.read_feather(os.path.join(feather_dir, f)) for f in feather_files]\n",
        "final_df = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "# Save the final merged file\n",
        "final_df.to_feather(merged_output_path)\n",
        "print(f\"🥚 Merged {len(chunks)} chunks and saved to → {merged_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-xW7UTp_UXt"
      },
      "outputs": [],
      "source": [
        "print(f\"Sensor weights: {final_df.sensor_weight.unique()}\")\n",
        "print(f\"RecommendedSensors: {final_df.RecommendedSensor.unique()}\")\n",
        "print(f\"disaster phases: {final_df.DisasterPhase.unique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLmH7IHO_UXt"
      },
      "outputs": [],
      "source": [
        "final_df.mission_distance.isna().unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_eskED8_UXt"
      },
      "outputs": [],
      "source": [
        "final_df.drone3.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBrh6Tf4_UXu"
      },
      "outputs": [],
      "source": [
        "final_df.drone1.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXuBVsws_UXu"
      },
      "outputs": [],
      "source": [
        "final_df.drone2.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hAOWjRT_UXv"
      },
      "source": [
        "-----\n",
        "3.8.1\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsaQpTm__UXv"
      },
      "outputs": [],
      "source": [
        "# Load feather file\n",
        "ghz_with_cpm = pd.read_feather(r\"D:\\NDIS_Database\\20_PaperSimulation\\ghz_with_cpm_v1.feather\")\n",
        "ghz_with_cpm.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Uuv-Hul_UXv"
      },
      "outputs": [],
      "source": [
        "def batch_assign_best_drones(merged_df, drone_df, chunk_size=50000, save_dir=\"D:/NDIS_Database/19_PostProcessing\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    drone_df = drone_df[[\"mfc_model\", \"max_payload_weight\", \"distance_range\", \"comm_range\"]].dropna()\n",
        "    all_results = []\n",
        "\n",
        "    for start in range(0, len(merged_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(merged_df))\n",
        "        chunk = merged_df.iloc[start:end].copy()\n",
        "        print(f\"\\U0001f373 Assigning drones for chunk {start} to {end}...\")\n",
        "\n",
        "        chunk[\"distance\"] = pd.to_numeric(chunk[\"distance\"], errors=\"coerce\")\n",
        "        chunk[\"mission_distance\"] = pd.to_numeric(chunk[\"mission_distance\"], errors=\"coerce\")\n",
        "        chunk[\"sensor_weight\"] = pd.to_numeric(chunk[\"sensor_weight\"], errors=\"coerce\")\n",
        "\n",
        "        drone_cols = [\"drone1\", \"drone2\", \"drone3\"]\n",
        "        note_cols = [\"note1\", \"note2\", \"note3\"]\n",
        "        for col in drone_cols + note_cols:\n",
        "            chunk[col] = None\n",
        "\n",
        "        for i, row in chunk.iterrows():\n",
        "            s_weight = row[\"sensor_weight\"]\n",
        "            h_dist = row[\"distance\"]\n",
        "            m_dist = row[\"mission_distance\"]\n",
        "            sensor = row[\"RecommendedSensor\"]\n",
        "\n",
        "            try:\n",
        "                h_dist = float(h_dist)\n",
        "            except:\n",
        "                h_dist = np.nan\n",
        "            try:\n",
        "                m_dist = float(m_dist)\n",
        "            except:\n",
        "                m_dist = np.nan\n",
        "\n",
        "            if sensor == \"Camera\":\n",
        "                candidates = drone_df.copy()\n",
        "            elif pd.isna(s_weight):\n",
        "                min_required = drone_df[\"max_payload_weight\"].quantile(0.75)\n",
        "                candidates = drone_df[drone_df[\"max_payload_weight\"] >= min_required].copy()\n",
        "            else:\n",
        "                candidates = drone_df[drone_df[\"max_payload_weight\"] >= s_weight].copy()\n",
        "\n",
        "            if candidates.empty:\n",
        "                continue\n",
        "\n",
        "            def eval_row(dr):\n",
        "                note = \"\"\n",
        "                tier = 6\n",
        "                if not pd.isna(h_dist) and dr[\"comm_range\"] < h_dist:\n",
        "                    note = \"Comm range insufficient\"\n",
        "                if not pd.isna(m_dist):\n",
        "                    if dr[\"distance_range\"] >= m_dist:\n",
        "                        tier = 1\n",
        "                        note = \"Full coverage\"\n",
        "                    elif dr[\"distance_range\"] >= 0.5 * m_dist:\n",
        "                        tier = 2\n",
        "                        note = \"Needs 2–3 flights\"\n",
        "                    elif dr[\"distance_range\"] >= 0.25 * m_dist:\n",
        "                        tier = 3\n",
        "                        note = \"Multiple sweeps\"\n",
        "                    elif h_dist > 300000:\n",
        "                        tier = 4\n",
        "                        note = \"Fallback: long range mission, relay/boat likely\"\n",
        "                else:\n",
        "                    tier = 5\n",
        "                    note = \"Fallback: distance unknown, payload match only\"\n",
        "                return pd.Series([tier, note])\n",
        "\n",
        "            candidates[[\"tier\", \"note\"]] = candidates.apply(eval_row, axis=1)\n",
        "            candidates = candidates[candidates[\"tier\"] < 6].sort_values(by=[\"tier\", \"max_payload_weight\"])\n",
        "\n",
        "            # If no match at all, force fallback by payload match only\n",
        "            if candidates.empty and not pd.isna(s_weight):\n",
        "                fallback = drone_df[drone_df[\"max_payload_weight\"] >= s_weight].copy()\n",
        "                fallback[\"note\"] = \"Force-deployed; no range match\"\n",
        "                fallback[\"tier\"] = 5\n",
        "                candidates = fallback.sort_values(\"max_payload_weight\").head(3)\n",
        "\n",
        "            # Get top 3 distinct drone models\n",
        "            selected = []\n",
        "            used_models = set()\n",
        "            for _, drone_row in candidates.iterrows():\n",
        "                model = drone_row[\"mfc_model\"]\n",
        "                if model not in used_models:\n",
        "                    selected.append((model, drone_row[\"note\"]))\n",
        "                    used_models.add(model)\n",
        "                if len(selected) == 3:\n",
        "                    break\n",
        "\n",
        "            for j in range(len(selected)):\n",
        "                chunk.at[i, drone_cols[j]] = selected[j][0]\n",
        "                chunk.at[i, note_cols[j]] = selected[j][1]\n",
        "\n",
        "        # Save intermediate result\n",
        "        out_path = os.path.join(save_dir, f\"drones_chunk_{start}_{end}.feather\")\n",
        "        chunk.reset_index(drop=True).to_feather(out_path)\n",
        "        print(f\"\\u2705 Saved chunk {start} to {end} → {out_path}\")\n",
        "\n",
        "        all_results.append(chunk)\n",
        "        gc.collect()\n",
        "\n",
        "    return pd.concat(all_results, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETfkn_Ad_UXw"
      },
      "outputs": [],
      "source": [
        "sample = ghz_with_cpm.sample(50000, random_state=42)\n",
        "sample.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmJY38xt_UXw"
      },
      "outputs": [],
      "source": [
        "def batch_assign_best_drones_with_usage(merged_df, drone_df, chunk_size=50000, save_dir=r\"D:\\NDIS_Database\\20_PaperSimulation\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    drone_df = drone_df[[\"mfc_model\", \"max_payload_weight\", \"distance_range\", \"comm_range\"]].dropna()\n",
        "    drone_df = drone_df.drop_duplicates(\"mfc_model\").reset_index(drop=True)\n",
        "\n",
        "    drone_usage = defaultdict(int)  # Track how often each drone is used\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    for start in range(0, len(merged_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(merged_df))\n",
        "        chunk = merged_df.iloc[start:end].copy()\n",
        "        print(f\"🚁 Processing chunk {start} to {end}...\")\n",
        "\n",
        "        chunk[\"distance\"] = pd.to_numeric(chunk[\"distance\"], errors=\"coerce\")\n",
        "        chunk[\"mission_distance\"] = pd.to_numeric(chunk[\"mission_distance\"], errors=\"coerce\")\n",
        "        chunk[\"sensor_weight\"] = pd.to_numeric(chunk[\"sensor_weight\"], errors=\"coerce\")\n",
        "\n",
        "        for col in [\"drone1\", \"drone2\", \"drone3\", \"note1\", \"note2\", \"note3\"]:\n",
        "            chunk[col] = None\n",
        "\n",
        "        for i, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f\"Matching drones {start}-{end}\"):\n",
        "\n",
        "            s_weight = row[\"sensor_weight\"]\n",
        "            h_dist = row[\"distance\"]\n",
        "            m_dist = row[\"mission_distance\"]\n",
        "            sensor = row[\"RecommendedSensor\"]\n",
        "\n",
        "            if np.isnan(h_dist):\n",
        "                h_dist = 999999\n",
        "                ocean_flag = True\n",
        "            else:\n",
        "                ocean_flag = False\n",
        "\n",
        "            if np.isnan(m_dist):\n",
        "                m_dist = h_dist\n",
        "\n",
        "            if pd.isna(s_weight):\n",
        "                continue\n",
        "\n",
        "            # Step 1: Filter by payload\n",
        "            suitable_drones = drone_df[drone_df[\"max_payload_weight\"] >= s_weight].copy()\n",
        "            if suitable_drones.empty:\n",
        "                continue\n",
        "\n",
        "            # Step 2: Assign coverage tier\n",
        "            def score_drone(dr):\n",
        "                note = \"\"\n",
        "                tier = 5  # default: lowest tier\n",
        "                if dr[\"comm_range\"] < h_dist:\n",
        "                    note = \"Comm range insufficient\"\n",
        "                if dr[\"distance_range\"] >= m_dist:\n",
        "                    tier = 1\n",
        "                    note = \"Full coverage\"\n",
        "                elif dr[\"distance_range\"] >= 0.5 * m_dist:\n",
        "                    tier = 2\n",
        "                    note = \"2–3 swaths\"\n",
        "                elif dr[\"distance_range\"] >= 0.25 * m_dist:\n",
        "                    tier = 3\n",
        "                    note = \"Multiple flights\"\n",
        "                elif h_dist > 300000:\n",
        "                    tier = 4\n",
        "                    note = \"Fallback: Consider boat/relay\"\n",
        "                else:\n",
        "                    note = \"Fallback: Coverage limited\"\n",
        "                return pd.Series([tier, note])\n",
        "\n",
        "            suitable_drones[[\"tier\", \"note\"]] = suitable_drones.apply(score_drone, axis=1)\n",
        "\n",
        "            # Step 3: Compute usage priority\n",
        "            suitable_drones[\"usage_count\"] = suitable_drones[\"mfc_model\"].map(drone_usage)\n",
        "            suitable_drones[\"payload_gap\"] = suitable_drones[\"max_payload_weight\"] - s_weight\n",
        "            suitable_drones[\"payload_gap\"] = suitable_drones[\"payload_gap\"].apply(lambda x: x if x >= 0 else 9999)\n",
        "\n",
        "            # Step 4: Sort by tier, then usage count, then closest payload match\n",
        "            ranked = suitable_drones.sort_values(by=[\"tier\", \"usage_count\", \"payload_gap\"]).drop_duplicates(\"mfc_model\")\n",
        "\n",
        "            if ranked.empty:\n",
        "                continue\n",
        "\n",
        "            top3 = ranked.head(3)\n",
        "\n",
        "            for j, (_, drone_row) in enumerate(top3.iterrows()):\n",
        "                model = drone_row[\"mfc_model\"]\n",
        "                note = drone_row[\"note\"]\n",
        "                chunk.at[i, f\"drone{j+1}\"] = model\n",
        "                chunk.at[i, f\"note{j+1}\"] = note if not ocean_flag else \"Likely ocean — distance unknown\"\n",
        "                drone_usage[model] += 1\n",
        "\n",
        "        # Save\n",
        "        out_path = os.path.join(save_dir, f\"drones_chunk_{start}_{end}.feather\")\n",
        "        chunk.reset_index(drop=True).to_feather(out_path)\n",
        "        print(f\"✅ Saved: {out_path}\")\n",
        "        all_chunks.append(chunk)\n",
        "        gc.collect()\n",
        "\n",
        "    final_df = pd.concat(all_chunks, ignore_index=True)\n",
        "    return final_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEtDUnF7_UXw"
      },
      "outputs": [],
      "source": [
        "# Start the background heartbeat thread\n",
        "threading.Thread(target=heartbeat, args=(3600,), daemon=True).start()\n",
        "\n",
        "# Run the main process with timer\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "final_df = batch_assign_best_drones_with_usage(sample, drone_df, chunk_size=50000)\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJi1Mo3p_UXx"
      },
      "outputs": [],
      "source": [
        "len(final_df.drone1.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYq60n1L_UXx"
      },
      "outputs": [],
      "source": [
        "final_df.drone1.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2gojUd1_UXx"
      },
      "outputs": [],
      "source": [
        "len(final_df.drone2.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD9NY4Kd_UXy"
      },
      "outputs": [],
      "source": [
        "final_df.drone2.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_MsEP53_UXy"
      },
      "outputs": [],
      "source": [
        "len(final_df.drone3.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vok2O9_N_UXy"
      },
      "outputs": [],
      "source": [
        "final_df.drone3.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls1tjXvD_UXz"
      },
      "outputs": [],
      "source": [
        "# Get drones count\n",
        "drone_counts = final_df[[\"drone1\", \"drone2\", \"drone3\"]].stack().value_counts()\n",
        "\n",
        "# Display top 10 most used drones\n",
        "print(drone_counts.head(60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBRruwSU_UXz"
      },
      "outputs": [],
      "source": [
        "drone_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2sY4Gzl_UXz"
      },
      "source": [
        "-----\n",
        "# ver 3.8.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNmHwNNJ_UX0"
      },
      "outputs": [],
      "source": [
        "def assign_best_drones_batch(\n",
        "    merged_df,\n",
        "    drone_df,\n",
        "    chunk_size=50000,\n",
        "    save_dir=r\"D:\\NDIS_Database\\20_PaperSimulation\\FinalDroneAssignment\"\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Pre-clean drone_df\n",
        "    drone_df = drone_df.dropna(subset=[\"mfc_model\", \"max_payload_weight\"]).copy()\n",
        "    drone_df[\"drone_id\"] = drone_df[\"mfc_model\"]\n",
        "    drone_df[\"comm_range\"] = pd.to_numeric(drone_df[\"comm_range\"], errors=\"coerce\")\n",
        "    drone_df[\"distance_range\"] = pd.to_numeric(drone_df[\"distance_range\"], errors=\"coerce\")\n",
        "    drone_df[\"max_payload_weight\"] = pd.to_numeric(drone_df[\"max_payload_weight\"], errors=\"coerce\")\n",
        "\n",
        "    # Tracking usage\n",
        "    usage_count = {model: 0 for model in drone_df[\"drone_id\"].unique()}\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for start in range(0, len(merged_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(merged_df))\n",
        "        chunk = merged_df.iloc[start:end].copy()\n",
        "        print(f\"\\U0001f680 Processing chunk {start}-{end}\")\n",
        "\n",
        "        # Clean fields\n",
        "        chunk[\"distance\"] = pd.to_numeric(chunk[\"distance\"], errors=\"coerce\")\n",
        "        chunk[\"mission_distance\"] = pd.to_numeric(chunk[\"mission_distance\"], errors=\"coerce\")\n",
        "        chunk[\"sensor_weight\"] = pd.to_numeric(chunk[\"sensor_weight\"], errors=\"coerce\")\n",
        "\n",
        "        chunk[[\"drone1\", \"drone2\", \"drone3\"]] = None\n",
        "        chunk[[\"note1\", \"note2\", \"note3\"]] = None\n",
        "\n",
        "        for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
        "            s_weight = row[\"sensor_weight\"]\n",
        "            h_dist = row[\"distance\"]\n",
        "            m_dist = row[\"mission_distance\"]\n",
        "            sensor = row[\"RecommendedSensor\"]\n",
        "\n",
        "            if pd.isna(s_weight):\n",
        "                continue  # skip if no sensor\n",
        "\n",
        "            # Primary filter: payload\n",
        "            candidates = drone_df[drone_df[\"max_payload_weight\"] >= s_weight].copy()\n",
        "            if candidates.empty:\n",
        "                continue\n",
        "\n",
        "            # Score each candidate\n",
        "            def score_drone(dr):\n",
        "                score = 100\n",
        "                note = \"\"\n",
        "\n",
        "                if not pd.isna(h_dist):\n",
        "                    if dr[\"comm_range\"] < h_dist:\n",
        "                        score += 50\n",
        "                        note = \"Comm range insufficient\"\n",
        "\n",
        "                if not pd.isna(m_dist):\n",
        "                    if dr[\"distance_range\"] >= m_dist:\n",
        "                        score -= 5\n",
        "                        note = \"Full coverage\"\n",
        "                    elif dr[\"distance_range\"] >= 0.5 * m_dist:\n",
        "                        score += 5\n",
        "                        note = \"2–3 swaths needed\"\n",
        "                    elif dr[\"distance_range\"] >= 0.25 * m_dist:\n",
        "                        score += 10\n",
        "                        note = \"Multiple passes\"\n",
        "                    else:\n",
        "                        score += 30\n",
        "                        note = \"Very limited range\"\n",
        "                else:\n",
        "                    note = \"Distance unknown — likely offshore\"\n",
        "                    score += 20\n",
        "\n",
        "                # Prefer payloads close to sensor weight\n",
        "                payload_diff = dr[\"max_payload_weight\"] - s_weight\n",
        "                score += abs(payload_diff) * 0.05\n",
        "\n",
        "                # Soft randomness + usage count bias\n",
        "                score += usage_count.get(dr[\"drone_id\"], 0) * 0.01\n",
        "                score += random.uniform(-0.5, 0.5)\n",
        "\n",
        "                return pd.Series([score, note])\n",
        "\n",
        "            candidates[[\"score\", \"note\"]] = candidates.apply(score_drone, axis=1)\n",
        "            candidates = candidates.sort_values(\"score\").drop_duplicates(\"drone_id\").head(10)\n",
        "\n",
        "            # Select top 3 unique models\n",
        "            assigned = []\n",
        "            notes = []\n",
        "            for _, dr in candidates.iterrows():\n",
        "                if dr[\"drone_id\"] not in assigned:\n",
        "                    assigned.append(dr[\"drone_id\"])\n",
        "                    notes.append(dr[\"note\"])\n",
        "                if len(assigned) == 3:\n",
        "                    break\n",
        "\n",
        "            for i in range(len(assigned)):\n",
        "                chunk.at[idx, f\"drone{i+1}\"] = assigned[i]\n",
        "                chunk.at[idx, f\"note{i+1}\"] = notes[i]\n",
        "                usage_count[assigned[i]] += 1\n",
        "\n",
        "        # Save chunk\n",
        "        out_path = os.path.join(save_dir, f\"drone_assign_chunk_{start}_{end}.feather\")\n",
        "        chunk.reset_index(drop=True).to_feather(out_path)\n",
        "        all_results.append(chunk)\n",
        "        gc.collect()\n",
        "\n",
        "    return pd.concat(all_results, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If-gdBbp_UX0"
      },
      "outputs": [],
      "source": [
        "# Start the background heartbeat thread\n",
        "threading.Thread(target=heartbeat, args=(3600,), daemon=True).start()\n",
        "\n",
        "# Run the main process with timer\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "final_df = assign_best_drones_batch(sample, drone_df, chunk_size=50000)\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuIZC7OU_UX2"
      },
      "outputs": [],
      "source": [
        "# Melt drone1, drone2, drone3 into one column\n",
        "melted = final_df.melt(\n",
        "    id_vars=['HazardType'],\n",
        "    value_vars=['drone1', 'drone2', 'drone3'],\n",
        "    var_name='drone_rank',\n",
        "    value_name='drone_model'\n",
        ")\n",
        "\n",
        "# Clean strings (optional but recommended)\n",
        "melted['HazardType'] = melted['HazardType'].str.strip()\n",
        "melted['drone_model'] = melted['drone_model'].str.strip()\n",
        "\n",
        "# Group and count\n",
        "drone_hazard_counts = melted.groupby(['HazardType', 'drone_model']).size().reset_index(name='count')\n",
        "drone_hazard_counts = drone_hazard_counts.sort_values(by='count', ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.barplot(data=drone_hazard_counts, x='count', y='drone_model', hue='HazardType')\n",
        "\n",
        "# Log scale\n",
        "plt.xscale('log')\n",
        "\n",
        "# Labels and formatting\n",
        "plt.title('Drone Model Usage per Hazard Type (Log Scale)', fontsize=16)\n",
        "plt.xlabel('Count (log scale)', fontsize=14)\n",
        "plt.ylabel('Drone Model', fontsize=14)\n",
        "plt.legend(title='Hazard Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_mleopj_UX2"
      },
      "source": [
        "----\n",
        "# Drone Rotation included"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K4ePp60_UX2"
      },
      "outputs": [],
      "source": [
        "drone_df.rename(columns={\"model_name\": \"mfc_model\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM6VnJr5_UX3"
      },
      "outputs": [],
      "source": [
        "def assign_best_drones_with_blacklist_batch(\n",
        "    merged_df,\n",
        "    drone_df,\n",
        "    chunk_size=50000,\n",
        "    save_dir=r\"D:\\NDIS_Database\\FinalDroneAssignment\",\n",
        "    blacklist_threshold=20000\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Clean drone_df\n",
        "    drone_df = drone_df.dropna(subset=[\"mfc_model\", \"max_payload_weight\"]).copy()\n",
        "    drone_df[\"drone_id\"] = drone_df[\"mfc_model\"]\n",
        "    drone_df[\"comm_range\"] = pd.to_numeric(drone_df[\"comm_range\"], errors=\"coerce\")\n",
        "    drone_df[\"distance_range\"] = pd.to_numeric(drone_df[\"distance_range\"], errors=\"coerce\")\n",
        "    drone_df[\"max_payload_weight\"] = pd.to_numeric(drone_df[\"max_payload_weight\"], errors=\"coerce\")\n",
        "\n",
        "    usage_count = {model: 0 for model in drone_df[\"drone_id\"].unique()}\n",
        "    all_results = []\n",
        "\n",
        "    for start in range(0, len(merged_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(merged_df))\n",
        "        chunk = merged_df.iloc[start:end].copy()\n",
        "        print(f\"\\U0001f680 Processing chunk {start}-{end}\")\n",
        "\n",
        "        chunk[\"distance\"] = pd.to_numeric(chunk[\"distance\"], errors=\"coerce\")\n",
        "        chunk[\"mission_distance\"] = pd.to_numeric(chunk[\"mission_distance\"], errors=\"coerce\")\n",
        "        chunk[\"sensor_weight\"] = pd.to_numeric(chunk[\"sensor_weight\"], errors=\"coerce\")\n",
        "\n",
        "        chunk[[\"drone1\", \"drone2\", \"drone3\"]] = None\n",
        "        chunk[[\"note1\", \"note2\", \"note3\"]] = None\n",
        "\n",
        "        for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
        "            s_weight = row[\"sensor_weight\"]\n",
        "            h_dist = row[\"distance\"]\n",
        "            m_dist = row[\"mission_distance\"]\n",
        "            sensor = row[\"RecommendedSensor\"]\n",
        "\n",
        "            if pd.isna(s_weight):\n",
        "                continue\n",
        "\n",
        "            candidates = drone_df[drone_df[\"max_payload_weight\"] >= s_weight].copy()\n",
        "            if candidates.empty:\n",
        "                continue\n",
        "\n",
        "            def score_drone(dr):\n",
        "                score = 100\n",
        "                note = \"\"\n",
        "\n",
        "                if not pd.isna(h_dist):\n",
        "                    if dr[\"comm_range\"] < h_dist:\n",
        "                        score += 50\n",
        "                        note = \"Comm range insufficient\"\n",
        "\n",
        "                if not pd.isna(m_dist):\n",
        "                    if dr[\"distance_range\"] >= m_dist:\n",
        "                        score -= 5\n",
        "                        note = \"Full coverage\"\n",
        "                    elif dr[\"distance_range\"] >= 0.5 * m_dist:\n",
        "                        score += 5\n",
        "                        note = \"2–3 swaths needed\"\n",
        "                    elif dr[\"distance_range\"] >= 0.25 * m_dist:\n",
        "                        score += 10\n",
        "                        note = \"Multiple passes\"\n",
        "                    else:\n",
        "                        score += 30\n",
        "                        note = \"Very limited range\"\n",
        "                else:\n",
        "                    note = \"Distance unknown — likely offshore\"\n",
        "                    score += 20\n",
        "\n",
        "                # Payload proximity\n",
        "                payload_diff = dr[\"max_payload_weight\"] - s_weight\n",
        "                score += abs(payload_diff) * 0.05\n",
        "\n",
        "                # Usage penalty\n",
        "                uc = usage_count.get(dr[\"drone_id\"], 0)\n",
        "                if uc >= blacklist_threshold:\n",
        "                    score += 9999  # Hard exclude\n",
        "                else:\n",
        "                    score += uc * 0.01  # Soft bias\n",
        "\n",
        "                score += random.uniform(-0.5, 0.5)\n",
        "                return pd.Series([score, note])\n",
        "\n",
        "            candidates[[\"score\", \"note\"]] = candidates.apply(score_drone, axis=1)\n",
        "            candidates = candidates.sort_values(\"score\").drop_duplicates(\"drone_id\").head(10)\n",
        "\n",
        "            assigned = []\n",
        "            notes = []\n",
        "\n",
        "            for _, dr in candidates.iterrows():\n",
        "                if dr[\"drone_id\"] not in assigned:\n",
        "                    assigned.append(dr[\"drone_id\"])\n",
        "                    notes.append(dr[\"note\"])\n",
        "                if len(assigned) == 3:\n",
        "                    break\n",
        "\n",
        "            for i in range(len(assigned)):\n",
        "                chunk.at[idx, f\"drone{i+1}\"] = assigned[i]\n",
        "                chunk.at[idx, f\"note{i+1}\"] = notes[i]\n",
        "                usage_count[assigned[i]] += 1\n",
        "\n",
        "        # Save feather\n",
        "        out_path = os.path.join(save_dir, f\"drone_assign_chunk_{start}_{end}.feather\")\n",
        "        chunk.reset_index(drop=True).to_feather(out_path)\n",
        "        all_results.append(chunk)\n",
        "        gc.collect()\n",
        "\n",
        "    return pd.concat(all_results, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU1uWu1q_UX3"
      },
      "outputs": [],
      "source": [
        "def heartbeat(interval=3600):\n",
        "    while True:\n",
        "        tqdm.write(f\"⏳ Heartbeat: still running at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        time.sleep(interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92dtr18-_UX3"
      },
      "outputs": [],
      "source": [
        "# Start the background heartbeat thread\n",
        "threading.Thread(target=heartbeat, args=(3600,), daemon=True).start()\n",
        "\n",
        "# Run the main process with timer\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "final_df = assign_best_drones_with_blacklist_batch(sample, drone_df, chunk_size=50000)\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mEdjWb0_UX4"
      },
      "outputs": [],
      "source": [
        "# Get drone count\n",
        "drone_counts = final_df[[\"drone1\", \"drone2\", \"drone3\"]].stack().value_counts()\n",
        "\n",
        "# Display top 10 most used drones\n",
        "print(drone_counts.head(60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V-gc01z_UX4"
      },
      "outputs": [],
      "source": [
        "final_df.drone1.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI0rAvp1_UX4"
      },
      "outputs": [],
      "source": [
        "len(final_df.drone1.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAhLR8PQ_UX4"
      },
      "outputs": [],
      "source": [
        "final_df.drone2.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pNS4f6p_UX5"
      },
      "outputs": [],
      "source": [
        "len(final_df.drone2.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOQ9tTHt_UX5"
      },
      "outputs": [],
      "source": [
        "final_df.drone3.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe_z6HGC_UX5"
      },
      "outputs": [],
      "source": [
        "len(final_df.drone3.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LuL56N__UX6"
      },
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    'country',\n",
        "    'length',\n",
        "    'width',\n",
        "    'height',\n",
        "    'max_speed',\n",
        "    'max_alt',\n",
        "    'power_source',\n",
        "    'price',\n",
        "    'image',\n",
        "    'source',\n",
        "    'configuration_harmonized'\n",
        "]\n",
        "\n",
        "dronesensor = dronesensor.drop(columns=cols_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7TC4A8M_UX7"
      },
      "outputs": [],
      "source": [
        "dronesensor.Sensor.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl3hw6iB_UX7"
      },
      "outputs": [],
      "source": [
        "sensor_df.sensor_name.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81FaJplG_UX8"
      },
      "outputs": [],
      "source": [
        "drone_df.configuration.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEM9K97w_UX8"
      },
      "outputs": [],
      "source": [
        "drone_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLW1ViaD_UX9"
      },
      "outputs": [],
      "source": [
        "final_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaIrC-m-_UX9"
      },
      "outputs": [],
      "source": [
        "# Define the mapping\n",
        "config_map = {\n",
        "    '0': 'Fixed Wing',\n",
        "    '2': 'Helicopter / Bicopter',\n",
        "    '3': 'Tricopter',\n",
        "    '4': 'Quadcopter',\n",
        "    '6': 'Hexacopter',\n",
        "    '8': 'Octocopter',\n",
        "    '5': 'Fixed Wing VTOL',\n",
        "    '5 fixed wing VTOL': 'Fixed Wing VTOL',\n",
        "    'fixed wing VTOL': 'Fixed Wing VTOL',\n",
        "    'fixedwing VTOL': 'Fixed Wing VTOL',\n",
        "    'fixed wing': 'Fixed Wing',\n",
        "    'helicopter': 'Helicopter / Bicopter',\n",
        "    'X8': 'X8 (Hybrid Octo)'\n",
        "}\n",
        "\n",
        "# Apply the mapping\n",
        "drone_df[\"configuration_harmonized\"] = drone_df[\"configuration\"].map(config_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XoBrRmI_UX-"
      },
      "outputs": [],
      "source": [
        "# Standardize both fields for safe matching\n",
        "dronesensor['DroneModel_clean'] = dronesensor['DroneModel'].str.strip().str.lower()\n",
        "drone_df['mfc_model_clean'] = drone_df['mfc_model'].str.strip().str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqU7e2ry_UX_"
      },
      "outputs": [],
      "source": [
        "# Step 1: Clean up and filter drone_df first to save memory\n",
        "exclude_fields = [\n",
        "    'rpas_id', 'reg', 'def_payload', 'purpose', 'flight_cont',\n",
        "    'materials', 'min_op_temp', 'max_op_temp', 'power',\n",
        "    'gps_accuracy', 'engine', 'oas', 'flight_time', 'configuration'\n",
        "]\n",
        "\n",
        "# Drop extra fields if they exist\n",
        "drone_extra = drone_df.drop(columns=[col for col in exclude_fields if col in drone_df.columns])\n",
        "\n",
        "# Step 2: Merge only needed columns to avoid memory bloating\n",
        "dronesensor_df = dronesensor.merge(\n",
        "    drone_extra[['mfc_model_clean', 'image']],  # merge only relevant fields\n",
        "    left_on='DroneModel_clean',\n",
        "    right_on='mfc_model_clean',\n",
        "    how='left',\n",
        "    copy=False\n",
        ")\n",
        "\n",
        "# Step 3: Patch missing images manually (lightweight fix)\n",
        "manual_image_map = {\n",
        "    'Dragandfly Innovations Inc Starling X.2': 'https://draganfly.com/wp-content/uploads/2023/07/Disaster-Response.webp',\n",
        "    'Dragandfly Innovations Inc Heavy Lift Drone': 'https://candrone.com/cdn/shop/products/ScreenShot2022-06-03at11.55.59AM.png'\n",
        "}\n",
        "mask = dronesensor_df['image'].isna() & dronesensor_df['DroneModel'].isin(manual_image_map)\n",
        "dronesensor_df.loc[mask, 'image'] = dronesensor_df.loc[mask, 'DroneModel'].map(manual_image_map)\n",
        "\n",
        "# Step 4: Run garbage collection to clean up memory (recommended in ArcGIS Pro)\n",
        "gc.collect()\n",
        "\n",
        "# Optional: Save to disk or GDB/table\n",
        "# dronesensor_df.to_feather(r\"path_to\\light_dronesensor.feather\")  # very fast load/save\n",
        "\n",
        "print(\"✅ Done. Remaining NaN images:\", dronesensor_df['image'].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62uEb7TJ_UX_"
      },
      "outputs": [],
      "source": [
        "# Step 1: Clean up and filter drone_df first to save memory\n",
        "exclude_fields = [\n",
        "    'rpas_id', 'reg', 'def_payload', 'purpose', 'flight_cont',\n",
        "    'materials', 'min_op_temp', 'max_op_temp', 'power',\n",
        "    'gps_accuracy', 'engine', 'oas', 'flight_time', 'configuration'\n",
        "]\n",
        "\n",
        "# Drop extra fields if they exist\n",
        "drone_extra = drone_df.drop(columns=[col for col in exclude_fields if col in drone_df.columns])\n",
        "\n",
        "# Step 2: Merge only needed columns to avoid memory bloating\n",
        "final_df = final_df.merge(\n",
        "    drone_extra[['mfc_model', 'image']],  # merge only relevant fields\n",
        "    left_on='DroneModel',\n",
        "    right_on='mfc_model',\n",
        "    how='left',\n",
        "    copy=False\n",
        ")\n",
        "\n",
        "# Step 3: Patch missing images manually (lightweight fix)\n",
        "manual_image_map = {\n",
        "    'Dragandfly Innovations Inc Starling X.2': 'https://draganfly.com/wp-content/uploads/2023/07/Disaster-Response.webp',\n",
        "    'Dragandfly Innovations Inc Heavy Lift Drone': 'https://candrone.com/cdn/shop/products/ScreenShot2022-06-03at11.55.59AM.png'\n",
        "}\n",
        "mask = dronesensor_df['image'].isna() & final_df['DroneModel'].isin(manual_image_map)\n",
        "final_df.loc[mask, 'image'] = final_df.loc[mask, 'DroneModel'].map(manual_image_map)\n",
        "\n",
        "# Step 4: Run garbage collection to clean up memory (recommended in ArcGIS Pro)\n",
        "gc.collect()\n",
        "\n",
        "# Optional: Save to disk or GDB/table\n",
        "# dronesensor_df.to_feather(r\"path_to\\light_dronesensor.feather\")  # very fast load/save\n",
        "\n",
        "print(\"✅ Done. Remaining NaN images:\", final_df['image'].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDFCKaea_UYA"
      },
      "outputs": [],
      "source": [
        "dronesensor_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPQV9Hss_UYA"
      },
      "outputs": [],
      "source": [
        "# Re-merge with selected non-excluded columns\n",
        "columns_to_add = [\n",
        "    'mfc_model_clean', 'country', 'manufacturer', 'mfc_model',\n",
        "    'length', 'width', 'height', 'max_speed', 'max_alt',\n",
        "    'power_source', 'price', 'image', 'source', 'configuration_harmonized'\n",
        "]\n",
        "\n",
        "# Filter out columns that don't exist in drone_df to avoid error\n",
        "columns_to_add = [col for col in columns_to_add if col in drone_df.columns]\n",
        "\n",
        "# Re-merge\n",
        "dronesensor_df = dronesensor_df.drop(columns=[col for col in columns_to_add if col != 'mfc_model_clean'], errors='ignore')\n",
        "dronesensor_df = dronesensor_df.merge(\n",
        "    drone_df[columns_to_add],\n",
        "    on='mfc_model_clean',\n",
        "    how='left'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EfOrC61_UYB"
      },
      "outputs": [],
      "source": [
        "# Step 1: Ensure DroneModel_clean is lowercase and stripped\n",
        "dronesensor_df[\"DroneModel_clean\"] = dronesensor_df[\"DroneModel\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "# Step 2: Prepare manual image mapping (lowercased keys)\n",
        "manual_image_map = {\n",
        "    \"dragandfly innovations inc starling x.2\": \"https://draganfly.com/wp-content/uploads/2023/07/Disaster-Response.webp\",\n",
        "    \"dragandfly innovations inc heavy lift drone\": \"https://candrone.com/cdn/shop/products/ScreenShot2022-06-03at11.55.59AM.png\"\n",
        "}\n",
        "\n",
        "# Step 3: Create mapping Series\n",
        "manual_map_series = pd.Series(manual_image_map)\n",
        "\n",
        "# Step 4: Fill only rows with missing images using `.fillna()` and `.map()`\n",
        "missing_mask = dronesensor_df[\"image\"].isna()\n",
        "dronesensor_df.loc[missing_mask, \"image\"] = (\n",
        "    dronesensor_df.loc[missing_mask, \"DroneModel_clean\"].map(manual_map_series)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd0y5tYV_UYB"
      },
      "outputs": [],
      "source": [
        "dronesensor_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9duJafx_UYB"
      },
      "outputs": [],
      "source": [
        "dronesensor_df.image.isna().unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIGSMVl7_UYC"
      },
      "outputs": [],
      "source": [
        "# Show DroneModel values where image is NaN after merge\n",
        "missing_image_models = dronesensor_df[dronesensor_df['image'].isna()]['DroneModel'].unique()\n",
        "\n",
        "print(f\"🚨 DroneModels with missing image after merge ({len(missing_image_models)}):\")\n",
        "print(missing_image_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYjhW5qk_UYC"
      },
      "outputs": [],
      "source": [
        "# Mapping dictionary from Sensor column values to sensor_name values\n",
        "sensor_mapping = {\n",
        "    'Thermal_Camera': 'Thermal Camera',\n",
        "    'Thermal_Camera': 'Thermal Camera',  # Ensure consistency for Thermal Camera\n",
        "    'Camera': 'Camera',\n",
        "    'Lidar': 'LiDAR',\n",
        "    'Magnetometers': 'Magnetometers',\n",
        "    'Seismic': 'Seismic'\n",
        "}\n",
        "\n",
        "# Standardize Sensor names in dronesensor_df\n",
        "dronesensor['Sensor'] = dronesensor['Sensor'].map(sensor_mapping).fillna(dronesensor['Sensor'])\n",
        "\n",
        "# List of fields to exclude from sensor_df\n",
        "exclude_sensor = [\n",
        "    'sensor_id', 'parameters_measured', 'method'\n",
        "]\n",
        "\n",
        "# Filter sensor_df to include only desired fields\n",
        "sensor_extra = sensor_df.drop(columns=exclude_sensor)\n",
        "\n",
        "# Merge the dataframes on the consistent sensor names\n",
        "dronesensor = dronesensor.merge(\n",
        "    sensor_extra,\n",
        "    left_on='Sensor',\n",
        "    right_on='sensor_name',\n",
        "    how='left',\n",
        "    suffixes=('', '_sensor')\n",
        ")\n",
        "\n",
        "# Check the resulting dataframe\n",
        "dronesensor.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gKxaKJb_UYC"
      },
      "outputs": [],
      "source": [
        "# List of columns to exclude from dronesensor_df before merge\n",
        "exclude_dronesensor_columns = [\n",
        "    'sensor_name',\n",
        "    'sensor_weight',\n",
        "    'model',\n",
        "    'source_sensor',\n",
        "    'sensor_name_sensor',\n",
        "    'sensor_weight_sensor',\n",
        "    'model_sensor',\n",
        "    'source_sensor'\n",
        "]\n",
        "\n",
        "# Drop the unwanted columns from dronesensor_df\n",
        "dronesensor_df = dronesensor.drop(columns=exclude_dronesensor_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P30dGnBq_UYD"
      },
      "outputs": [],
      "source": [
        "# List of fields to exclude from sensor_df\n",
        "exclude_sensor = [\n",
        "    'sensor_id', 'parameters_measured', 'method'\n",
        "]\n",
        "\n",
        "# Filter sensor_df to include only desired fields\n",
        "sensor_extra = sensor_df.drop(columns=exclude_sensor)\n",
        "\n",
        "# Merge\n",
        "dronesensor_df = dronesensor_df.merge(\n",
        "    sensor_extra,\n",
        "    left_on='Sensor',\n",
        "    right_on='sensor_name',\n",
        "    how='left',\n",
        "    suffixes=('', '_sensor')\n",
        ")\n",
        "dronesensor_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni3ZnnIC_UYD"
      },
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    \"DroneModel\",\n",
        "    \"mfc_model\",\n",
        "    \"mfc_model_clean\",\n",
        "    \"manufacturer\"\n",
        "]\n",
        "\n",
        "dronesensor_df = dronesensor_df.drop(columns=cols_to_drop)\n",
        "dronesensor_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-Lj_IfD_UYE"
      },
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    \"DroneRank\",\n",
        "    \"PayloadOverkill\",\n",
        "    \"DistanceOverkill\",\n",
        "    \"mtow\",\n",
        "    \"distance_range\",\n",
        "    \"max_payload_weight\",\n",
        "    \"comm_range\"\n",
        "]\n",
        "\n",
        "dronesensor_df = dronesensor_df.drop(columns=cols_to_drop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4IoogD-_UYE"
      },
      "source": [
        "----\n",
        "# SD ver 3.8.3\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBfkEr9Y_UYF"
      },
      "outputs": [],
      "source": [
        "def heartbeat(interval=3600):\n",
        "    while True:\n",
        "        tqdm.write(f\"⏳ Heartbeat: still running at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        time.sleep(interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5xQTbI6_UYF"
      },
      "outputs": [],
      "source": [
        "def assign_best_drones_with_blacklist_batch(\n",
        "    merged_df,\n",
        "    drone_df,\n",
        "    chunk_size=50000,\n",
        "    save_dir=r\"D:\\NDIS_Database\\FinalDroneAssignment\",\n",
        "    blacklist_threshold=20000\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Clean drone_df\n",
        "    drone_df = drone_df.dropna(subset=[\"mfc_model\", \"max_payload_weight\"]).copy()\n",
        "    drone_df[\"drone_id\"] = drone_df[\"mfc_model\"]\n",
        "    drone_df[\"comm_range\"] = pd.to_numeric(drone_df[\"comm_range\"], errors=\"coerce\")\n",
        "    drone_df[\"distance_range\"] = pd.to_numeric(drone_df[\"distance_range\"], errors=\"coerce\")\n",
        "    drone_df[\"max_payload_weight\"] = pd.to_numeric(drone_df[\"max_payload_weight\"], errors=\"coerce\")\n",
        "\n",
        "    usage_count = {model: 0 for model in drone_df[\"drone_id\"].unique()}\n",
        "    all_results = []\n",
        "\n",
        "    for start in range(0, len(merged_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(merged_df))\n",
        "        chunk = merged_df.iloc[start:end].copy()\n",
        "        print(f\"\\U0001f680 Processing chunk {start}-{end}\")\n",
        "\n",
        "        chunk[\"distance\"] = pd.to_numeric(chunk[\"distance\"], errors=\"coerce\")\n",
        "        chunk[\"mission_distance\"] = pd.to_numeric(chunk[\"mission_distance\"], errors=\"coerce\")\n",
        "        chunk[\"sensor_weight\"] = pd.to_numeric(chunk[\"sensor_weight\"], errors=\"coerce\")\n",
        "\n",
        "        chunk[[\"drone1\", \"drone2\", \"drone3\"]] = None\n",
        "        chunk[[\"note1\", \"note2\", \"note3\"]] = None\n",
        "\n",
        "        for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
        "            s_weight = row[\"sensor_weight\"]\n",
        "            h_dist = row[\"distance\"]\n",
        "            m_dist = row[\"mission_distance\"]\n",
        "            sensor = row[\"RecommendedSensor\"]\n",
        "\n",
        "            if pd.isna(s_weight):\n",
        "                continue\n",
        "\n",
        "            candidates = drone_df[drone_df[\"max_payload_weight\"] >= s_weight].copy()\n",
        "            if candidates.empty:\n",
        "                continue\n",
        "\n",
        "            def score_drone(dr):\n",
        "                score = 100\n",
        "                note = \"\"\n",
        "\n",
        "                if not pd.isna(h_dist):\n",
        "                    if dr[\"comm_range\"] < h_dist:\n",
        "                        score += 50\n",
        "                        note = \"Comm range insufficient\"\n",
        "\n",
        "                if not pd.isna(m_dist):\n",
        "                    if dr[\"distance_range\"] >= m_dist:\n",
        "                        score -= 5\n",
        "                        note = \"Full coverage\"\n",
        "                    elif dr[\"distance_range\"] >= 0.5 * m_dist:\n",
        "                        score += 5\n",
        "                        note = \"2–3 swaths needed\"\n",
        "                    elif dr[\"distance_range\"] >= 0.25 * m_dist:\n",
        "                        score += 10\n",
        "                        note = \"Multiple passes\"\n",
        "                    else:\n",
        "                        score += 30\n",
        "                        note = \"Very limited range\"\n",
        "                else:\n",
        "                    note = \"Distance unknown — likely offshore\"\n",
        "                    score += 20\n",
        "\n",
        "                # Payload proximity\n",
        "                payload_diff = dr[\"max_payload_weight\"] - s_weight\n",
        "                score += abs(payload_diff) * 0.05\n",
        "\n",
        "                # Usage penalty\n",
        "                uc = usage_count.get(dr[\"drone_id\"], 0)\n",
        "                if uc >= blacklist_threshold:\n",
        "                    score += 9999  # Hard exclude\n",
        "                else:\n",
        "                    score += uc * 0.01  # Soft bias\n",
        "\n",
        "                score += random.uniform(-0.5, 0.5)\n",
        "                return pd.Series([score, note])\n",
        "\n",
        "            candidates[[\"score\", \"note\"]] = candidates.apply(score_drone, axis=1)\n",
        "            candidates = candidates.sort_values(\"score\").drop_duplicates(\"drone_id\").head(10)\n",
        "\n",
        "            assigned = []\n",
        "            notes = []\n",
        "\n",
        "            for _, dr in candidates.iterrows():\n",
        "                if dr[\"drone_id\"] not in assigned:\n",
        "                    assigned.append(dr[\"drone_id\"])\n",
        "                    notes.append(dr[\"note\"])\n",
        "                if len(assigned) == 3:\n",
        "                    break\n",
        "\n",
        "            for i in range(len(assigned)):\n",
        "                chunk.at[idx, f\"drone{i+1}\"] = assigned[i]\n",
        "                chunk.at[idx, f\"note{i+1}\"] = notes[i]\n",
        "                usage_count[assigned[i]] += 1\n",
        "\n",
        "        # Save feather\n",
        "        out_path = os.path.join(save_dir, f\"drone_assign_chunk_{start}_{end}.feather\")\n",
        "        chunk.reset_index(drop=True).to_feather(out_path)\n",
        "        all_results.append(chunk)\n",
        "        gc.collect()\n",
        "\n",
        "    return pd.concat(all_results, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcFRX3fA_UYF"
      },
      "outputs": [],
      "source": [
        "# Start the background heartbeat thread\n",
        "threading.Thread(target=heartbeat, args=(3600,), daemon=True).start()\n",
        "\n",
        "# Run the main process with timer\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "final_df = assign_best_drones_with_blacklist_batch(sample, drone_df, chunk_size=50000)\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuKnr--O_UYG"
      },
      "outputs": [],
      "source": [
        "save_dir = r\"D:\\NDIS_Database\\FinalDroneAssignment\"\n",
        "\n",
        "# List all feather files in the save directory\n",
        "feather_files = [os.path.join(save_dir, f) for f in os.listdir(save_dir) if f.endswith(\".feather\")]\n",
        "\n",
        "# Read and concatenate all feather chunks\n",
        "all_chunks = [pd.read_feather(fp) for fp in sorted(feather_files)]\n",
        "final_df = pd.concat(all_chunks, ignore_index=True)\n",
        "\n",
        "# Now `final_df` contains the full output\n",
        "final_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owf0dEv4_UYG"
      },
      "outputs": [],
      "source": [
        "len(final_df.drone3.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlXTNpUK_UYH"
      },
      "outputs": [],
      "source": [
        "def assign_best_drones_with_hybrid_penalty(\n",
        "    merged_df,\n",
        "    drone_df,\n",
        "    chunk_size=50000,\n",
        "    save_dir=r\"D:\\NDIS_Database\\FinalDroneAssignment\",\n",
        "    global_penalty=0.001,\n",
        "    category_penalty=0.01\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Clean drone_df\n",
        "    drone_df = drone_df.dropna(subset=[\"mfc_model\", \"max_payload_weight\"]).copy()\n",
        "    drone_df[\"drone_id\"] = drone_df[\"mfc_model\"]\n",
        "    drone_df[\"comm_range\"] = pd.to_numeric(drone_df[\"comm_range\"], errors=\"coerce\")\n",
        "    drone_df[\"distance_range\"] = pd.to_numeric(drone_df[\"distance_range\"], errors=\"coerce\")\n",
        "    drone_df[\"max_payload_weight\"] = pd.to_numeric(drone_df[\"max_payload_weight\"], errors=\"coerce\")\n",
        "\n",
        "    # Initialize usage tracking\n",
        "    global_usage_count = {model: 0 for model in drone_df[\"drone_id\"].unique()}\n",
        "    usage_by_category = {}\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for start in range(0, len(merged_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(merged_df))\n",
        "        chunk = merged_df.iloc[start:end].copy()\n",
        "        print(f\"\\U0001f680 Processing chunk {start}-{end}\")\n",
        "\n",
        "        chunk[\"distance\"] = pd.to_numeric(chunk[\"distance\"], errors=\"coerce\")\n",
        "        chunk[\"mission_distance\"] = pd.to_numeric(chunk[\"mission_distance\"], errors=\"coerce\")\n",
        "        chunk[\"sensor_weight\"] = pd.to_numeric(chunk[\"sensor_weight\"], errors=\"coerce\")\n",
        "\n",
        "        chunk[[\"drone1\", \"drone2\", \"drone3\"]] = None\n",
        "        chunk[[\"note1\", \"note2\", \"note3\"]] = None\n",
        "\n",
        "        for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
        "            s_weight = row[\"sensor_weight\"]\n",
        "            h_dist = row[\"distance\"]\n",
        "            m_dist = row[\"mission_distance\"]\n",
        "            sensor = row[\"sensor_model\"] if \"sensor_model\" in row else row[\"RecommendedSensor\"]\n",
        "            hazard = row.get(\"HazardType\", \"Unknown\")\n",
        "\n",
        "            if pd.isna(s_weight):\n",
        "                continue\n",
        "\n",
        "            candidates = drone_df[drone_df[\"max_payload_weight\"] >= s_weight].copy()\n",
        "            if candidates.empty:\n",
        "                continue\n",
        "\n",
        "            def score_drone(dr):\n",
        "                score = 100\n",
        "                note = \"\"\n",
        "\n",
        "                if not pd.isna(h_dist):\n",
        "                    if dr[\"comm_range\"] < h_dist:\n",
        "                        score += 50\n",
        "                        note = \"Comm range insufficient\"\n",
        "\n",
        "                if not pd.isna(m_dist):\n",
        "                    if dr[\"distance_range\"] >= m_dist:\n",
        "                        score -= 5\n",
        "                        note = \"Full coverage\"\n",
        "                    elif dr[\"distance_range\"] >= 0.5 * m_dist:\n",
        "                        score += 5\n",
        "                        note = \"2–3 swaths needed\"\n",
        "                    elif dr[\"distance_range\"] >= 0.25 * m_dist:\n",
        "                        score += 10\n",
        "                        note = \"Multiple passes\"\n",
        "                    else:\n",
        "                        score += 30\n",
        "                        note = \"Very limited range\"\n",
        "                else:\n",
        "                    note = \"Distance unknown — likely offshore\"\n",
        "                    score += 20\n",
        "\n",
        "                # Payload proximity\n",
        "                payload_diff = dr[\"max_payload_weight\"] - s_weight\n",
        "                score += abs(payload_diff) * 0.05\n",
        "\n",
        "                # Hybrid penalty\n",
        "                drone_id = dr[\"drone_id\"]\n",
        "                cat_key = (hazard, sensor, drone_id)\n",
        "\n",
        "                g_usage = global_usage_count.get(drone_id, 0)\n",
        "                c_usage = usage_by_category.get(cat_key, 0)\n",
        "\n",
        "                score += g_usage * global_penalty\n",
        "                score += c_usage * category_penalty\n",
        "\n",
        "                score += random.uniform(-0.5, 0.5)\n",
        "                return pd.Series([score, note])\n",
        "\n",
        "            candidates[[\"score\", \"note\"]] = candidates.apply(score_drone, axis=1)\n",
        "            candidates = candidates.sort_values(\"score\").drop_duplicates(\"drone_id\").head(10)\n",
        "\n",
        "            assigned = []\n",
        "            notes = []\n",
        "\n",
        "            for _, dr in candidates.iterrows():\n",
        "                if dr[\"drone_id\"] not in assigned:\n",
        "                    assigned.append(dr[\"drone_id\"])\n",
        "                    notes.append(dr[\"note\"])\n",
        "                if len(assigned) == 3:\n",
        "                    break\n",
        "\n",
        "            for i in range(len(assigned)):\n",
        "                chunk.at[idx, f\"drone{i+1}\"] = assigned[i]\n",
        "                chunk.at[idx, f\"note{i+1}\"] = notes[i]\n",
        "                drone_id = assigned[i]\n",
        "                cat_key = (hazard, sensor, drone_id)\n",
        "\n",
        "                global_usage_count[drone_id] += 1\n",
        "                usage_by_category[cat_key] = usage_by_category.get(cat_key, 0) + 1\n",
        "\n",
        "        # Save feather\n",
        "        out_path = os.path.join(save_dir, f\"drone_assign_chunk_{start}_{end}.feather\")\n",
        "        chunk.reset_index(drop=True).to_feather(out_path)\n",
        "        all_results.append(chunk)\n",
        "        gc.collect()\n",
        "\n",
        "    return pd.concat(all_results, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74usoGZs_UYH"
      },
      "outputs": [],
      "source": [
        "# Start the background heartbeat thread\n",
        "threading.Thread(target=heartbeat, args=(3600,), daemon=True).start()\n",
        "\n",
        "# Run the main process with timer\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "final_df = assign_best_drones_with_hybrid_penalty(sample, drone_df, chunk_size=50000)\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhMRBeN0_UYH"
      },
      "outputs": [],
      "source": [
        "final_df.drone3.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fDQODyc_UYI"
      },
      "outputs": [],
      "source": [
        "def assign_best_drones_with_hybrid_penalty(\n",
        "    merged_df,\n",
        "    drone_df,\n",
        "    chunk_size=50000,\n",
        "    save_dir=r\"D:\\NDIS_Database\\20_PaperSimulation\\FinalDroneAssignment\",\n",
        "    global_penalty=0.001,\n",
        "    category_penalty=0.01\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Clean drone_df\n",
        "    drone_df = drone_df.dropna(subset=[\"mfc_model\", \"max_payload_weight\"]).copy()\n",
        "    drone_df[\"drone_id\"] = drone_df[\"mfc_model\"]\n",
        "    drone_df[\"comm_range\"] = pd.to_numeric(drone_df[\"comm_range\"], errors=\"coerce\")\n",
        "    drone_df[\"distance_range\"] = pd.to_numeric(drone_df[\"distance_range\"], errors=\"coerce\")\n",
        "    drone_df[\"max_payload_weight\"] = pd.to_numeric(drone_df[\"max_payload_weight\"], errors=\"coerce\")\n",
        "\n",
        "    # Usage trackers\n",
        "    global_usage_count = {model: 0 for model in drone_df[\"drone_id\"].unique()}\n",
        "    usage_by_category = {}\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for start in range(0, len(merged_df), chunk_size):\n",
        "        end = min(start + chunk_size, len(merged_df))\n",
        "        chunk = merged_df.iloc[start:end].copy()\n",
        "        print(f\"\\U0001f680 Processing chunk {start}-{end}\")\n",
        "\n",
        "        chunk[\"distance\"] = pd.to_numeric(chunk[\"distance\"], errors=\"coerce\")\n",
        "        chunk[\"mission_distance\"] = pd.to_numeric(chunk[\"mission_distance\"], errors=\"coerce\")\n",
        "        chunk[\"sensor_weight\"] = pd.to_numeric(chunk[\"sensor_weight\"], errors=\"coerce\")\n",
        "\n",
        "        chunk[[\"drone1\", \"drone2\", \"drone3\"]] = None\n",
        "        chunk[[\"note1\", \"note2\", \"note3\"]] = None\n",
        "\n",
        "        for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
        "            s_weight = row[\"sensor_weight\"]\n",
        "            h_dist = row[\"distance\"]\n",
        "            m_dist = row[\"mission_distance\"]\n",
        "            sensor = row.get(\"sensor_model\") or row.get(\"RecommendedSensor\")\n",
        "            hazard = row.get(\"HazardType\", \"Unknown\")\n",
        "\n",
        "            if pd.isna(s_weight):\n",
        "                continue\n",
        "\n",
        "            candidates = drone_df[drone_df[\"max_payload_weight\"] >= s_weight].copy()\n",
        "            if candidates.empty:\n",
        "                continue\n",
        "\n",
        "            def score_drone(dr):\n",
        "                score = 100\n",
        "                note = \"\"\n",
        "\n",
        "                # Comm range penalty\n",
        "                if not pd.isna(h_dist) and dr[\"comm_range\"] < h_dist:\n",
        "                    score += 30\n",
        "                    note = \"Comm range insufficient\"\n",
        "\n",
        "                # Mission distance penalty\n",
        "                if not pd.isna(m_dist):\n",
        "                    if dr[\"distance_range\"] >= m_dist:\n",
        "                        score -= 10\n",
        "                        note = \"Full coverage\"\n",
        "                    elif dr[\"distance_range\"] >= 0.75 * m_dist:\n",
        "                        score += 2\n",
        "                        note = \"Near full coverage\"\n",
        "                    elif dr[\"distance_range\"] >= 0.5 * m_dist:\n",
        "                        score += 5\n",
        "                        note = \"2–3 swaths needed\"\n",
        "                    elif dr[\"distance_range\"] >= 0.25 * m_dist:\n",
        "                        score += 15\n",
        "                        note = \"Multiple passes\"\n",
        "                    else:\n",
        "                        score += 25\n",
        "                        note = \"Very limited range\"\n",
        "                else:\n",
        "                    score += 10\n",
        "                    note = \"Distance unknown\"\n",
        "\n",
        "                # Payload proximity penalty (mild)\n",
        "                payload_diff = dr[\"max_payload_weight\"] - s_weight\n",
        "                score += min(abs(payload_diff) * 0.01, 10)\n",
        "\n",
        "                # Hybrid penalties\n",
        "                drone_id = dr[\"drone_id\"]\n",
        "                cat_key = (hazard, sensor, drone_id)\n",
        "                g_usage = global_usage_count.get(drone_id, 0)\n",
        "                c_usage = usage_by_category.get(cat_key, 0)\n",
        "                score += g_usage * global_penalty\n",
        "                score += c_usage * category_penalty\n",
        "\n",
        "                score += random.uniform(-0.25, 0.25)\n",
        "                return pd.Series([score, note])\n",
        "\n",
        "            candidates[[\"score\", \"note\"]] = candidates.apply(score_drone, axis=1)\n",
        "            candidates = candidates.sort_values(\"score\").drop_duplicates(\"drone_id\").head(10)\n",
        "\n",
        "\n",
        "\n",
        "            assigned = []\n",
        "            notes = []\n",
        "\n",
        "            for _, dr in candidates.iterrows():\n",
        "                if dr[\"drone_id\"] not in assigned:\n",
        "                    assigned.append(dr[\"drone_id\"])\n",
        "                    notes.append(dr[\"note\"])\n",
        "                if len(assigned) == 3:\n",
        "                    break\n",
        "\n",
        "            for i in range(len(assigned)):\n",
        "                chunk.at[idx, f\"drone{i+1}\"] = assigned[i]\n",
        "                chunk.at[idx, f\"note{i+1}\"] = notes[i]\n",
        "                drone_id = assigned[i]\n",
        "                cat_key = (hazard, sensor, drone_id)\n",
        "                global_usage_count[drone_id] += 1\n",
        "                usage_by_category[cat_key] = usage_by_category.get(cat_key, 0) + 1\n",
        "\n",
        "        # Save feather\n",
        "        out_path = os.path.join(save_dir, f\"drone_assign_chunk_{start}_{end}.feather\")\n",
        "        chunk.reset_index(drop=True).to_feather(out_path)\n",
        "        all_results.append(chunk)\n",
        "        gc.collect()\n",
        "\n",
        "    return pd.concat(all_results, ignore_index=True), global_usage_count, usage_by_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnL76bNc_UYI"
      },
      "outputs": [],
      "source": [
        "# Start the background heartbeat thread\n",
        "threading.Thread(target=heartbeat, args=(3600,), daemon=True).start()\n",
        "\n",
        "# Run the main process with timer\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "result_df, global_usage_count, usage_by_category = assign_best_drones_with_hybrid_penalty(\n",
        "    merged_df=ghz_with_cpm,\n",
        "    drone_df=drone_df,\n",
        "    chunk_size=50000\n",
        ")\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(\"\\u2705 All processing completed! Elapsed time: %s minutes\"%str(elapsed/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfaZUvBd_UYI"
      },
      "outputs": [],
      "source": [
        "result_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8wk6My7_UYJ"
      },
      "outputs": [],
      "source": [
        "len(final_df.drone2.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-yDtISg_UYJ"
      },
      "outputs": [],
      "source": [
        "# --- 1) Helper: rebuild usage + detect finished chunks ---\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def rebuild_usage_from_saved(save_dir):\n",
        "    global_usage_count = defaultdict(int)\n",
        "    usage_by_category = defaultdict(int)\n",
        "    done_ranges = set()\n",
        "\n",
        "    pattern = os.path.join(save_dir, \"drone_assign_chunk_*_*.feather\")\n",
        "    for fp in sorted(glob.glob(pattern)):\n",
        "        base = os.path.basename(fp)\n",
        "        try:\n",
        "            _, s, e = base.replace(\".feather\", \"\").split(\"_\")[-3:]\n",
        "            start_i, end_i = int(s), int(e)\n",
        "            done_ranges.add((start_i, end_i))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        df = pd.read_feather(fp)\n",
        "        sensor_col = \"sensor_model\" if \"sensor_model\" in df.columns else (\"RecommendedSensor\" if \"RecommendedSensor\" in df.columns else None)\n",
        "        sens = df[sensor_col] if sensor_col else pd.Series([\"Unknown\"] * len(df))\n",
        "        haz  = df[\"HazardType\"] if \"HazardType\" in df.columns else pd.Series([\"Unknown\"] * len(df))\n",
        "\n",
        "        for k in (1, 2, 3):\n",
        "            dcol = f\"drone{k}\"\n",
        "            if dcol not in df.columns:\n",
        "                continue\n",
        "            mask = df[dcol].notna()\n",
        "            if not mask.any():\n",
        "                continue\n",
        "            sub = df.loc[mask, [dcol]].copy()\n",
        "            sub_h = haz.loc[mask]\n",
        "            sub_s = sens.loc[mask]\n",
        "            for h, s, d in zip(sub_h, sub_s, sub[dcol]):\n",
        "                d_id = str(d)\n",
        "                global_usage_count[d_id] += 1\n",
        "                usage_by_category[(h if pd.notna(h) else \"Unknown\",\n",
        "                                   s if pd.notna(s) else \"Unknown\",\n",
        "                                   d_id)] += 1\n",
        "\n",
        "    return dict(global_usage_count), dict(usage_by_category), done_ranges\n",
        "\n",
        "\n",
        "# --- 2) Faster + resumable assigner (hybrid penalties, vectorized scoring) ---\n",
        "import os, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def assign_best_drones_with_hybrid_penalty_resumable(\n",
        "    merged_df,\n",
        "    drone_df,\n",
        "    chunk_size=50_000,\n",
        "    save_dir=r\"D:\\NDIS_Database\\20_PaperSimulation\\FinalDroneAssignment\",\n",
        "    global_penalty=0.001,\n",
        "    category_penalty=0.01,\n",
        "    resume=True,\n",
        "    return_results=False\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    ddf = drone_df.dropna(subset=[\"mfc_model\", \"max_payload_weight\"]).copy()\n",
        "    ddf = ddf.assign(\n",
        "        drone_id = ddf[\"mfc_model\"].astype(str),\n",
        "        comm_range = pd.to_numeric(ddf[\"comm_range\"], errors=\"coerce\"),\n",
        "        distance_range = pd.to_numeric(ddf[\"distance_range\"], errors=\"coerce\"),\n",
        "        max_payload_weight = pd.to_numeric(ddf[\"max_payload_weight\"], errors=\"coerce\"),\n",
        "    )\n",
        "    d_ids   = ddf[\"drone_id\"].to_numpy()\n",
        "    d_comm  = ddf[\"comm_range\"].to_numpy()\n",
        "    d_dist  = ddf[\"distance_range\"].to_numpy()\n",
        "    d_pay   = ddf[\"max_payload_weight\"].to_numpy()\n",
        "\n",
        "    if resume:\n",
        "        global_usage_count, usage_by_category, done_ranges = rebuild_usage_from_saved(save_dir)\n",
        "    else:\n",
        "        global_usage_count, usage_by_category, done_ranges = {}, {}, set()\n",
        "\n",
        "    all_results = []\n",
        "    n = len(merged_df)\n",
        "\n",
        "    for start in range(0, n, chunk_size):\n",
        "        end = min(start + chunk_size, n)\n",
        "\n",
        "        if (start, end) in done_ranges:\n",
        "            print(f\"⏭️  Skipping chunk {start}-{end} (already saved)\")\n",
        "            continue\n",
        "\n",
        "        print(f\"🚀 Processing chunk {start}-{end}\")\n",
        "        chunk = merged_df.iloc[start:end].copy()\n",
        "\n",
        "        h_dist_col   = pd.to_numeric(chunk[\"distance\"], errors=\"coerce\") if \"distance\" in chunk.columns else pd.Series(np.nan, index=chunk.index)\n",
        "        m_dist_col   = pd.to_numeric(chunk[\"mission_distance\"], errors=\"coerce\") if \"mission_distance\" in chunk.columns else pd.Series(np.nan, index=chunk.index)\n",
        "        s_weight_col = pd.to_numeric(chunk[\"sensor_weight\"], errors=\"coerce\") if \"sensor_weight\" in chunk.columns else pd.Series(np.nan, index=chunk.index)\n",
        "\n",
        "        for k in (1,2,3):\n",
        "            chunk[f\"drone{k}\"] = None\n",
        "            chunk[f\"note{k}\"]  = None\n",
        "\n",
        "        it = tqdm(list(chunk.itertuples(index=True, name=None)), total=len(chunk))\n",
        "        for tup in it:\n",
        "            idx = tup[0]\n",
        "            s_weight = s_weight_col.at[idx]\n",
        "            if pd.isna(s_weight):\n",
        "                continue\n",
        "\n",
        "            h_dist = h_dist_col.at[idx]\n",
        "            m_dist = m_dist_col.at[idx]\n",
        "            sensor = (chunk.at[idx, \"sensor_model\"]\n",
        "                      if \"sensor_model\" in chunk.columns and pd.notna(chunk.at[idx, \"sensor_model\"])\n",
        "                      else chunk.at[idx, \"RecommendedSensor\"]\n",
        "                      if \"RecommendedSensor\" in chunk.columns and pd.notna(chunk.at[idx, \"RecommendedSensor\"])\n",
        "                      else \"Unknown\")\n",
        "            hazard = chunk.at[idx, \"HazardType\"] if \"HazardType\" in chunk.columns and pd.notna(chunk.at[idx, \"HazardType\"]) else \"Unknown\"\n",
        "\n",
        "            mask = d_pay >= s_weight\n",
        "            if not mask.any():\n",
        "                continue\n",
        "\n",
        "            cand_ids  = d_ids[mask]\n",
        "            cand_comm = d_comm[mask]\n",
        "            cand_dist = d_dist[mask]\n",
        "            cand_pay  = d_pay[mask]\n",
        "\n",
        "            scores = np.full(cand_ids.shape[0], 100.0)\n",
        "            notes  = np.empty(cand_ids.shape[0], dtype=object); notes[:] = \"\"\n",
        "\n",
        "            if pd.notna(h_dist):\n",
        "                comm_bad = cand_comm < h_dist\n",
        "                scores[comm_bad] += 30.0\n",
        "                notes[comm_bad] = \"Comm range insufficient\"\n",
        "\n",
        "            if pd.notna(m_dist):\n",
        "                full      = cand_dist >= m_dist\n",
        "                near_full = (~full) & (cand_dist >= 0.75*m_dist)\n",
        "                half      = (~full) & (~near_full) & (cand_dist >= 0.50*m_dist)\n",
        "                quarter   = (~full) & (~near_full) & (~half) & (cand_dist >= 0.25*m_dist)\n",
        "                very_low  = (~full) & (~near_full) & (~half) & (~quarter)\n",
        "\n",
        "                scores[full]      -= 10.0; notes[full]      = \"Full coverage\"\n",
        "                scores[near_full] +=  2.0; notes[near_full] = \"Near full coverage\"\n",
        "                scores[half]      +=  5.0; notes[half]      = \"2–3 swaths needed\"\n",
        "                scores[quarter]   += 15.0; notes[quarter]   = \"Multiple passes\"\n",
        "                scores[very_low]  += 25.0; notes[very_low]  = \"Very limited range\"\n",
        "            else:\n",
        "                scores += 10.0; notes[:] = \"Distance unknown\"\n",
        "\n",
        "            payload_diff = np.abs(cand_pay - s_weight)\n",
        "            scores += np.minimum(payload_diff * 0.01, 10.0)\n",
        "\n",
        "            g_vec = np.fromiter((global_usage_count.get(d, 0) for d in cand_ids), dtype=float, count=cand_ids.size)\n",
        "            c_vec = np.fromiter((usage_by_category.get((hazard, sensor, d), 0) for d in cand_ids), dtype=float, count=cand_ids.size)\n",
        "            scores += g_vec * global_penalty\n",
        "            scores += c_vec * category_penalty\n",
        "\n",
        "            # jitter can be disabled for determinism:\n",
        "            # scores += np.random.uniform(-0.25, 0.25, size=cand_ids.size)\n",
        "\n",
        "            # top10 then top3 without full sort\n",
        "            k10 = min(10, scores.size-1)\n",
        "            top10_idx = np.argpartition(scores, kth=k10)[:k10+1]\n",
        "            order10 = np.argsort(scores[top10_idx])\n",
        "            pick_idx = top10_idx[order10][:3]\n",
        "\n",
        "            chosen_ids  = cand_ids[pick_idx]\n",
        "            chosen_notes= notes[pick_idx]\n",
        "\n",
        "            for rank, (d_id, nte) in enumerate(zip(chosen_ids, chosen_notes), start=1):\n",
        "                chunk.at[idx, f\"drone{rank}\"] = d_id\n",
        "                chunk.at[idx, f\"note{rank}\"]  = nte\n",
        "                global_usage_count[d_id] = global_usage_count.get(d_id, 0) + 1\n",
        "                usage_by_category[(hazard, sensor, d_id)] = usage_by_category.get((hazard, sensor, d_id), 0) + 1\n",
        "\n",
        "        out_tmp  = os.path.join(save_dir, f\"drone_assign_chunk_{start}_{end}.feather.tmp\")\n",
        "        out_path = os.path.join(save_dir, f\"drone_assign_chunk_{start}_{end}.feather\")\n",
        "        chunk.reset_index(drop=True).to_feather(out_tmp)\n",
        "        os.replace(out_tmp, out_path)\n",
        "\n",
        "        if return_results:\n",
        "            all_results.append(chunk)\n",
        "\n",
        "        del chunk\n",
        "        gc.collect()\n",
        "\n",
        "    if return_results:\n",
        "        return pd.concat(all_results, ignore_index=True), global_usage_count, usage_by_category\n",
        "    else:\n",
        "        return None, global_usage_count, usage_by_category\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNSdPbs4_UYK"
      },
      "outputs": [],
      "source": [
        "save_dir = r\"D:\\NDIS_Database\\20_PaperSimulation\\FinalDroneAssignment\"\n",
        "\n",
        "# heartbeat is fine to keep\n",
        "threading.Thread(target=heartbeat, args=(3600,), daemon=True).start()\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "_, global_usage_count, usage_by_category = assign_best_drones_with_hybrid_penalty_resumable(\n",
        "    merged_df=ghz_with_cpm,\n",
        "    drone_df=drone_df,\n",
        "    chunk_size=50_000,\n",
        "    save_dir=save_dir,\n",
        "    resume=True,          # <-- key: skips already-saved chunks, rebuilds usage\n",
        "    return_results=False  # keep RAM low; results are on disk\n",
        ")\n",
        "\n",
        "elapsed = (timeit.default_timer() - start_time)/60\n",
        "print(f\"✅ Finished (resume). Elapsed minutes: {elapsed:.2f}\")\n",
        "#print(\"S900 usage:\", global_usage_count.get(\"DJI S900\", 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j4on_wV_UYL"
      },
      "outputs": [],
      "source": [
        "import glob, os\n",
        "feathers = sorted(glob.glob(os.path.join(save_dir, \"drone_assign_chunk_*_*.feather\")))\n",
        "final_df = pd.concat((pd.read_feather(fp) for fp in feathers), ignore_index=True)\n",
        "final_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QtAXAm_UYL"
      },
      "source": [
        "----\n",
        "# ExplodeData\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxXPvDxv_UYM"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "\n",
        "# ======= CONFIG =======\n",
        "CHUNK_SIZE = 500_000  # tune for RAM\n",
        "OUT_DIR = r\"D:\\NDIS_Database\\20_PaperSimulation\\FinalExploded\"\n",
        "OUT_PREFIX = \"part\"\n",
        "\n",
        "# Parquet speed/size tradeoffs (good defaults for 10M+ rows)\n",
        "PARQUET_KW = dict(\n",
        "    engine=\"pyarrow\",\n",
        "    compression=\"zstd\",\n",
        "    compression_level=3,     # 3–5 sweet spot\n",
        "    use_dictionary=True,     # repeated strings (e.g., models) compress well\n",
        "    write_statistics=True,\n",
        "    row_group_size=1_000_000 # ~1M rows per row group\n",
        ")\n",
        "\n",
        "def explode_drones(df: pd.DataFrame,\n",
        "                   drone_cols=('drone1','drone2','drone3'),\n",
        "                   note_cols=('note1','note2','note3'),\n",
        "                   out_drone_col='drone_model',\n",
        "                   out_note_col='note') -> pd.DataFrame:\n",
        "    # keep id columns\n",
        "    id_cols = [c for c in df.columns if c not in (*drone_cols, *note_cols)]\n",
        "\n",
        "    # align columns by rank labels \"1\",\"2\",\"3\"\n",
        "    d = df.loc[:, drone_cols].rename(columns=dict(zip(drone_cols, ['1','2','3'])))\n",
        "    n = df.loc[:, note_cols ].rename(columns=dict(zip(note_cols,  ['1','2','3'])))\n",
        "\n",
        "    # build hierarchical columns then stack → long form (vectorized)\n",
        "    tmp = pd.concat({out_drone_col: d, out_note_col: n}, axis=1)\n",
        "    long = tmp.stack(level=1).reset_index(level=1).rename(columns={'level_1': 'drone_rank'})\n",
        "\n",
        "    # join back id columns; drop empty drones\n",
        "    out = (\n",
        "        df[id_cols]\n",
        "        .join(long)\n",
        "        .loc[lambda x: x[out_drone_col].notna() & (x[out_drone_col].astype(str).str.len() > 0)]\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # tidy up types and sort (stable)\n",
        "    if 'drone_rank' in out.columns:\n",
        "        try:\n",
        "            out['drone_rank'] = out['drone_rank'].astype('int8')\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    sort_cols = [c for c in ['HazardID','RecommendedSensor','DisasterPhase','drone_rank'] if c in out.columns]\n",
        "    if sort_cols:\n",
        "        out = out.sort_values(sort_cols, kind='stable', ignore_index=True)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S48FkZig_UYM"
      },
      "outputs": [],
      "source": [
        "# ======= CHUNKED PROCESS =======\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "n = len(final_df)\n",
        "num_chunks = math.ceil(n / CHUNK_SIZE)\n",
        "print(f\"Processing {n:,} rows in {num_chunks} chunk(s) of {CHUNK_SIZE:,}...\")\n",
        "\n",
        "t0 = time.time()\n",
        "for i in range(0, n, CHUNK_SIZE):\n",
        "    j = min(i + CHUNK_SIZE, n)\n",
        "    t_chunk0 = time.time()\n",
        "\n",
        "    chunk = final_df.iloc[i:j]\n",
        "    exploded = explode_drones(chunk)\n",
        "\n",
        "    out_path = os.path.join(OUT_DIR, f\"{OUT_PREFIX}_{i//CHUNK_SIZE:05d}.parquet\")\n",
        "    exploded.to_parquet(out_path, index=False, **PARQUET_KW)\n",
        "\n",
        "    del chunk, exploded\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"✓ {i//CHUNK_SIZE+1}/{num_chunks}  rows[{i:,}:{j:,}) -> {out_path}  ({time.time()-t_chunk0:.1f}s)\")\n",
        "\n",
        "print(f\"Done in {(time.time()-t0)/60:.2f} min. Parts in: {OUT_DIR}\")\n",
        "\n",
        "# ---- OPTIONAL: FEATHER FOR MAX I/O SPEED (scratch only) ----\n",
        "# exploded.to_feather(out_path.replace(\".parquet\", \".feather\"), compression=\"lz4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQLb5FQ5_UYM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ArcGISPro",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}